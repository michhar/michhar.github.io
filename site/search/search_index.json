{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A blog on data science in the world of software development My name is Micheleen Harris (Twitter: @rheartpython ) and I'm interested in data science, have taught it some and am still learning much. My posts tend to be more like tutorials around exciting projects I've come across in my career. I'm currently fascinated by: Deep learning for computer vision and NLP PyTorch The Jupyter project ML on the Edge (IoT)","title":"Home"},{"location":"#a-blog-on-data-science-in-the-world-of-software-development","text":"My name is Micheleen Harris (Twitter: @rheartpython ) and I'm interested in data science, have taught it some and am still learning much. My posts tend to be more like tutorials around exciting projects I've come across in my career. I'm currently fascinated by: Deep learning for computer vision and NLP PyTorch The Jupyter project ML on the Edge (IoT)","title":"A blog on data science in the world of software development"},{"location":"a-python-flask-webapp-gets-smart/","text":"tl;dr : Azure Machine Learning + Visual Studio + Python Flask + GitHub + Azure = A Live Custom ML Model for You! Posted: 2017-02-05 Introduction Ok, so I have an interesting REST endpoint (in my case, a machine learning model for using a company's Wikipedia article to find similar companies), what can I do next? Why not serve it up in a simple web app to impress friends and wow colleagues? (Really, you can use this intel to create a web app around any REST endpoint, as half of my purpose in writing this is to show how fast and easy Python Flask is). Essentially, we are making a web app wrapper around a data submission and retrieval REST endpoint that is created through Azure Machine Learning (AML) Studio ( https://studio.azureml.net ), a friendly and powerful machine learning tool with a handy browser UI. In this post, the endpoint is a service that clusters companies based on descriptive text (our input data). The clustering model, a k-means algorithm, has been trained on close to 500 wikipedia entries, a cool example of unsupervised learning . If you don't know much yet about AML Studio and would like to know more this is a good place to start or dive in and learn by doing with a quick getting-started tutorial here . You'll need to know, at least, how to publish an experiment from Studio to get your Flask web app going. The ML web service is based around an AML scoring experiment, built from a training experiment in which K-Means Clustering module is used to assign companies to groups based on features in their processed Wikipedia text. The Extract N-Gram Features from Text module (more info here ) is used after some initial cleansing of the text data (remove stop words, numbers, special characters, detect sentences, etc. - see the Preprocess Text AML module here ) to extract features upon which to train the k-means clustering model and reduce the dimensionality to the most important chunks of information. The scoring experiment uses a stored vocabulary from the training data n-gram feature extraction process (a good explanation of n-grams can be found in this blog on extracting features from text for classification, a different kind of ML algorithm - check it out here ). Real quick, an example of extracting n-grams from: \" Time lost is never found. \" An example from the blog link I just listed above (this one ) Where n=1, that is a uni-gram Where n=2, that is a bi-gram Where n=3, that is a tri-gram Time Time lost Time lost is lost lost is lost is never is is never is never found never never found found So, you have an idea of the initial training dataset (but imagine 10,000 or more of these n-grams as our features from all of that Wikipedia text - it can be seen why feature selection is sometimes helpful for narrowing down to the most important features and we can also do this with the Extract N-Gram Features from Text module in AML). Ok, let's move on to the app building fun. Our web app is going to utilize a microframework for building web apps purely in the Python programming language. A big reason to begin in this framework is that Python, a popular Data Science language, is easy to read and learn and Visual Studio has a Flask web app template as part of the Python Tools for Visual Studio extension, making life much easier for us. Python, as a language, is also known for being a popular web app development language and has other projects like Django and Bottle for these ends (also with templates in VS). That all being said, most of this post is about creating the Flask web app. I'll leave it to other guides and articles to discuss working with AML and k-means in detail. Above: The deployed web app site Before you Begin, a Few Things to Do... Tools [recommended] Visual Studio installed (Community 2015 Edition is what I use; NB: the preview of 2017 is adding PTVS soon...I'll update on this later; also, VS 2017 is available for Mac OSX) ( Visual Studio Community ) with Python Tools for Visual Studio installed (to get the Flask Web App template) which can be added during the install of VS or separately from here Git Bash or git installed - included in git download https://git-scm.com/downloads Accounts Azure Machine Learning Studio account from https://studio.azureml.net (free) GitHub Account - a code repository and collaboration tool we'll use (free) https://github.com/join Azure account - use the one you have, sign up for a free trial at https://azure.microsoft.com/en-us/free/ , or, if you have an MSDN account and Azure as a benefit, link your Microsoft Account or Work/School Account to MSDN and activate the Azure benefit by following this guide Prerequisites The deployed Azure Machine Learning scoring experiment Note: We won't cover this experiment and model here as it's not the focus of this particular post, but a link to instructions is just below. Aside: These experiments are often called \"predictive\", but in a clustering model we really just look for scores and cluster assignments, not predictions so let's call it scoring experiment The scoring experiment which utilizes the k-means model and n-gram featurizer vocabulary created in the training experiment has the following layout in AML Studio: The scoring experiment you will need can be found here (this will allow you to launch it in AML Studio). Essentially, we are using AML Studio as a clever way to deploy a web service and not much more, but it's capabilities as a canvas for creating a data science workflow are worth checking out if you like a visual workflow-type setup. Start at this spot in the Azure Docs to get this experiment deployed as a web service to use later in this guide post: https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-walkthrough-5-publish-web-service#deploy-the-web-service . A similar guide with good diagrams on deploying the experiment as a web service can be picked up at section \"4.2.2. Publishing a trained model as Web Service\" in this tutorial: https://github.com/Azure-Readiness/hol-azure-machine-learning/blob/master/004-lab-azureml-experiment.md#422-publishing-a-trained-model-as-web-service . The Web App Development Note: Under REQUEST/RESPONSE for the AML Studio experiment (found after deploying as web service from Studio), one will find all of the specs needed to work with this endpoint. Let's write a web app! We're going to begin in Visual Studio. As an aside, did you know VS 2017 is available for Mac?! What's especially cool is that developers can share projects across Mac and Windows. The Python Tools for Visual Studio extension isn't available, however, on VS 2017 so I'm eagerly awaiting this capability. Will report back later. Since VS 2015 with PTVS is available for us on Windows we will be using the awesome Flask Web Project template that comes with it to kick start our web app dev in Windows 10. Open VS Create a new Python Flask web app project (this template should exist if one chooses Python - scroll down Templates -> Python -> Flask Web Project) with Python 3.5 (or whichever 3 you have will do) into a virtual environment. At this point, you literally have a functioning web app. Hit the Run (your default browser choice is next to the button) in VS and test out this template. Add a new file called forms.py to the main directory (alongside views.py ). This will contain the form-building code through which data will be sent to the REST endpoint for analysis. There are three fields we need in our input form: title, category and text. Title is the company title, category is an optional field for the category of company (e.g. information technology) and text is the Wikipedia article text about that company or some descriptive corpus. The input form: define in \"forms\" Place the following text in the forms.py file: from wtforms import Form , StringField , TextAreaField , validators # This class will be used in the webapp as the main input form class SubmissionForm ( Form ): title = StringField ( 'Title' , [ validators . Length ( min = 2 , max = 30 )]) category = StringField ( 'Category' , [ validators . Length ( min = 0 , max = 30 )]) text = TextAreaField ( 'Text' , [ validators . Length ( min = 1 , max = 1000 )]) The routing on the page: define in the \"views\" We will be modifying the existing template code as follows. The imports should look like: import json import urllib.request import os from datetime import datetime from flask import render_template , request , redirect from FlaskAppAML import app from FlaskAppAML.forms import SubmissionForm Which added json handling, http request handling, os interaction and the way in which the forms class from above is available for use. Add a way to grab the API_KEY and URL at the beginning of the views.py file: # Deployment environment variables defined on Azure (pull in with os.environ) API_KEY = os . environ . get ( 'API_KEY' , \"optionally place a default value for local dev here\" ) URL = os . environ . get ( 'URL' , \"optionally place a default value for local dev here\" ) and HEADERS global variable: # Construct the HTTP request header HEADERS = { 'Content-Type' : 'application/json' , 'Authorization' :( 'Bearer ' + API_KEY )} Change the \"home route\" (landing page functionality), def home method definition, to be: # Our main app page/route @app . route ( '/' , methods = [ 'GET' , 'POST' ]) @app . route ( '/home' , methods = [ 'GET' , 'POST' ]) def home (): \"\"\"Renders the home page which is the CNS of the web app currently, nothing pretty.\"\"\" form = SubmissionForm ( request . form ) # Form has been submitted if request . method == 'POST' and form . validate (): # Plug in the data into a dictionary object # - data from the input form # - text data must be converted to lowercase data = { \"Inputs\" : { \"input1\" : { \"ColumnNames\" : [ \"Title\" , \"Category\" , \"Text\" ], \"Values\" : [ [ form . title . data , form . category . data , form . text . data . lower () ] ] } }, \"GlobalParameters\" : {} } # Serialize the input data into json string body = str . encode ( json . dumps ( data )) # Formulate the request req = urllib . request . Request ( URL , body , HEADERS ) # Send this request to the AML service and render the results on page try : # response = requests.post(URL, headers=HEADERS, data=body) response = urllib . request . urlopen ( req ) respdata = response . read () result = json . loads ( str ( respdata , 'utf-8' )) result = json . dumps ( result , indent = 4 , sort_keys = True ) return render_template ( 'result.html' , title = \"From your friendly AML experiment's Web Service:\" , result = result ) # An HTTP error except Exception as err : result = json . loads ( str ( err . code )) return render_template ( 'result.html' , title = 'There was an error' , result = result ) # Just serve up the input form return render_template ( 'form.html' , form = form , title = 'Run App' , year = datetime . now () . year , message = 'Input form to gain insights into a company using Azure Machine Learning' ) The html templates: how the information gets served We add two new templates: form.html, result.html The form.html gives us a construct for the user to enter in input data and the result.html , a construct in which the results from the machine learning experiment can be displayed. Grab the form.html code here . Grab the result.html code here . Note, this code may result in slightly different web app appearances to this article. Now that we have some new code to handle calling the AML web service and html templates to handle input and output, let's prepare to deploy by taking a look at some configuration. Prepare to Deploy the Web App to Azure Before we publish, we must add two configuration-type files: A web configuration file (web.config) Virtual environment proxy (ptvs_virtualenv_proxy.py) The web.config file may need some modifications, however the virtual environment proxy file should work as is from this folder. Web Configuration file Add web.config at project level (alongside requirements.txt file) It should look something like the following (you can actually add a template web.config similar to this one in VS by right-clicking on the FlaskAppAML folder -> Add -> New Item -> Azure web.config for FastCGI, but it will need a few modifications). Note that the Python version may change in the future and this script might need modification. <?xml version=\"1.0\"?> <configuration> <appSettings> <add key= \"WSGI_ALT_VIRTUALENV_HANDLER\" value= \"FlaskAppAML.app\" /> <add key= \"WSGI_ALT_VIRTUALENV_ACTIVATE_THIS\" value= \"D:\\home\\site\\wwwroot\\env\\Scripts\\python.exe\" /> <add key= \"WSGI_HANDLER\" value= \"ptvs_virtualenv_proxy.get_venv_handler()\" /> <add key= \"PYTHONPATH\" value= \"D:\\home\\site\\wwwroot\" /> </appSettings> <system.web> <compilation debug= \"true\" targetFramework= \"4.0\" /> </system.web> <system.webServer> <modules runAllManagedModulesForAllRequests= \"true\" /> <handlers> <remove name= \"Python27_via_FastCGI\" /> <remove name= \"Python34_via_FastCGI\" /> <add name= \"Python FastCGI\" path= \"handler.fcgi\" verb= \"*\" modules= \"FastCgiModule\" scriptProcessor= \"D:\\Python34\\python.exe|D:\\Python34\\Scripts\\wfastcgi.py\" resourceType= \"Unspecified\" requireAccess= \"Script\" /> </handlers> <rewrite> <rules> <rule name= \"Static Files\" stopProcessing= \"true\" > <match url= \"^/static/.*\" ignoreCase= \"true\" /> <action type= \"Rewrite\" url= \"^/FlaskAppAML/static/.*\" appendQueryString= \"true\" /> </rule> <rule name= \"Configure Python\" stopProcessing= \"true\" > <match url= \"(.*)\" ignoreCase= \"false\" /> <conditions> </conditions> <action type= \"Rewrite\" url= \"handler.fcgi/{R:1}\" appendQueryString= \"true\" /> </rule> </rules> </rewrite> </system.webServer> </configuration> Possible modifications or places of note in the web config: The WSGI_ALT_VIRTUALENV_HANDLER will very likely need to be modified. Here it is FlaskAppAML.app , referring to my flask application itself. The line <action type=\"Rewrite\" url=\"^/FlaskAppAML/static/.*\" appendQueryString=\"true\" /> under rules MUST have the correct project name (here mine was FlaskAppAML). This section ensures the static files (important for the web service appearance) can be found. The scriptProcessor , under handlers in the web.config xml above, must correspond to the resources existing on the web server's file system (e.g. D:\\Python34\\python.exe ). Virtual Environment Proxy The code for this set of helper functions can be found here in the Azure documentation (that similar article talks about deploying continuously from a git repository - a good method to know as well). Just include ptvs_virtualenv_proxy in the base of your project along with the web.config (and auto-created requirements.txt and runserver.py ). Finally, to test all of this code locally just click the run button in the navi in VS (your default browser should also appear there). Congrats, you have tested this locally and things seem to be good. Deploy the Flask Web App Option 1: Set up a GitHub Repository as the Deployment Option This is the most customizable way, hence more complex, but also the most transparent and easy to troubleshoot. Log in to GitHub and create a new repository (I called mine flask-webapp-aml , initializing with a README and a .gitignore for Visual Studio files. In Git bash on the Desktop, type into the terminal the command to clone the new repository, for example: (I'm using SSH because it will allow me to push changes back up): git clone git@github.com:<your github username without these triangle brackets>/flask-webapp-aml.git Copy all of the project code to this new repository folder locally (I just cp on the command line in Git bash) to match this structure: FlaskAppAML/ FlaskAppAML/__init__.py FlaskAppAML/forms.py FlaskAppAML/views.py FlaskAppAML/static -> *our static files* FlaskAppAML/templates -> *the html page templates* env/ -> *the entire python environment* ptvs_virtualenv_proxy.py README.md requirements.txt runserver.py runtime.txt web.config .skipPythonDeployment If the empty .skipPythonDeployment file is not in the base of your repository, add one now. Also, make sure the env folder from the VS project is present. This contains all of the python environment needed for running this web app (really anywhere). We are skipping having the web service custom install all of the necessary modules by giving the service this .skipPythonDeployment file and the env folder. Add \"__pycache__\" on it's own line to my \".gitignore\" file and anything you don't want uploaded to the GitHub when we \"push\" changes. Now it all seems pretty tidy, so it's time to push the changes up to be hosted on GitHub. I add (\"stage\"), commit (commit my code locally with a message) and push (push up to the web to be hosted on GitHub) all of my additions or any changes I've made. I can do this add/commit/push again as many times as I want in the future. I must, however, do all three consecutively and in that order otherwise it gets complicated. So, my commands look like: git add . (from the base of the repository) git commit -m \"initial commit message\" (the -m is our message so be brief, but descriptive - visible to the world) git push (we could also have written git push origin master , but it's not necessary to be so verbose right now) Create an App Service Web App in the Azure Portal ( https://portal.azure.com ) by clicking \"+\" and search for \"web app\", then go through the wizard to create one. Update the Deployment options in the Azure Portal for the web app. For our Web App, under \"APP DEPLOYMENT\", open the \"Deployment options\" blade. For Choose Source, choose GitHub (you may have to log in to your GitHub here to link it). Under Choose project, pick the GitHub repository to which you just pushed code and click OK. Now we add a couple of variables to the Azure Portal Web App for safe-keeping. There are \"environmental variables\" in the code (they look like os.environ.get('FOO') ): one for the AML Web Service's URL and one for the API_KEY - these are the necessary values we need to access our published AML scoring experiment. To have these available for our web app we need to put them somewhere discoverable and that is as variables under \"App settings\" in the \"Application settings\" blade for our own Web App in the Azure Portal. Ensure that, in the Portal, under Application Settings, Python is set to the appropriate version (default is that \"Python\" is Off in settings - so will need to manually switch to it's version). If we go back to the \"Deployment options\" we can see how our build is going. This process will automatically happen for us every time a new change is made to our GitHub repository. Ensure that this build completes successfully. Finally, if you go to \"Overview\" and click on the web app's URL, you'll see your site. Congrats on completing this process! You should now have a functioning barebones, machine learning web app. Go ahead and try it out. :) If you encounter any problems, check the Troubleshooting section below, Azure docs, or StackOverflow. Also, leave a comment if it's a bug in the code or process. Option 2: Publish and Deploy from VS as an Azure App Service Web App To deploy we must also publish this project to Azure (it's done together with VS). Fortunately, from within VS (note, I'm in VS 2017, but it's available in previous releases) there's a \"Publish...\" option. Right-click on the project name and in the pop-up \"Publish...\" should be available. Click this and simply go through the wizard to set up an Azure App Service Web App. It should be very straightforward and easy to do. As an alternative to publishing/deploying directly from VS, one can leverage a git repository or use code on GitHub as a deployment option. Similar instructions can be found in this Azure article. Make it Your Own Modify the layout.html file with app name and navi layout changes. Or change your custom stylesheet under static -> content -> site.css. Go grab all of the code at https://github.com/michhar/flask-webapp-aml and add it to a project, test, develop and deploy. You could even if you wish just fork this repository and deploy directly from that in the Azure Portal, but then that would have been too easy. ;) Troubleshooting All sample code can be found at https://github.com/michhar/flask-webapp-aml - it may, over time, have more complex samples, so check it out. Go to Application Settings and ensure Python is enabled along with other key settings in the Azure Portal Go to Console (under Development Tools) and make sure all files and programs specified in the web.config exists. Ensure in web.config , that the \"scriptProcessor\" key/value in handlers is correct (that these paths exist on the server file system). Check FREB Logs in the Portal for more information around warnings and errors (make sure you are logging for those during this phase). Post comments here or if around the code, under issues here: https://github.com/michhar/flask-webapp-aml/issues - many thanks!","title":"Creating a Smart Python Flask Web App using Azure Machine Learning"},{"location":"a-python-flask-webapp-gets-smart/#introduction","text":"Ok, so I have an interesting REST endpoint (in my case, a machine learning model for using a company's Wikipedia article to find similar companies), what can I do next? Why not serve it up in a simple web app to impress friends and wow colleagues? (Really, you can use this intel to create a web app around any REST endpoint, as half of my purpose in writing this is to show how fast and easy Python Flask is). Essentially, we are making a web app wrapper around a data submission and retrieval REST endpoint that is created through Azure Machine Learning (AML) Studio ( https://studio.azureml.net ), a friendly and powerful machine learning tool with a handy browser UI. In this post, the endpoint is a service that clusters companies based on descriptive text (our input data). The clustering model, a k-means algorithm, has been trained on close to 500 wikipedia entries, a cool example of unsupervised learning . If you don't know much yet about AML Studio and would like to know more this is a good place to start or dive in and learn by doing with a quick getting-started tutorial here . You'll need to know, at least, how to publish an experiment from Studio to get your Flask web app going. The ML web service is based around an AML scoring experiment, built from a training experiment in which K-Means Clustering module is used to assign companies to groups based on features in their processed Wikipedia text. The Extract N-Gram Features from Text module (more info here ) is used after some initial cleansing of the text data (remove stop words, numbers, special characters, detect sentences, etc. - see the Preprocess Text AML module here ) to extract features upon which to train the k-means clustering model and reduce the dimensionality to the most important chunks of information. The scoring experiment uses a stored vocabulary from the training data n-gram feature extraction process (a good explanation of n-grams can be found in this blog on extracting features from text for classification, a different kind of ML algorithm - check it out here ). Real quick, an example of extracting n-grams from: \" Time lost is never found. \" An example from the blog link I just listed above (this one ) Where n=1, that is a uni-gram Where n=2, that is a bi-gram Where n=3, that is a tri-gram Time Time lost Time lost is lost lost is lost is never is is never is never found never never found found So, you have an idea of the initial training dataset (but imagine 10,000 or more of these n-grams as our features from all of that Wikipedia text - it can be seen why feature selection is sometimes helpful for narrowing down to the most important features and we can also do this with the Extract N-Gram Features from Text module in AML). Ok, let's move on to the app building fun. Our web app is going to utilize a microframework for building web apps purely in the Python programming language. A big reason to begin in this framework is that Python, a popular Data Science language, is easy to read and learn and Visual Studio has a Flask web app template as part of the Python Tools for Visual Studio extension, making life much easier for us. Python, as a language, is also known for being a popular web app development language and has other projects like Django and Bottle for these ends (also with templates in VS). That all being said, most of this post is about creating the Flask web app. I'll leave it to other guides and articles to discuss working with AML and k-means in detail. Above: The deployed web app site","title":"Introduction"},{"location":"a-python-flask-webapp-gets-smart/#before-you-begin-a-few-things-to-do","text":"","title":"Before you Begin, a Few Things to Do..."},{"location":"a-python-flask-webapp-gets-smart/#tools","text":"[recommended] Visual Studio installed (Community 2015 Edition is what I use; NB: the preview of 2017 is adding PTVS soon...I'll update on this later; also, VS 2017 is available for Mac OSX) ( Visual Studio Community ) with Python Tools for Visual Studio installed (to get the Flask Web App template) which can be added during the install of VS or separately from here Git Bash or git installed - included in git download https://git-scm.com/downloads","title":"Tools"},{"location":"a-python-flask-webapp-gets-smart/#accounts","text":"Azure Machine Learning Studio account from https://studio.azureml.net (free) GitHub Account - a code repository and collaboration tool we'll use (free) https://github.com/join Azure account - use the one you have, sign up for a free trial at https://azure.microsoft.com/en-us/free/ , or, if you have an MSDN account and Azure as a benefit, link your Microsoft Account or Work/School Account to MSDN and activate the Azure benefit by following this guide","title":"Accounts"},{"location":"a-python-flask-webapp-gets-smart/#prerequisites","text":"The deployed Azure Machine Learning scoring experiment Note: We won't cover this experiment and model here as it's not the focus of this particular post, but a link to instructions is just below. Aside: These experiments are often called \"predictive\", but in a clustering model we really just look for scores and cluster assignments, not predictions so let's call it scoring experiment The scoring experiment which utilizes the k-means model and n-gram featurizer vocabulary created in the training experiment has the following layout in AML Studio: The scoring experiment you will need can be found here (this will allow you to launch it in AML Studio). Essentially, we are using AML Studio as a clever way to deploy a web service and not much more, but it's capabilities as a canvas for creating a data science workflow are worth checking out if you like a visual workflow-type setup. Start at this spot in the Azure Docs to get this experiment deployed as a web service to use later in this guide post: https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-walkthrough-5-publish-web-service#deploy-the-web-service . A similar guide with good diagrams on deploying the experiment as a web service can be picked up at section \"4.2.2. Publishing a trained model as Web Service\" in this tutorial: https://github.com/Azure-Readiness/hol-azure-machine-learning/blob/master/004-lab-azureml-experiment.md#422-publishing-a-trained-model-as-web-service .","title":"Prerequisites"},{"location":"a-python-flask-webapp-gets-smart/#the-web-app-development","text":"Note: Under REQUEST/RESPONSE for the AML Studio experiment (found after deploying as web service from Studio), one will find all of the specs needed to work with this endpoint. Let's write a web app! We're going to begin in Visual Studio. As an aside, did you know VS 2017 is available for Mac?! What's especially cool is that developers can share projects across Mac and Windows. The Python Tools for Visual Studio extension isn't available, however, on VS 2017 so I'm eagerly awaiting this capability. Will report back later. Since VS 2015 with PTVS is available for us on Windows we will be using the awesome Flask Web Project template that comes with it to kick start our web app dev in Windows 10. Open VS Create a new Python Flask web app project (this template should exist if one chooses Python - scroll down Templates -> Python -> Flask Web Project) with Python 3.5 (or whichever 3 you have will do) into a virtual environment. At this point, you literally have a functioning web app. Hit the Run (your default browser choice is next to the button) in VS and test out this template. Add a new file called forms.py to the main directory (alongside views.py ). This will contain the form-building code through which data will be sent to the REST endpoint for analysis. There are three fields we need in our input form: title, category and text. Title is the company title, category is an optional field for the category of company (e.g. information technology) and text is the Wikipedia article text about that company or some descriptive corpus.","title":"The Web App Development"},{"location":"a-python-flask-webapp-gets-smart/#the-input-form-define-in-forms","text":"Place the following text in the forms.py file: from wtforms import Form , StringField , TextAreaField , validators # This class will be used in the webapp as the main input form class SubmissionForm ( Form ): title = StringField ( 'Title' , [ validators . Length ( min = 2 , max = 30 )]) category = StringField ( 'Category' , [ validators . Length ( min = 0 , max = 30 )]) text = TextAreaField ( 'Text' , [ validators . Length ( min = 1 , max = 1000 )])","title":"The input form:  define in \"forms\""},{"location":"a-python-flask-webapp-gets-smart/#the-routing-on-the-page-define-in-the-views","text":"We will be modifying the existing template code as follows. The imports should look like: import json import urllib.request import os from datetime import datetime from flask import render_template , request , redirect from FlaskAppAML import app from FlaskAppAML.forms import SubmissionForm Which added json handling, http request handling, os interaction and the way in which the forms class from above is available for use. Add a way to grab the API_KEY and URL at the beginning of the views.py file: # Deployment environment variables defined on Azure (pull in with os.environ) API_KEY = os . environ . get ( 'API_KEY' , \"optionally place a default value for local dev here\" ) URL = os . environ . get ( 'URL' , \"optionally place a default value for local dev here\" ) and HEADERS global variable: # Construct the HTTP request header HEADERS = { 'Content-Type' : 'application/json' , 'Authorization' :( 'Bearer ' + API_KEY )} Change the \"home route\" (landing page functionality), def home method definition, to be: # Our main app page/route @app . route ( '/' , methods = [ 'GET' , 'POST' ]) @app . route ( '/home' , methods = [ 'GET' , 'POST' ]) def home (): \"\"\"Renders the home page which is the CNS of the web app currently, nothing pretty.\"\"\" form = SubmissionForm ( request . form ) # Form has been submitted if request . method == 'POST' and form . validate (): # Plug in the data into a dictionary object # - data from the input form # - text data must be converted to lowercase data = { \"Inputs\" : { \"input1\" : { \"ColumnNames\" : [ \"Title\" , \"Category\" , \"Text\" ], \"Values\" : [ [ form . title . data , form . category . data , form . text . data . lower () ] ] } }, \"GlobalParameters\" : {} } # Serialize the input data into json string body = str . encode ( json . dumps ( data )) # Formulate the request req = urllib . request . Request ( URL , body , HEADERS ) # Send this request to the AML service and render the results on page try : # response = requests.post(URL, headers=HEADERS, data=body) response = urllib . request . urlopen ( req ) respdata = response . read () result = json . loads ( str ( respdata , 'utf-8' )) result = json . dumps ( result , indent = 4 , sort_keys = True ) return render_template ( 'result.html' , title = \"From your friendly AML experiment's Web Service:\" , result = result ) # An HTTP error except Exception as err : result = json . loads ( str ( err . code )) return render_template ( 'result.html' , title = 'There was an error' , result = result ) # Just serve up the input form return render_template ( 'form.html' , form = form , title = 'Run App' , year = datetime . now () . year , message = 'Input form to gain insights into a company using Azure Machine Learning' )","title":"The routing on the page:  define in the \"views\""},{"location":"a-python-flask-webapp-gets-smart/#the-html-templates-how-the-information-gets-served","text":"We add two new templates: form.html, result.html The form.html gives us a construct for the user to enter in input data and the result.html , a construct in which the results from the machine learning experiment can be displayed. Grab the form.html code here . Grab the result.html code here . Note, this code may result in slightly different web app appearances to this article. Now that we have some new code to handle calling the AML web service and html templates to handle input and output, let's prepare to deploy by taking a look at some configuration.","title":"The html templates:  how the information gets served"},{"location":"a-python-flask-webapp-gets-smart/#prepare-to-deploy-the-web-app-to-azure","text":"Before we publish, we must add two configuration-type files: A web configuration file (web.config) Virtual environment proxy (ptvs_virtualenv_proxy.py) The web.config file may need some modifications, however the virtual environment proxy file should work as is from this folder.","title":"Prepare to Deploy the Web App to Azure"},{"location":"a-python-flask-webapp-gets-smart/#web-configuration-file","text":"Add web.config at project level (alongside requirements.txt file) It should look something like the following (you can actually add a template web.config similar to this one in VS by right-clicking on the FlaskAppAML folder -> Add -> New Item -> Azure web.config for FastCGI, but it will need a few modifications). Note that the Python version may change in the future and this script might need modification. <?xml version=\"1.0\"?> <configuration> <appSettings> <add key= \"WSGI_ALT_VIRTUALENV_HANDLER\" value= \"FlaskAppAML.app\" /> <add key= \"WSGI_ALT_VIRTUALENV_ACTIVATE_THIS\" value= \"D:\\home\\site\\wwwroot\\env\\Scripts\\python.exe\" /> <add key= \"WSGI_HANDLER\" value= \"ptvs_virtualenv_proxy.get_venv_handler()\" /> <add key= \"PYTHONPATH\" value= \"D:\\home\\site\\wwwroot\" /> </appSettings> <system.web> <compilation debug= \"true\" targetFramework= \"4.0\" /> </system.web> <system.webServer> <modules runAllManagedModulesForAllRequests= \"true\" /> <handlers> <remove name= \"Python27_via_FastCGI\" /> <remove name= \"Python34_via_FastCGI\" /> <add name= \"Python FastCGI\" path= \"handler.fcgi\" verb= \"*\" modules= \"FastCgiModule\" scriptProcessor= \"D:\\Python34\\python.exe|D:\\Python34\\Scripts\\wfastcgi.py\" resourceType= \"Unspecified\" requireAccess= \"Script\" /> </handlers> <rewrite> <rules> <rule name= \"Static Files\" stopProcessing= \"true\" > <match url= \"^/static/.*\" ignoreCase= \"true\" /> <action type= \"Rewrite\" url= \"^/FlaskAppAML/static/.*\" appendQueryString= \"true\" /> </rule> <rule name= \"Configure Python\" stopProcessing= \"true\" > <match url= \"(.*)\" ignoreCase= \"false\" /> <conditions> </conditions> <action type= \"Rewrite\" url= \"handler.fcgi/{R:1}\" appendQueryString= \"true\" /> </rule> </rules> </rewrite> </system.webServer> </configuration> Possible modifications or places of note in the web config: The WSGI_ALT_VIRTUALENV_HANDLER will very likely need to be modified. Here it is FlaskAppAML.app , referring to my flask application itself. The line <action type=\"Rewrite\" url=\"^/FlaskAppAML/static/.*\" appendQueryString=\"true\" /> under rules MUST have the correct project name (here mine was FlaskAppAML). This section ensures the static files (important for the web service appearance) can be found. The scriptProcessor , under handlers in the web.config xml above, must correspond to the resources existing on the web server's file system (e.g. D:\\Python34\\python.exe ).","title":"Web Configuration file"},{"location":"a-python-flask-webapp-gets-smart/#virtual-environment-proxy","text":"The code for this set of helper functions can be found here in the Azure documentation (that similar article talks about deploying continuously from a git repository - a good method to know as well). Just include ptvs_virtualenv_proxy in the base of your project along with the web.config (and auto-created requirements.txt and runserver.py ). Finally, to test all of this code locally just click the run button in the navi in VS (your default browser should also appear there). Congrats, you have tested this locally and things seem to be good.","title":"Virtual Environment Proxy"},{"location":"a-python-flask-webapp-gets-smart/#deploy-the-flask-web-app","text":"","title":"Deploy the Flask Web App"},{"location":"a-python-flask-webapp-gets-smart/#option-1-set-up-a-github-repository-as-the-deployment-option","text":"This is the most customizable way, hence more complex, but also the most transparent and easy to troubleshoot. Log in to GitHub and create a new repository (I called mine flask-webapp-aml , initializing with a README and a .gitignore for Visual Studio files. In Git bash on the Desktop, type into the terminal the command to clone the new repository, for example: (I'm using SSH because it will allow me to push changes back up): git clone git@github.com:<your github username without these triangle brackets>/flask-webapp-aml.git Copy all of the project code to this new repository folder locally (I just cp on the command line in Git bash) to match this structure: FlaskAppAML/ FlaskAppAML/__init__.py FlaskAppAML/forms.py FlaskAppAML/views.py FlaskAppAML/static -> *our static files* FlaskAppAML/templates -> *the html page templates* env/ -> *the entire python environment* ptvs_virtualenv_proxy.py README.md requirements.txt runserver.py runtime.txt web.config .skipPythonDeployment If the empty .skipPythonDeployment file is not in the base of your repository, add one now. Also, make sure the env folder from the VS project is present. This contains all of the python environment needed for running this web app (really anywhere). We are skipping having the web service custom install all of the necessary modules by giving the service this .skipPythonDeployment file and the env folder. Add \"__pycache__\" on it's own line to my \".gitignore\" file and anything you don't want uploaded to the GitHub when we \"push\" changes. Now it all seems pretty tidy, so it's time to push the changes up to be hosted on GitHub. I add (\"stage\"), commit (commit my code locally with a message) and push (push up to the web to be hosted on GitHub) all of my additions or any changes I've made. I can do this add/commit/push again as many times as I want in the future. I must, however, do all three consecutively and in that order otherwise it gets complicated. So, my commands look like: git add . (from the base of the repository) git commit -m \"initial commit message\" (the -m is our message so be brief, but descriptive - visible to the world) git push (we could also have written git push origin master , but it's not necessary to be so verbose right now) Create an App Service Web App in the Azure Portal ( https://portal.azure.com ) by clicking \"+\" and search for \"web app\", then go through the wizard to create one. Update the Deployment options in the Azure Portal for the web app. For our Web App, under \"APP DEPLOYMENT\", open the \"Deployment options\" blade. For Choose Source, choose GitHub (you may have to log in to your GitHub here to link it). Under Choose project, pick the GitHub repository to which you just pushed code and click OK. Now we add a couple of variables to the Azure Portal Web App for safe-keeping. There are \"environmental variables\" in the code (they look like os.environ.get('FOO') ): one for the AML Web Service's URL and one for the API_KEY - these are the necessary values we need to access our published AML scoring experiment. To have these available for our web app we need to put them somewhere discoverable and that is as variables under \"App settings\" in the \"Application settings\" blade for our own Web App in the Azure Portal. Ensure that, in the Portal, under Application Settings, Python is set to the appropriate version (default is that \"Python\" is Off in settings - so will need to manually switch to it's version). If we go back to the \"Deployment options\" we can see how our build is going. This process will automatically happen for us every time a new change is made to our GitHub repository. Ensure that this build completes successfully. Finally, if you go to \"Overview\" and click on the web app's URL, you'll see your site. Congrats on completing this process! You should now have a functioning barebones, machine learning web app. Go ahead and try it out. :) If you encounter any problems, check the Troubleshooting section below, Azure docs, or StackOverflow. Also, leave a comment if it's a bug in the code or process.","title":"Option 1: Set up a GitHub Repository as the Deployment Option"},{"location":"a-python-flask-webapp-gets-smart/#option-2-publish-and-deploy-from-vs-as-an-azure-app-service-web-app","text":"To deploy we must also publish this project to Azure (it's done together with VS). Fortunately, from within VS (note, I'm in VS 2017, but it's available in previous releases) there's a \"Publish...\" option. Right-click on the project name and in the pop-up \"Publish...\" should be available. Click this and simply go through the wizard to set up an Azure App Service Web App. It should be very straightforward and easy to do. As an alternative to publishing/deploying directly from VS, one can leverage a git repository or use code on GitHub as a deployment option. Similar instructions can be found in this Azure article.","title":"Option 2: Publish and Deploy from VS as an Azure App Service Web App"},{"location":"a-python-flask-webapp-gets-smart/#make-it-your-own","text":"Modify the layout.html file with app name and navi layout changes. Or change your custom stylesheet under static -> content -> site.css. Go grab all of the code at https://github.com/michhar/flask-webapp-aml and add it to a project, test, develop and deploy. You could even if you wish just fork this repository and deploy directly from that in the Azure Portal, but then that would have been too easy. ;)","title":"Make it Your Own"},{"location":"a-python-flask-webapp-gets-smart/#troubleshooting","text":"All sample code can be found at https://github.com/michhar/flask-webapp-aml - it may, over time, have more complex samples, so check it out. Go to Application Settings and ensure Python is enabled along with other key settings in the Azure Portal Go to Console (under Development Tools) and make sure all files and programs specified in the web.config exists. Ensure in web.config , that the \"scriptProcessor\" key/value in handlers is correct (that these paths exist on the server file system). Check FREB Logs in the Portal for more information around warnings and errors (make sure you are logging for those during this phase). Post comments here or if around the code, under issues here: https://github.com/michhar/flask-webapp-aml/issues - many thanks!","title":"Troubleshooting"},{"location":"backprop-dl-frameworks/","text":"tl;dr : Posted: 2018-04-22 Introduction A rather lengthy introduction and code sample for back-propagation is written in NumPy below to set the stage for all future work in deep learning frameworks. Then, how several deep learning frameworks perform/think about back-propagation follows. I've found, recently, that the Sequential class in Keras and PyTorch are very similar to the Layer or Layers APIs in CNTK and TensorFlow - perhaps Sequential is a little bit higher-level so it depends on how much customizability you want, as usual in these cases. Below you will see examples of the the same CNN architecture in the four different frameworks along with their back-propagation code. NumPy for Comparison This will set the stage for working with deep learning frameworks such as TensorFlow and PyTorch. NumPy is the currency of the data used in these frameworks. It is good to have a solid grasp on backpropagation and for a deeper explanation see Wikipedia . To quote a good article that can say this better than me: \"The goal of back-propagation training is to minimize the squared error. To do that, the gradient of the error function must be calculated. The gradient is a calculus derivative with a value like +1.23 or -0.33. The sign of the gradient tells you whether to increase or decrease the weights and biases in order to reduce error. The magnitude of the gradient is used, along with the learning rate, to determine how much to increase or decrease the weights and biases. Using some very clever mathematics, you can compute the gradient.\" The Neural Net class uses the following to kick-off the back propagation calculation (taken from this Code ). Before anything a few variables were set up in the NeuralNetwork class. # Number of input, hidden and output nodes respectively self . ni = numInput self . nh = numHidden self . no = numOutput # The values on the nodes self . iNodes = np . zeros ( shape = [ self . ni ], dtype = np . float32 ) self . hNodes = np . zeros ( shape = [ self . nh ], dtype = np . float32 ) self . oNodes = np . zeros ( shape = [ self . no ], dtype = np . float32 ) # The weight matrices self . ihWeights = np . zeros ( shape = [ self . ni , self . nh ], dtype = np . float32 ) self . hoWeights = np . zeros ( shape = [ self . nh , self . no ], dtype = np . float32 ) # The bias matrices self . hBiases = np . zeros ( shape = [ self . nh ], dtype = np . float32 ) self . oBiases = np . zeros ( shape = [ self . no ], dtype = np . float32 ) Part of the training code to initialize the gradients and signals (like gradients without their input terms) and looks like the following: def train ( self , trainData , maxEpochs , learnRate ): ... hoGrads = np . zeros ( shape = [ self . nh , self . no ], dtype = np . float32 ) # hidden-to-output weights gradients obGrads = np . zeros ( shape = [ self . no ], dtype = np . float32 ) # output node biases gradients ihGrads = np . zeros ( shape = [ self . ni , self . nh ], dtype = np . float32 ) # input-to-hidden weights gradients hbGrads = np . zeros ( shape = [ self . nh ], dtype = np . float32 ) # hidden biases gradients oSignals = np . zeros ( shape = [ self . no ], dtype = np . float32 ) # output signals: gradients w/o assoc. input terms hSignals = np . zeros ( shape = [ self . nh ], dtype = np . float32 ) # hidden signals: gradients w/o assoc. input terms ... Pro tip: \"When working with neural networks, it's common, but not required, to work with the float32 rather than float64 data type\" - Lee Stott Now, followed with these arrays which will hold the signals (remember these are gradients without their input terms mainly for convenience) ( oSignals the one from output to hidden and hSignals the hidden to input layer). oSignals = np . zeros ( shape = [ self . no ], dtype = np . float32 ) hSignals = np . zeros ( shape = [ self . nh ], dtype = np . float32 ) The calculation of the gradients, or amount used for the weight updates, are the steps as follows: Compute output node signals (an intermediate value) Compute hidden-to-output weight gradients using output signals Compute output node bias gradients using output signals Compute hidden node signals Compute input-to-hidden weight gradients using hidden signals Compute hidden node bias gradients using hidden signals # In setting up the training we had two variables x_values = np . zeros ( shape = [ self . ni ], dtype = np . float32 ) # Expected values (training labels) t_values = np . zeros ( shape = [ self . no ], dtype = np . float32 ) # 1. compute output node signals (an intermediate value) for k in range ( self . no ): derivative = ( 1 - self . oNodes [ k ]) * self . oNodes [ k ] # softmax oSignals [ k ] = derivative * ( self . oNodes [ k ] - t_values [ k ]) # E=(t-o)^2 do E'=(o-t) # 2. compute hidden-to-output weight gradients using output signals for j in range ( self . nh ): for k in range ( self . no ): hoGrads [ j , k ] = oSignals [ k ] * self . hNodes [ j ] # 3. compute output node bias gradients using output signals for k in range ( self . no ): obGrads [ k ] = oSignals [ k ] * 1.0 # 1.0 dummy input can be dropped # 4. compute hidden node signals for j in range ( self . nh ): sum = 0.0 for k in range ( self . no ): sum += oSignals [ k ] * self . hoWeights [ j , k ] derivative = ( 1 - self . hNodes [ j ]) * ( 1 + self . hNodes [ j ]) # tanh activation hSignals [ j ] = derivative * sum # 5 compute input-to-hidden weight gradients using hidden signals for i in range ( self . ni ): for j in range ( self . nh ): ihGrads [ i , j ] = hSignals [ j ] * self . iNodes [ i ] # 6. compute hidden node bias gradients using hidden signals for j in range ( self . nh ): hbGrads [ j ] = hSignals [ j ] * 1.0 # 1.0 dummy input can be dropped # update weights and biases using the gradients ... Updating the weight matrix now with these gradients will be left up to the reader (or use that code sample link above). The Code Speed Conclusion References Thanks for reading.","title":"Back-Propagation in Deep Learning Frameworks"},{"location":"backprop-dl-frameworks/#introduction","text":"A rather lengthy introduction and code sample for back-propagation is written in NumPy below to set the stage for all future work in deep learning frameworks. Then, how several deep learning frameworks perform/think about back-propagation follows. I've found, recently, that the Sequential class in Keras and PyTorch are very similar to the Layer or Layers APIs in CNTK and TensorFlow - perhaps Sequential is a little bit higher-level so it depends on how much customizability you want, as usual in these cases. Below you will see examples of the the same CNN architecture in the four different frameworks along with their back-propagation code.","title":"Introduction"},{"location":"backprop-dl-frameworks/#numpy-for-comparison","text":"This will set the stage for working with deep learning frameworks such as TensorFlow and PyTorch. NumPy is the currency of the data used in these frameworks. It is good to have a solid grasp on backpropagation and for a deeper explanation see Wikipedia . To quote a good article that can say this better than me: \"The goal of back-propagation training is to minimize the squared error. To do that, the gradient of the error function must be calculated. The gradient is a calculus derivative with a value like +1.23 or -0.33. The sign of the gradient tells you whether to increase or decrease the weights and biases in order to reduce error. The magnitude of the gradient is used, along with the learning rate, to determine how much to increase or decrease the weights and biases. Using some very clever mathematics, you can compute the gradient.\" The Neural Net class uses the following to kick-off the back propagation calculation (taken from this Code ). Before anything a few variables were set up in the NeuralNetwork class. # Number of input, hidden and output nodes respectively self . ni = numInput self . nh = numHidden self . no = numOutput # The values on the nodes self . iNodes = np . zeros ( shape = [ self . ni ], dtype = np . float32 ) self . hNodes = np . zeros ( shape = [ self . nh ], dtype = np . float32 ) self . oNodes = np . zeros ( shape = [ self . no ], dtype = np . float32 ) # The weight matrices self . ihWeights = np . zeros ( shape = [ self . ni , self . nh ], dtype = np . float32 ) self . hoWeights = np . zeros ( shape = [ self . nh , self . no ], dtype = np . float32 ) # The bias matrices self . hBiases = np . zeros ( shape = [ self . nh ], dtype = np . float32 ) self . oBiases = np . zeros ( shape = [ self . no ], dtype = np . float32 ) Part of the training code to initialize the gradients and signals (like gradients without their input terms) and looks like the following: def train ( self , trainData , maxEpochs , learnRate ): ... hoGrads = np . zeros ( shape = [ self . nh , self . no ], dtype = np . float32 ) # hidden-to-output weights gradients obGrads = np . zeros ( shape = [ self . no ], dtype = np . float32 ) # output node biases gradients ihGrads = np . zeros ( shape = [ self . ni , self . nh ], dtype = np . float32 ) # input-to-hidden weights gradients hbGrads = np . zeros ( shape = [ self . nh ], dtype = np . float32 ) # hidden biases gradients oSignals = np . zeros ( shape = [ self . no ], dtype = np . float32 ) # output signals: gradients w/o assoc. input terms hSignals = np . zeros ( shape = [ self . nh ], dtype = np . float32 ) # hidden signals: gradients w/o assoc. input terms ... Pro tip: \"When working with neural networks, it's common, but not required, to work with the float32 rather than float64 data type\" - Lee Stott Now, followed with these arrays which will hold the signals (remember these are gradients without their input terms mainly for convenience) ( oSignals the one from output to hidden and hSignals the hidden to input layer). oSignals = np . zeros ( shape = [ self . no ], dtype = np . float32 ) hSignals = np . zeros ( shape = [ self . nh ], dtype = np . float32 ) The calculation of the gradients, or amount used for the weight updates, are the steps as follows: Compute output node signals (an intermediate value) Compute hidden-to-output weight gradients using output signals Compute output node bias gradients using output signals Compute hidden node signals Compute input-to-hidden weight gradients using hidden signals Compute hidden node bias gradients using hidden signals # In setting up the training we had two variables x_values = np . zeros ( shape = [ self . ni ], dtype = np . float32 ) # Expected values (training labels) t_values = np . zeros ( shape = [ self . no ], dtype = np . float32 ) # 1. compute output node signals (an intermediate value) for k in range ( self . no ): derivative = ( 1 - self . oNodes [ k ]) * self . oNodes [ k ] # softmax oSignals [ k ] = derivative * ( self . oNodes [ k ] - t_values [ k ]) # E=(t-o)^2 do E'=(o-t) # 2. compute hidden-to-output weight gradients using output signals for j in range ( self . nh ): for k in range ( self . no ): hoGrads [ j , k ] = oSignals [ k ] * self . hNodes [ j ] # 3. compute output node bias gradients using output signals for k in range ( self . no ): obGrads [ k ] = oSignals [ k ] * 1.0 # 1.0 dummy input can be dropped # 4. compute hidden node signals for j in range ( self . nh ): sum = 0.0 for k in range ( self . no ): sum += oSignals [ k ] * self . hoWeights [ j , k ] derivative = ( 1 - self . hNodes [ j ]) * ( 1 + self . hNodes [ j ]) # tanh activation hSignals [ j ] = derivative * sum # 5 compute input-to-hidden weight gradients using hidden signals for i in range ( self . ni ): for j in range ( self . nh ): ihGrads [ i , j ] = hSignals [ j ] * self . iNodes [ i ] # 6. compute hidden node bias gradients using hidden signals for j in range ( self . nh ): hbGrads [ j ] = hSignals [ j ] * 1.0 # 1.0 dummy input can be dropped # update weights and biases using the gradients ... Updating the weight matrix now with these gradients will be left up to the reader (or use that code sample link above).","title":"NumPy for Comparison"},{"location":"backprop-dl-frameworks/#the-code","text":"","title":"The Code"},{"location":"backprop-dl-frameworks/#speed","text":"","title":"Speed"},{"location":"backprop-dl-frameworks/#conclusion","text":"","title":"Conclusion"},{"location":"backprop-dl-frameworks/#references","text":"Thanks for reading.","title":"References"},{"location":"bilstm-crf-this-is-mind-bending/","text":"tl;dr : The Bidirectional LSTM (Bi-LSTM) is trained on both past as well as future information from the given data. This will make more sense shortly. Posted: 2019-04-03 Outline Definitions Bi-LSTM Named Entity Recognition Task CRF and potentials Viterbi Definitions Bi-LSTM (Bidirectional-Long Short-Term Memory) As you may know an LSTM addresses the vanishing gradient problem of the generic RNN by adding cell state and more non-linear activation function layers to pass on or attenuate signals to varying degrees. However, the main limitation of an LSTM is that it can only account for context from the past , that is, the hidden state, h_t, takes only past information as input. Named Entity Recognition Task For the task of Named Entity Recognition (NER) it is helpful to have context from past as well as the future, or left and right contexts. This can be addressed with a Bi-LSTM which is two LSTMs, one processing information in a forward fashion and another LSTM that processes the sequences in a reverse fashion giving the future context. That second LSTM is just reading the sentence in reverse. The hidden states from both LSTMs are then concatenated into a final output layer or vector. Conditional Random Field We don't have to stop at the output vector from the Bi-LSTM! We're not at our tag for the entity, yet. We need to understand costs of moving from one tag to the next (or staying put on a tag, even). In a CRF, we have the concept of a transition matrix which is the costs associated with transitioning from one tag to another - a transition matrix is calculated/trained for each time step. It is used in the determination of the best path through all potential sequences. Say B is the tag for the beginning of an entity, I signifies that we are inside an entity (will follow a B ) and O means we are outside an entity. Next, is an example of B-I-O scheme labeling for finding nouns in a sentence (by the way, there are a myriad of other schemes out there - see Referenes for some more). Word Scheme Tag She B was O born O in O North B Carolina I but O grew O up O in O Texas B Let's look at the transition matrix for the costs of moving from one tag (using our B-I-O scheme) to the next (remember our Bi-LSTM is understanding both the forward and reverse ordering to get more accurate boundaries for the named entities). The mathematical derivations for calculating this matrix and decoding it is beyond the scope of this post, however if you wish to learn more see this article. Viterbi Algorithm If each Bi-LSTM instance (time step) has an associated output feature map and CRF transition and emission values, then each of these time step outputs will need to be decoded into a path through potential tags and a final score determined. This is the purpose of the Viterbi algorithm, here, which is commonly used in conjunction with CRFs. Specifically, the Viterbi algorithm finds the optimal path through a sequence given a cost function by tracing backwards through a graph of all possible paths. There are computational tricks to finding this path in the high dimensional space and you can find out more in the PyTorch tutorial code link below ( _forward_backwards_trick ). Here, let's see a simple example of just the Viterbi algorithm. The simplicity of Viterbi is that at each time step, it \"looks to the left\" to find that best path and then moves to the right, repeating this \"look to the left\" until a \"survivor path\" or optimal path is found with the last column being the possible tags. The score may also be found by tracing backwards along this path and using the metric decided upon. In this example the optimal score (via a metric) is the lowest one, however, one could also look for the highest scoring path if another metric is used as is shown next. Getting more realistic... With regards to our NER work here, below is an example of a \"survivor\" path within the context of the linear-CRF where we are trying to find the highest scoring path through a sequence (giving us the tags and final score). The transition matrix values are represented by the arrows and a sequence vector is shown as part of the overall cost function. Putting it All Together Here we have word embeddings as the data for the forward and reverse LSTMs. The resulting forward vector (V_f) and backwards vector (V_b or Output layer, here) are concatenated into a final vector (V_o) that feeds into the CRF layer and is decoded via the Viterbi algorithm (part of CRF layer) into the optimal tag for that input. Note, the initial values for the Hidden inputs for each LSTM (forward and reverse) are often a vector of random numbers. Reference For a more in-depth discussion, see this excellent post describing the Bi-LSTM, CRF and usage of the Viterbi Algorithm (among other NER concepts and equations): Reference . Code See this PyTorch official Tutorial Link for the code and good explanations. References Understanding Bidirectional RNN in PyTorch Conditional Random Field Tutorial in PyTorch Character-level neural network for biomedical named entity recognition Other named entity tag schemes /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */ var disqus_config = function () { this.page.url = 'https://michhar.github.io/bilstm-crf-this-is-mind-bending/'; // Replace PAGE_URL with your page's canonical URL variable this.page.identifier = 'happycat1'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable }; (function() { // DON'T EDIT BELOW THIS LINE var d = document, s = d.createElement('script'); s.src = 'https://michhar.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); Please enable JavaScript to view the comments powered by Disqus.","title":"Named Entity Recognition using a Bi-LSTM with the Conditional Random Field Algorithm"},{"location":"bilstm-crf-this-is-mind-bending/#outline","text":"Definitions Bi-LSTM Named Entity Recognition Task CRF and potentials Viterbi","title":"Outline"},{"location":"bilstm-crf-this-is-mind-bending/#definitions","text":"","title":"Definitions"},{"location":"bilstm-crf-this-is-mind-bending/#bi-lstm-bidirectional-long-short-term-memory","text":"As you may know an LSTM addresses the vanishing gradient problem of the generic RNN by adding cell state and more non-linear activation function layers to pass on or attenuate signals to varying degrees. However, the main limitation of an LSTM is that it can only account for context from the past , that is, the hidden state, h_t, takes only past information as input.","title":"Bi-LSTM (Bidirectional-Long Short-Term Memory)"},{"location":"bilstm-crf-this-is-mind-bending/#named-entity-recognition-task","text":"For the task of Named Entity Recognition (NER) it is helpful to have context from past as well as the future, or left and right contexts. This can be addressed with a Bi-LSTM which is two LSTMs, one processing information in a forward fashion and another LSTM that processes the sequences in a reverse fashion giving the future context. That second LSTM is just reading the sentence in reverse. The hidden states from both LSTMs are then concatenated into a final output layer or vector.","title":"Named Entity Recognition Task"},{"location":"bilstm-crf-this-is-mind-bending/#conditional-random-field","text":"We don't have to stop at the output vector from the Bi-LSTM! We're not at our tag for the entity, yet. We need to understand costs of moving from one tag to the next (or staying put on a tag, even). In a CRF, we have the concept of a transition matrix which is the costs associated with transitioning from one tag to another - a transition matrix is calculated/trained for each time step. It is used in the determination of the best path through all potential sequences. Say B is the tag for the beginning of an entity, I signifies that we are inside an entity (will follow a B ) and O means we are outside an entity. Next, is an example of B-I-O scheme labeling for finding nouns in a sentence (by the way, there are a myriad of other schemes out there - see Referenes for some more). Word Scheme Tag She B was O born O in O North B Carolina I but O grew O up O in O Texas B Let's look at the transition matrix for the costs of moving from one tag (using our B-I-O scheme) to the next (remember our Bi-LSTM is understanding both the forward and reverse ordering to get more accurate boundaries for the named entities). The mathematical derivations for calculating this matrix and decoding it is beyond the scope of this post, however if you wish to learn more see this article.","title":"Conditional Random Field"},{"location":"bilstm-crf-this-is-mind-bending/#viterbi-algorithm","text":"If each Bi-LSTM instance (time step) has an associated output feature map and CRF transition and emission values, then each of these time step outputs will need to be decoded into a path through potential tags and a final score determined. This is the purpose of the Viterbi algorithm, here, which is commonly used in conjunction with CRFs. Specifically, the Viterbi algorithm finds the optimal path through a sequence given a cost function by tracing backwards through a graph of all possible paths. There are computational tricks to finding this path in the high dimensional space and you can find out more in the PyTorch tutorial code link below ( _forward_backwards_trick ). Here, let's see a simple example of just the Viterbi algorithm. The simplicity of Viterbi is that at each time step, it \"looks to the left\" to find that best path and then moves to the right, repeating this \"look to the left\" until a \"survivor path\" or optimal path is found with the last column being the possible tags. The score may also be found by tracing backwards along this path and using the metric decided upon. In this example the optimal score (via a metric) is the lowest one, however, one could also look for the highest scoring path if another metric is used as is shown next. Getting more realistic... With regards to our NER work here, below is an example of a \"survivor\" path within the context of the linear-CRF where we are trying to find the highest scoring path through a sequence (giving us the tags and final score). The transition matrix values are represented by the arrows and a sequence vector is shown as part of the overall cost function.","title":"Viterbi Algorithm"},{"location":"bilstm-crf-this-is-mind-bending/#putting-it-all-together","text":"Here we have word embeddings as the data for the forward and reverse LSTMs. The resulting forward vector (V_f) and backwards vector (V_b or Output layer, here) are concatenated into a final vector (V_o) that feeds into the CRF layer and is decoded via the Viterbi algorithm (part of CRF layer) into the optimal tag for that input. Note, the initial values for the Hidden inputs for each LSTM (forward and reverse) are often a vector of random numbers. Reference For a more in-depth discussion, see this excellent post describing the Bi-LSTM, CRF and usage of the Viterbi Algorithm (among other NER concepts and equations): Reference .","title":"Putting it All Together"},{"location":"bilstm-crf-this-is-mind-bending/#code","text":"See this PyTorch official Tutorial Link for the code and good explanations.","title":"Code"},{"location":"bilstm-crf-this-is-mind-bending/#references","text":"Understanding Bidirectional RNN in PyTorch Conditional Random Field Tutorial in PyTorch Character-level neural network for biomedical named entity recognition Other named entity tag schemes /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */ var disqus_config = function () { this.page.url = 'https://michhar.github.io/bilstm-crf-this-is-mind-bending/'; // Replace PAGE_URL with your page's canonical URL variable this.page.identifier = 'happycat1'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable }; (function() { // DON'T EDIT BELOW THIS LINE var d = document, s = d.createElement('script'); s.src = 'https://michhar.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); Please enable JavaScript to view the comments powered by Disqus.","title":"References"},{"location":"cntk-has-feelings-too/","text":"Posted: 2017-12-12 Some Background CNTK The original name for Microsoft's CNTK was the Computational Network Toolkit , now known today simply as the Cognitive Toolkit , still abbreviated CNTK for short. It was orignally written and offered up as a C++ package and now has Python bindings, making it much more widely adoptable. In its original words: [CNTK is] a unified deep-learning toolkit that describes neural networks as a series of computational steps via a directed graph It was first open-sourced in April of 2015 with intended use for researchers and protoypers using GPUs for accelerated matrix calculations, much of what deep learning is built upon these days. Interestingly, TensorFlow has its initial public release in November of 2015. Of note, 2015 was also a good year for Microsoft Research in the computer vision space as they won the ImageNet challenge that December using this toolkit and a 152-layer deep neural network. Since the beginning CNTK has been available for Linux and Windows. We will be using a Linux Docker image in a minute. FER Faces Data This data comes via a Kaggle competition on facial expressions found here . The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression in to one of two categories, Happy or Sad. That being said this dataset has images (all but two of interest were dropped in this analysis) for a total of 6 emotions: Angry, Disgust, Fear, Happy, Sad, Surprise, and Neutral. How I set things up Since I'm on a Mac, I chose to use the Docker image of CNTK (instructions found here ). This pulls down an image of a pre-created system and I run it in my own Docker container basically recreating an Ubuntu setup with CNTK locally. It's pretty neat! And then I can run Jupyter notebooks on my system and pull in local files and write out data as needed. Let me show you how and then after we'll talk about the CNNs. By following this Doc I got a Jupyter notebook up and running with CNTK with all of the Tutorial notebooks at the ready and the ability to upload or create new ones as needed. I ran these commands to get a Jupyter notebook set up with CNTK (v2.1 used here). An important note: in the run command for Docker I mounted a volume with \"$PWD/data:/data\" . This \"/data\" folder can be accessed from the Jupyter notebook, as you can see if you check them out (link below), but also used to add data or pull data from the docker container just like any file folder on your system. A very handy trick! docker pull microsoft/cntk:2.1-cpu-python3.5 docker run -d --volume \"$PWD/data:/data\" -p 8888:8888 --name cntk-jupyter-notebooks -t microsoft/cntk:2.1-cpu-python3.5 docker exec -it cntk-jupyter-notebooks bash -c \"source /cntk/activate-cntk && jupyter-notebook --no-browser --port=8888 --ip=0.0.0.0 --notebook-dir=/cntk/Tutorials --allow-root\" What I Did In a Nutshell So, I tried many different CNN network architectures (simple three-layer CNN, ones with pooling and dropout layers, etc.) and several hyperparameter combinations (minibatch sizes for training, learning rate, etc.). Here are just a few basic improvement tips for image classification based on what I found and a bit on how to do this with CNTK, plus my results - my hope is that you can do this on your own with some tips to start or maybe add to your workflow. The Jupyter notebooks associated with this post can be found on GitHub here Positive Results Improvement #1: Use sufficient data I began this journey with about 1000 images that I sorted and curated into a \"sad\" or \"happy\" bucket - using the PICS 2D face sets . I even got up to 72% accuracy with this dataset and a CNN with pooling. Then I discovered a Kaggle competition from about 5 years ago dealing with recognizing facial expressions - Challenges in Representation Learning: Facial Expression Recognition Challenge and this provided me with about 13500 training images and 2300 test images of the \"happy\" or \"sad\" kind (there were also images for categories: angry, disgust, fear, surprise and neutral in case you wish to try these out). Improvement #2: Intensity normalization I read about the sensitivity of CNNs to certain normalizations. So, I decided to try a recommended pixel intensity normalization ( Ref to nice StackOverflow hint ). This update resulted in about a 5% increase in accuracy on the held-out test set. My code looked like: def normalize ( arr ): \"\"\" Linear normalization http://en.wikipedia.org/wiki/Normalization_%28image_processing%29 \"\"\" arr = arr . astype ( 'float' ) minval = arr . min () maxval = arr . max () if minval != maxval : arr -= minval arr *= ( 255.0 / ( maxval - minval )) return arr Improvement #3: Adjust or even search hyperparameter space Play with the hyperparameters. I had a code cell full of my hyperparameters, e.g. learning rate and minibatch sizes. I varied one or two at a time. Just as an example, I took the learning rate down 10x (from 0.2 to 0.02) and my resulting accuracy increased approximately 5%. Using the CNTK train package one can design a \"trainer\" object encapsulating all training tasks and include parameter ranges inside it. I also played with the strides and filter sizes or receptive fields in my CNN for the convolutional and pooling layers (see the Stanford CV course notes for great explanations of pretty much anything CNN and much more). My big tip if you are starting out is simply to tune these adjustable parameters yourself and see what happens. Improvement #4: Add more layers (but be careful of overfitting) Interestingly, layering in three pooling layers in between the convolutional layers (last one before the dense output layer) resulted in about another 5% jump in accuracy on the test set. (more on CNNs in the Stanford course notes ) Here's what the relatively simple model looked like in the CNTK Python API: def create_model ( features ): with C . layers . default_options ( init = C . glorot_uniform (), activation = C . relu ): model = C . layers . Sequential ([ C . layers . For ( range ( 3 ), lambda i : [ C . layers . Convolution2D ( filter_shape = ( 5 , 5 ), num_filters = [ 8 , 16 , 16 ][ i ], pad = True , strides = [( 1 , 1 ), ( 1 , 1 ), ( 2 , 2 )][ i ]), C . layers . AveragePooling ( filter_shape = ( 2 , 2 ), strides = 1 , pad = True ) ]), C . layers . Dense ( num_output_classes , activation = None ) ]) return model ( features ) If you have seen some networks in CNTK, note I'm using the Sequential notation \"shortcut\" for repeating some layers, but maybe with different params (more in this Doc ). The above \"simple\" CNN resulted in an average test error of 28.53%. I decided I'd like to try a more complex network and so began reading some papers on emotion recognition. I came across one that appeared to be using the same size images (likely the same dataset). Their proposed CNN architecture looked like this ( Ref ): I implemented the paper's architecture in the CNTK Python API as follows (it had 5935330 parameters in 12 parameter tensors): def create_model ( features ): with C . layers . default_options ( init = C . glorot_uniform (), activation = C . relu ): model = C . layers . Sequential ([ C . layers . For ( range ( 3 ), lambda i : [ C . layers . Convolution2D ( filter_shape = [( 5 , 5 ), ( 5 , 5 ), ( 3 , 3 )][ i ], num_filters = 10 , pad = True , strides = ( 1 , 1 )), [ C . layers . AveragePooling ( filter_shape = ( 2 , 2 ), strides = 1 , pad = True ), C . layers . MaxPooling ( filter_shape = ( 2 , 2 ), strides = 1 , pad = True ), C . layers . MaxPooling ( filter_shape = ( 2 , 2 ), strides = 1 , pad = True )][ i ]]), C . layers . Dense ( 256 ), C . layers . Dropout ( 0.5 ), C . layers . Dense ( 128 ), C . layers . Dropout ( 0.5 ), C . layers . Dense ( num_output_classes , activation = None ) ]) return model ( features ) The above CNN architecture which matched the paper, resulted in a 25.61% average test error. Let's see if we can do even better. A slightly modified, even more complex version of this architecture looks like this in the CNTK Python API (my update was to increase the number of filters for each conv layer): def create_model ( features ): with C . layers . default_options ( init = C . glorot_uniform (), activation = C . relu ): model = C . layers . Sequential ([ C . layers . For ( range ( 3 ), lambda i : [ C . layers . Convolution2D ( filter_shape = ( 5 , 5 ), num_filters = [ 48 , 48 , 64 ][ i ], pad = True , strides = ( 1 , 1 ), C . layers . MaxPooling ( filter_shape = ( 2 , 2 ), strides = 1 , pad = True ) ]), C . layers . Dense ( 256 ), C . layers . Dropout ( 0.5 ), C . layers . Dense ( 128 ), C . layers . Dropout ( 0.5 ), C . layers . Dense ( num_output_classes , activation = None ) ]) return model ( features ) In the last architecture I had 37917906 parameters in 12 parameter tensors with an average test error of 22.35% (which, to be honest, could cause an issue of over-fitting due to the very large number of parameters - just something to consider). A validation step would be highly recommended in this case given sufficient computational power! Conclusion I picked a few random pictures from an online image search of happy and sad faces. With my error rate of 22% it did pretty well, even on pictures of my own face (not shown). These, for instance, were correctly identified as happy and sad. (Note, the images may not show correctly as their 48x48 square size in this browser). The improvements are not an exhaustive list of everything one can try to gain better results of course, but do represent some common ways to deal with image data using CNNs. If you have GPU-acceleration options, give some more complex network architectures a try - you could spin up an Azure Deep Learning VM which is what I usually do at my day job. Happy deep learning! The Jupyter notebooks associated with this post can be found on GitHub here References Historical (Jan. 2016) Readme on CNTK Repo Microsoft researchers win ImageNet computer vision challenge Facial Emotion Detection Using Convolutional Neural Networks and Representational Autoencoder Units by Prudhvi Raj Dachapally","title":"The Cognitive Toolkit (CNTK) Understands How You Feel"},{"location":"cntk-has-feelings-too/#some-background","text":"","title":"Some Background"},{"location":"cntk-has-feelings-too/#cntk","text":"The original name for Microsoft's CNTK was the Computational Network Toolkit , now known today simply as the Cognitive Toolkit , still abbreviated CNTK for short. It was orignally written and offered up as a C++ package and now has Python bindings, making it much more widely adoptable. In its original words: [CNTK is] a unified deep-learning toolkit that describes neural networks as a series of computational steps via a directed graph It was first open-sourced in April of 2015 with intended use for researchers and protoypers using GPUs for accelerated matrix calculations, much of what deep learning is built upon these days. Interestingly, TensorFlow has its initial public release in November of 2015. Of note, 2015 was also a good year for Microsoft Research in the computer vision space as they won the ImageNet challenge that December using this toolkit and a 152-layer deep neural network. Since the beginning CNTK has been available for Linux and Windows. We will be using a Linux Docker image in a minute.","title":"CNTK"},{"location":"cntk-has-feelings-too/#fer-faces-data","text":"This data comes via a Kaggle competition on facial expressions found here . The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression in to one of two categories, Happy or Sad. That being said this dataset has images (all but two of interest were dropped in this analysis) for a total of 6 emotions: Angry, Disgust, Fear, Happy, Sad, Surprise, and Neutral.","title":"FER Faces Data"},{"location":"cntk-has-feelings-too/#how-i-set-things-up","text":"Since I'm on a Mac, I chose to use the Docker image of CNTK (instructions found here ). This pulls down an image of a pre-created system and I run it in my own Docker container basically recreating an Ubuntu setup with CNTK locally. It's pretty neat! And then I can run Jupyter notebooks on my system and pull in local files and write out data as needed. Let me show you how and then after we'll talk about the CNNs. By following this Doc I got a Jupyter notebook up and running with CNTK with all of the Tutorial notebooks at the ready and the ability to upload or create new ones as needed. I ran these commands to get a Jupyter notebook set up with CNTK (v2.1 used here). An important note: in the run command for Docker I mounted a volume with \"$PWD/data:/data\" . This \"/data\" folder can be accessed from the Jupyter notebook, as you can see if you check them out (link below), but also used to add data or pull data from the docker container just like any file folder on your system. A very handy trick! docker pull microsoft/cntk:2.1-cpu-python3.5 docker run -d --volume \"$PWD/data:/data\" -p 8888:8888 --name cntk-jupyter-notebooks -t microsoft/cntk:2.1-cpu-python3.5 docker exec -it cntk-jupyter-notebooks bash -c \"source /cntk/activate-cntk && jupyter-notebook --no-browser --port=8888 --ip=0.0.0.0 --notebook-dir=/cntk/Tutorials --allow-root\"","title":"How I set things up"},{"location":"cntk-has-feelings-too/#what-i-did-in-a-nutshell","text":"So, I tried many different CNN network architectures (simple three-layer CNN, ones with pooling and dropout layers, etc.) and several hyperparameter combinations (minibatch sizes for training, learning rate, etc.). Here are just a few basic improvement tips for image classification based on what I found and a bit on how to do this with CNTK, plus my results - my hope is that you can do this on your own with some tips to start or maybe add to your workflow. The Jupyter notebooks associated with this post can be found on GitHub here","title":"What I Did In a Nutshell"},{"location":"cntk-has-feelings-too/#positive-results","text":"","title":"Positive Results"},{"location":"cntk-has-feelings-too/#improvement-1-use-sufficient-data","text":"I began this journey with about 1000 images that I sorted and curated into a \"sad\" or \"happy\" bucket - using the PICS 2D face sets . I even got up to 72% accuracy with this dataset and a CNN with pooling. Then I discovered a Kaggle competition from about 5 years ago dealing with recognizing facial expressions - Challenges in Representation Learning: Facial Expression Recognition Challenge and this provided me with about 13500 training images and 2300 test images of the \"happy\" or \"sad\" kind (there were also images for categories: angry, disgust, fear, surprise and neutral in case you wish to try these out).","title":"Improvement #1:  Use sufficient data"},{"location":"cntk-has-feelings-too/#improvement-2-intensity-normalization","text":"I read about the sensitivity of CNNs to certain normalizations. So, I decided to try a recommended pixel intensity normalization ( Ref to nice StackOverflow hint ). This update resulted in about a 5% increase in accuracy on the held-out test set. My code looked like: def normalize ( arr ): \"\"\" Linear normalization http://en.wikipedia.org/wiki/Normalization_%28image_processing%29 \"\"\" arr = arr . astype ( 'float' ) minval = arr . min () maxval = arr . max () if minval != maxval : arr -= minval arr *= ( 255.0 / ( maxval - minval )) return arr","title":"Improvement #2:  Intensity normalization"},{"location":"cntk-has-feelings-too/#improvement-3-adjust-or-even-search-hyperparameter-space","text":"Play with the hyperparameters. I had a code cell full of my hyperparameters, e.g. learning rate and minibatch sizes. I varied one or two at a time. Just as an example, I took the learning rate down 10x (from 0.2 to 0.02) and my resulting accuracy increased approximately 5%. Using the CNTK train package one can design a \"trainer\" object encapsulating all training tasks and include parameter ranges inside it. I also played with the strides and filter sizes or receptive fields in my CNN for the convolutional and pooling layers (see the Stanford CV course notes for great explanations of pretty much anything CNN and much more). My big tip if you are starting out is simply to tune these adjustable parameters yourself and see what happens.","title":"Improvement #3:  Adjust or even search hyperparameter space"},{"location":"cntk-has-feelings-too/#improvement-4-add-more-layers-but-be-careful-of-overfitting","text":"Interestingly, layering in three pooling layers in between the convolutional layers (last one before the dense output layer) resulted in about another 5% jump in accuracy on the test set. (more on CNNs in the Stanford course notes ) Here's what the relatively simple model looked like in the CNTK Python API: def create_model ( features ): with C . layers . default_options ( init = C . glorot_uniform (), activation = C . relu ): model = C . layers . Sequential ([ C . layers . For ( range ( 3 ), lambda i : [ C . layers . Convolution2D ( filter_shape = ( 5 , 5 ), num_filters = [ 8 , 16 , 16 ][ i ], pad = True , strides = [( 1 , 1 ), ( 1 , 1 ), ( 2 , 2 )][ i ]), C . layers . AveragePooling ( filter_shape = ( 2 , 2 ), strides = 1 , pad = True ) ]), C . layers . Dense ( num_output_classes , activation = None ) ]) return model ( features ) If you have seen some networks in CNTK, note I'm using the Sequential notation \"shortcut\" for repeating some layers, but maybe with different params (more in this Doc ). The above \"simple\" CNN resulted in an average test error of 28.53%. I decided I'd like to try a more complex network and so began reading some papers on emotion recognition. I came across one that appeared to be using the same size images (likely the same dataset). Their proposed CNN architecture looked like this ( Ref ): I implemented the paper's architecture in the CNTK Python API as follows (it had 5935330 parameters in 12 parameter tensors): def create_model ( features ): with C . layers . default_options ( init = C . glorot_uniform (), activation = C . relu ): model = C . layers . Sequential ([ C . layers . For ( range ( 3 ), lambda i : [ C . layers . Convolution2D ( filter_shape = [( 5 , 5 ), ( 5 , 5 ), ( 3 , 3 )][ i ], num_filters = 10 , pad = True , strides = ( 1 , 1 )), [ C . layers . AveragePooling ( filter_shape = ( 2 , 2 ), strides = 1 , pad = True ), C . layers . MaxPooling ( filter_shape = ( 2 , 2 ), strides = 1 , pad = True ), C . layers . MaxPooling ( filter_shape = ( 2 , 2 ), strides = 1 , pad = True )][ i ]]), C . layers . Dense ( 256 ), C . layers . Dropout ( 0.5 ), C . layers . Dense ( 128 ), C . layers . Dropout ( 0.5 ), C . layers . Dense ( num_output_classes , activation = None ) ]) return model ( features ) The above CNN architecture which matched the paper, resulted in a 25.61% average test error. Let's see if we can do even better. A slightly modified, even more complex version of this architecture looks like this in the CNTK Python API (my update was to increase the number of filters for each conv layer): def create_model ( features ): with C . layers . default_options ( init = C . glorot_uniform (), activation = C . relu ): model = C . layers . Sequential ([ C . layers . For ( range ( 3 ), lambda i : [ C . layers . Convolution2D ( filter_shape = ( 5 , 5 ), num_filters = [ 48 , 48 , 64 ][ i ], pad = True , strides = ( 1 , 1 ), C . layers . MaxPooling ( filter_shape = ( 2 , 2 ), strides = 1 , pad = True ) ]), C . layers . Dense ( 256 ), C . layers . Dropout ( 0.5 ), C . layers . Dense ( 128 ), C . layers . Dropout ( 0.5 ), C . layers . Dense ( num_output_classes , activation = None ) ]) return model ( features ) In the last architecture I had 37917906 parameters in 12 parameter tensors with an average test error of 22.35% (which, to be honest, could cause an issue of over-fitting due to the very large number of parameters - just something to consider). A validation step would be highly recommended in this case given sufficient computational power!","title":"Improvement #4:  Add more layers (but be careful of overfitting)"},{"location":"cntk-has-feelings-too/#conclusion","text":"I picked a few random pictures from an online image search of happy and sad faces. With my error rate of 22% it did pretty well, even on pictures of my own face (not shown). These, for instance, were correctly identified as happy and sad. (Note, the images may not show correctly as their 48x48 square size in this browser). The improvements are not an exhaustive list of everything one can try to gain better results of course, but do represent some common ways to deal with image data using CNNs. If you have GPU-acceleration options, give some more complex network architectures a try - you could spin up an Azure Deep Learning VM which is what I usually do at my day job. Happy deep learning! The Jupyter notebooks associated with this post can be found on GitHub here","title":"Conclusion"},{"location":"cntk-has-feelings-too/#references","text":"Historical (Jan. 2016) Readme on CNTK Repo Microsoft researchers win ImageNet computer vision challenge Facial Emotion Detection Using Convolutional Neural Networks and Representational Autoencoder Units by Prudhvi Raj Dachapally","title":"References"},{"location":"confusion-matrix-code-revealed/","text":"tl;dr : We make a confusion matrix (or ML metric) in python for a k-means algorithm and it's good lookin' :) Posted: 2017-02-12 Step 1 The AML Workflow Our story starts with an Azure Machine Learning experiment or what I like to call data science workflow (I'll use the word workflow here). We could also have started with a file (see Step 2 Second Way ) instead, but either way, cleansed data gets fed into a k-means clustering algorithm after some initial processing (I like this brief post on k-means and it's got python snippets as well!). This post is about coding up one of the metrics that tells us how well an algorithm did if we have some \"groundtruth\" data to which to compare (remember that often we won't in unsupervised learning, but we are lucky today). This workflow is for text feature extraction, selection and clustering based on extracted features as n-grams (check out the intro here for a quick explanation of this workflow and n-grams). I have one workflow with an a priori value for the centroids of 10 for the k-means algorithm. Here's a screenshot of the workflow (starting dataset is a listing of 500 Wikipedia articles, cleaned up, along with some category labels for \"groundtruth\" comparisons later - remember, k-means is unsupervised). This workflow is already ready for you to use for free (using a Microsoft ID like outlook.com, xbox, hotmail, etc. accounts.) Find it in Cortana Intelligence Gallery (love this place for all of its abundance of resources): https://gallery.cortanaintelligence.com/Experiment/N-Grams-and-Clustering-Find-similar-companies-Training-1 Just to highlight, in the AML workflow I selected my desired columns for the confusion matrix with Select Columns in Dataset module to get 'Category' and 'Assignment' (cluster assignment as an integer from 0 to number of centroids I specified at the beginning). Step 2 First Way Notice, I added a Convert to CSV module (as you can see in above workflow diagram) after the Select Columns in Dataset . I right clicked on the output node of the Convert to CSV and a little menu popped up from which I selected \"Open in a new Notebook\" and \"Python 3\" (because Python 3 rules of course - my R colleagues are going to really chide me now). This opened up a jupyter notebook with the following code snippet: from azureml import Workspace ws = Workspace () experiment = ws . experiments [ '<your experiment id shows up here>' ] ds = experiment . get_intermediate_dataset ( node_id = '<your node id shows up here>' , port_name = 'Results dataset' , data_type_id = 'GenericCSV' ) frame = ds . to_dataframe () And imported my final dataset as a pandas DataFrame. To get a confusion matrix I used pandas.crosstab and matplotlib . I created a cell and used pandas 's crosstab to aggregate the Categories by Assignments and place into a matrix. # Creating our confusion matrix data cm = pd . crosstab ( frame [ 'Category' ], frame [ 'Assignments' ]) print ( cm ) So we went from Category Assignments 0 Information Technology 0 1 Information Technology 9 2 Consumer Discretionary 0 3 Energy 4 4 Consumer Discretionary 0 5 Information Technology 2 6 Information Technology 0 7 Consumer Discretionary 0 8 Information Technology 3 9 Information Technology 2 10 Financials 8 11 Consumer Staples 0 12 Information Technology 6 13 Consumer Discretionary 7 14 Information Technology 2 15 Information Technology 2 16 Information Technology 0 17 Industrials 6 18 Consumer Staples 9 19 Health Care 9 ... to Assignments 0 1 2 3 4 5 6 7 8 9 Category Consumer Discretionary 43 0 3 1 0 0 1 20 4 4 Consumer Staples 14 0 0 0 9 0 2 4 0 6 Energy 2 1 0 1 12 0 28 0 0 0 Financials 16 0 3 3 0 0 3 8 42 3 Health Care 3 0 0 1 1 0 0 0 0 47 ... And finally, I used matplotlib and a modified example from the python docs, with this code, # Plot our confusion matrix # Code based on: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html def plot_confusion_matrix ( cm , title = 'Confusion matrix' , cmap = plt . cm . BuPu , normalize = False ): # Set figure size before anything else colcnt = len ( cm . columns ) rowcnt = len ( cm . index ) # Adjust the size of the plot area () plt . figure ( figsize = ( colcnt / 0.8 , rowcnt / 0.8 )) if normalize : # Normalize each row by the row sum sum_row = [ a [ 0 ] for a in cm . sum ( axis = 1 )[:, np . newaxis ]] df_cm = pd . DataFrame ( cm ) df_sum = pd . DataFrame ( sum_row ) df = df_cm . as_matrix () / df_sum . as_matrix () cm = pd . DataFrame ( df , index = cm . index , columns = cm . columns ) # Show the plot plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) # Give the plot a title and colorbar legend plt . title ( title , size = 12 ) plt . colorbar () # All thes stuff for the tick mark labels xtick_marks = np . arange ( len ( cm . columns )) ytick_marks = np . arange ( len ( cm . index )) plt . xticks ( xtick_marks , cm . columns , size = 12 ) plt . yticks ( ytick_marks , cm . index , size = 12 ) # Just the regular xlabel and ylabel for plot plt . ylabel ( cm . index . name , size = 12 ) plt . xlabel ( cm . columns . name , size = 12 ) # Setting to offset the labels with some space so they show up plt . subplots_adjust ( left = 0.5 , bottom = 0.5 ) # Plot the confusion matrix DataFrame plot_confusion_matrix ( cm , normalize = False , title = 'Confusion matrix ( %d centroids): no normalization' % len ( cm . columns )) plot_confusion_matrix ( cm , normalize = True , title = 'Confusion matrix ( %d centroids): with normalization' % len ( cm . columns )) to create the following awesome plots (a non-normalized and normalized confusion matrix): Step 2 Second Way I could have exported the AML Studio data as a file from the Convert to CSV module and downloaded the dataset after running. I would then upload the dataset to a notebook (as is also shown in the sample notebook here ) and use the csv file with a 'Category' column and 'Assigments' column like is found here . It imports the data as a pandas dataframe. The code snippet would have been: # Dataset living on my github account exported from Azure ML url = 'https://raw.githubusercontent.com/michhar/michhar.github.io/gh-pages-source/data/ngrams_and_clustering_result_dataset.csv' # Importing the csv data with pandas frame = pd . read_csv ( url ) Thanks for reading, check out the sample (static) jupyter notebook here and best of luck with those confusion matrices!","title":"A Simple, Presentable Confusion Matrix with K-means Data"},{"location":"confusion-matrix-code-revealed/#step-1-the-aml-workflow","text":"Our story starts with an Azure Machine Learning experiment or what I like to call data science workflow (I'll use the word workflow here). We could also have started with a file (see Step 2 Second Way ) instead, but either way, cleansed data gets fed into a k-means clustering algorithm after some initial processing (I like this brief post on k-means and it's got python snippets as well!). This post is about coding up one of the metrics that tells us how well an algorithm did if we have some \"groundtruth\" data to which to compare (remember that often we won't in unsupervised learning, but we are lucky today). This workflow is for text feature extraction, selection and clustering based on extracted features as n-grams (check out the intro here for a quick explanation of this workflow and n-grams). I have one workflow with an a priori value for the centroids of 10 for the k-means algorithm. Here's a screenshot of the workflow (starting dataset is a listing of 500 Wikipedia articles, cleaned up, along with some category labels for \"groundtruth\" comparisons later - remember, k-means is unsupervised). This workflow is already ready for you to use for free (using a Microsoft ID like outlook.com, xbox, hotmail, etc. accounts.) Find it in Cortana Intelligence Gallery (love this place for all of its abundance of resources): https://gallery.cortanaintelligence.com/Experiment/N-Grams-and-Clustering-Find-similar-companies-Training-1 Just to highlight, in the AML workflow I selected my desired columns for the confusion matrix with Select Columns in Dataset module to get 'Category' and 'Assignment' (cluster assignment as an integer from 0 to number of centroids I specified at the beginning).","title":"Step 1 The AML Workflow"},{"location":"confusion-matrix-code-revealed/#step-2-first-way","text":"Notice, I added a Convert to CSV module (as you can see in above workflow diagram) after the Select Columns in Dataset . I right clicked on the output node of the Convert to CSV and a little menu popped up from which I selected \"Open in a new Notebook\" and \"Python 3\" (because Python 3 rules of course - my R colleagues are going to really chide me now). This opened up a jupyter notebook with the following code snippet: from azureml import Workspace ws = Workspace () experiment = ws . experiments [ '<your experiment id shows up here>' ] ds = experiment . get_intermediate_dataset ( node_id = '<your node id shows up here>' , port_name = 'Results dataset' , data_type_id = 'GenericCSV' ) frame = ds . to_dataframe () And imported my final dataset as a pandas DataFrame. To get a confusion matrix I used pandas.crosstab and matplotlib . I created a cell and used pandas 's crosstab to aggregate the Categories by Assignments and place into a matrix. # Creating our confusion matrix data cm = pd . crosstab ( frame [ 'Category' ], frame [ 'Assignments' ]) print ( cm ) So we went from Category Assignments 0 Information Technology 0 1 Information Technology 9 2 Consumer Discretionary 0 3 Energy 4 4 Consumer Discretionary 0 5 Information Technology 2 6 Information Technology 0 7 Consumer Discretionary 0 8 Information Technology 3 9 Information Technology 2 10 Financials 8 11 Consumer Staples 0 12 Information Technology 6 13 Consumer Discretionary 7 14 Information Technology 2 15 Information Technology 2 16 Information Technology 0 17 Industrials 6 18 Consumer Staples 9 19 Health Care 9 ... to Assignments 0 1 2 3 4 5 6 7 8 9 Category Consumer Discretionary 43 0 3 1 0 0 1 20 4 4 Consumer Staples 14 0 0 0 9 0 2 4 0 6 Energy 2 1 0 1 12 0 28 0 0 0 Financials 16 0 3 3 0 0 3 8 42 3 Health Care 3 0 0 1 1 0 0 0 0 47 ... And finally, I used matplotlib and a modified example from the python docs, with this code, # Plot our confusion matrix # Code based on: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html def plot_confusion_matrix ( cm , title = 'Confusion matrix' , cmap = plt . cm . BuPu , normalize = False ): # Set figure size before anything else colcnt = len ( cm . columns ) rowcnt = len ( cm . index ) # Adjust the size of the plot area () plt . figure ( figsize = ( colcnt / 0.8 , rowcnt / 0.8 )) if normalize : # Normalize each row by the row sum sum_row = [ a [ 0 ] for a in cm . sum ( axis = 1 )[:, np . newaxis ]] df_cm = pd . DataFrame ( cm ) df_sum = pd . DataFrame ( sum_row ) df = df_cm . as_matrix () / df_sum . as_matrix () cm = pd . DataFrame ( df , index = cm . index , columns = cm . columns ) # Show the plot plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) # Give the plot a title and colorbar legend plt . title ( title , size = 12 ) plt . colorbar () # All thes stuff for the tick mark labels xtick_marks = np . arange ( len ( cm . columns )) ytick_marks = np . arange ( len ( cm . index )) plt . xticks ( xtick_marks , cm . columns , size = 12 ) plt . yticks ( ytick_marks , cm . index , size = 12 ) # Just the regular xlabel and ylabel for plot plt . ylabel ( cm . index . name , size = 12 ) plt . xlabel ( cm . columns . name , size = 12 ) # Setting to offset the labels with some space so they show up plt . subplots_adjust ( left = 0.5 , bottom = 0.5 ) # Plot the confusion matrix DataFrame plot_confusion_matrix ( cm , normalize = False , title = 'Confusion matrix ( %d centroids): no normalization' % len ( cm . columns )) plot_confusion_matrix ( cm , normalize = True , title = 'Confusion matrix ( %d centroids): with normalization' % len ( cm . columns )) to create the following awesome plots (a non-normalized and normalized confusion matrix):","title":"Step 2 First Way"},{"location":"confusion-matrix-code-revealed/#step-2-second-way","text":"I could have exported the AML Studio data as a file from the Convert to CSV module and downloaded the dataset after running. I would then upload the dataset to a notebook (as is also shown in the sample notebook here ) and use the csv file with a 'Category' column and 'Assigments' column like is found here . It imports the data as a pandas dataframe. The code snippet would have been: # Dataset living on my github account exported from Azure ML url = 'https://raw.githubusercontent.com/michhar/michhar.github.io/gh-pages-source/data/ngrams_and_clustering_result_dataset.csv' # Importing the csv data with pandas frame = pd . read_csv ( url ) Thanks for reading, check out the sample (static) jupyter notebook here and best of luck with those confusion matrices!","title":"Step 2 Second Way"},{"location":"convert-pytorch-onnx/","text":"Posted: 2018-09-27 It might seem tricky or intimidating to convert model formats, but ONNX makes it easier. However, we must get our PyTorch model into the ONNX format. This involves both the weights and network architecture defined by a PyToch model class (inheriting from nn.Module ). I don't write out the model classes, however, I wanted to share the steps and code from the point of having the class definition and some weights (either in memory or from a model path file). One could also do this with the pre-trained models from the torchvision library. The General Steps Define the model class (if using a custom model) Train the model and/or load the weights, usually a .pth or .pt file by convention, to something usually called the state_dict - note, we are only loading the weights from a file. A pre-trained model such as is found in torchvision.models may also be used with the provided weights (using pretrained=True - see below). Create a properly shaped input vector (can be some sample data - the important part is the shape) (Optional) Give the input and output layers names (to later reference back) Export to ONNX format with the PyTorch ONNX exporter Prerequisites PyTorch and torchvision installed A PyTorch model class and model weights Using a Custom Model Class and Weights File The Python looks something like: import torch import torch.onnx # A model class instance (class not shown) model = MyModelClass () # Load the weights from a file (.pth usually) state_dict = torch . load ( weights_path ) # Load the weights now into a model net architecture defined by our class model . load_state_dict ( state_dict ) # Create the right input shape (e.g. for an image) dummy_input = torch . randn ( sample_batch_size , channel , height , width ) torch . onnx . export ( model , dummy_input , \"onnx_model_name.onnx\" ) The state dictionary, or state_dict , is a Python dict containing parameter values and persistent buffers. ( Docs ) Note: The preferred way of saving the weights is with torch.save(the_model.state_dict(), <name_here.pth>) . ( Docs ) A Pre-Trained Model from torchvision If using the torchvision.models pretrained vision models all you need to do is, e.g., for AlexNet: import torch import torchvision.models as models # Use an existing model from Torchvision, note it # will download this if not already on your computer (might take time) model = models . alexnet ( pretrained = True ) # Create some sample input in the shape this model expects dummy_input = torch . randn ( 10 , 3 , 224 , 224 ) # It's optional to label the input and output layers input_names = [ \"actual_input_1\" ] + [ \"learned_ %d \" % i for i in range ( 16 ) ] output_names = [ \"output1\" ] # Use the exporter from torch to convert to onnx # model (that has the weights and net arch) torch . onnx . export ( model , dummy_input , \"alexnet.onnx\" , verbose = True , input_names = input_names , output_names = output_names ) Note, the pretrained model weights that comes with torchvision.models went into a home folder ~/.torch/models in case you go looking for it later. Summary Here, I showed how to take a pre-trained PyTorch model (a weights object and network class object) and convert it to ONNX format (that contains the weights and net structure). As of now, we can not import an ONNX model for use in PyTorch. There are other projects that are working on this as well as is shown in this list . More References Example: End-to-end AlexNet from PyTorch to Caffe2 ONNX GitHub PyTorch.org For a more complicated example, see this conversion /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */ var disqus_config = function () { this.page.url = 'https://michhar.github.io/convolutional-in-layers-and-sequences/'; // Replace PAGE_URL with your page's canonical URL variable this.page.identifier = 'happycat1'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable }; (function() { // DON'T EDIT BELOW THIS LINE var d = document, s = d.createElement('script'); s.src = 'https://michhar.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); Please enable JavaScript to view the comments powered by Disqus.","title":"How to Convert a PyTorch Model to ONNX Format"},{"location":"convert-pytorch-onnx/#the-general-steps","text":"Define the model class (if using a custom model) Train the model and/or load the weights, usually a .pth or .pt file by convention, to something usually called the state_dict - note, we are only loading the weights from a file. A pre-trained model such as is found in torchvision.models may also be used with the provided weights (using pretrained=True - see below). Create a properly shaped input vector (can be some sample data - the important part is the shape) (Optional) Give the input and output layers names (to later reference back) Export to ONNX format with the PyTorch ONNX exporter","title":"The General Steps"},{"location":"convert-pytorch-onnx/#prerequisites","text":"PyTorch and torchvision installed A PyTorch model class and model weights","title":"Prerequisites"},{"location":"convert-pytorch-onnx/#using-a-custom-model-class-and-weights-file","text":"The Python looks something like: import torch import torch.onnx # A model class instance (class not shown) model = MyModelClass () # Load the weights from a file (.pth usually) state_dict = torch . load ( weights_path ) # Load the weights now into a model net architecture defined by our class model . load_state_dict ( state_dict ) # Create the right input shape (e.g. for an image) dummy_input = torch . randn ( sample_batch_size , channel , height , width ) torch . onnx . export ( model , dummy_input , \"onnx_model_name.onnx\" ) The state dictionary, or state_dict , is a Python dict containing parameter values and persistent buffers. ( Docs ) Note: The preferred way of saving the weights is with torch.save(the_model.state_dict(), <name_here.pth>) . ( Docs )","title":"Using a Custom Model Class and Weights File"},{"location":"convert-pytorch-onnx/#a-pre-trained-model-from-torchvision","text":"If using the torchvision.models pretrained vision models all you need to do is, e.g., for AlexNet: import torch import torchvision.models as models # Use an existing model from Torchvision, note it # will download this if not already on your computer (might take time) model = models . alexnet ( pretrained = True ) # Create some sample input in the shape this model expects dummy_input = torch . randn ( 10 , 3 , 224 , 224 ) # It's optional to label the input and output layers input_names = [ \"actual_input_1\" ] + [ \"learned_ %d \" % i for i in range ( 16 ) ] output_names = [ \"output1\" ] # Use the exporter from torch to convert to onnx # model (that has the weights and net arch) torch . onnx . export ( model , dummy_input , \"alexnet.onnx\" , verbose = True , input_names = input_names , output_names = output_names ) Note, the pretrained model weights that comes with torchvision.models went into a home folder ~/.torch/models in case you go looking for it later.","title":"A Pre-Trained Model from torchvision"},{"location":"convert-pytorch-onnx/#summary","text":"Here, I showed how to take a pre-trained PyTorch model (a weights object and network class object) and convert it to ONNX format (that contains the weights and net structure). As of now, we can not import an ONNX model for use in PyTorch. There are other projects that are working on this as well as is shown in this list .","title":"Summary"},{"location":"convert-pytorch-onnx/#more-references","text":"Example: End-to-end AlexNet from PyTorch to Caffe2 ONNX GitHub PyTorch.org For a more complicated example, see this conversion /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */ var disqus_config = function () { this.page.url = 'https://michhar.github.io/convolutional-in-layers-and-sequences/'; // Replace PAGE_URL with your page's canonical URL variable this.page.identifier = 'happycat1'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable }; (function() { // DON'T EDIT BELOW THIS LINE var d = document, s = d.createElement('script'); s.src = 'https://michhar.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); Please enable JavaScript to view the comments powered by Disqus.","title":"More References"},{"location":"convolutional-in-layers-and-sequences/","text":"tl;dr : No secret: ConvNets are still complex even when you compare across APIs that sound similar for four deep learning frameworks. Here, you'll find an attempt to compare simple ConvNets in these frameworks. Also, included is a little ConvNet conceptual breakdown. Lots of reference code. Posted: 2018-05-13 Introduction I've found recently that the Sequential classes and Layer/Layers modules are names used across Keras, PyTorch, TensorFlow and CNTK - making it a little confusing to switch from one framework to another. I was also curious how easy it would be to use these modules/APIs in each framework to define the same Convolutional neural network ( ConvNet ). Let's get through some terminology, first. You can skip to the Code if you are already familiar with ConvNets on images. Note, the code originates from projects working with MNIST handwritten digits dataset. The neural network architecture used in this post is as follows. Convolutional layer Max pooling layer Convolutional layer Max pooling layer Fully connected or dense layer with 10 outputs and softmax activation (to get probabilities) A convolutional layer creates a feature map (using a filter or kernel , which I like to refer to as a \"flashlight\", shinning on the image and stepping through with a sliding window of 1 unit, that's a stride of 1, by the way). A good reference for this is in the CNTK Tutorial . Source A pooling layer is a way to subsample an input feature map, or output from the convolutional layer that has already extracted salient features from an image in our case. Source A fully connected layer is defined such that every input unit is connected to every output unit much like the multilayer perceptron . Source Not represented in the code below, but important nonetheless, is dropout. Dropout removes a percentage of the neuron connections - helping to prevent overfitting by reducing the feature space for convolutional and, especially, dense layers. Source Remember, the power of a convolutional layer is that we don't have to do much upfront raw image processing. The layer(s) will subsequently find the most salient features for us. In this post you will find ConvNets defined for four frameworks with adaptations to create easier comparisons (please leave comments as needed). The full example code can be found as a Jupyter notebook - Ref . The Frameworks Keras Below is a ConvNet defined with the Sequential model in Keras ( Ref ). This is a snippet with only the model definition parts - see the References for the full code example. \"\"\" Adapted from: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py \"\"\" import keras from keras.models import Sequential from keras.layers import Dense , Flatten from keras.layers import Conv2D , MaxPooling2D model = Sequential () model . add ( Conv2D ( 32 , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), padding = 'same' , activation = 'relu' , input_shape = input_shape )) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Conv2D ( 64 , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), padding = 'same' , activation = 'relu' )) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Flatten ()) model . add ( Dense ( num_classes , activation = 'softmax' )) sgd = SGD ( lr = 0.05 , decay = 1e-6 , momentum = 0.9 , nesterov = True ) model . compile ( loss = 'categorical_crossentropy' , optimizer = sgd , metrics = [ 'accuracy' ]) What you don't see is: Fit/train ( model.fit() ) Evaluate with given metric ( model.evaluate() ) To add dropout after the Convolution2D() layer (or after the fully connected in any of these examples) a dropout function will be used, e.g., Dropout(0.5) Sometimes another fully connected (dense) layer with, say, ReLU activation, is added right before the final fully connected layer. PyTorch Below is a ConvNet defined with the Sequential container in PyTorch ( Ref ). This is a snippet with only the model definition parts - see the References for the full code example. import torch import torch.nn as nn import torch.nn.functional as F class ConvNetPyTorch ( nn . Module ): \"\"\"Adapted from: https://github.com/rasbt/deep-learning-book/blob/master/code/model_zoo/pytorch_ipynb/convnet.ipynb \"\"\" def __init__ ( self , num_classes = 10 ): super ( ConvNetPyTorch , self ) . __init__ () self . layer1 = nn . Sequential ( # 28x28x1 => 28x28x32 nn . Conv2d ( in_channels = 1 , out_channels = 32 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), # (1(28-1) - 28 + 3) / 2 = 1 nn . ReLU (), # 28x28x32 => 14x14x32 nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 ), padding = 0 )) # (2(14-1) - 28 + 2) = 0 self . layer2 = nn . Sequential ( # 14x14x32 => 14x14x64 nn . Conv2d ( in_channels = 32 , out_channels = 64 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), # (1(14-1) - 14 + 3) / 2 = 1 nn . ReLU (), # 14x14x64 => 7x7x64 nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 ), padding = 0 )) # (2(7-1) - 14 + 2) = 0 self . linear_1 = nn . Linear ( 7 * 7 * 64 , num_classes ) def forward ( self , x ): out = self . layer1 ( x ) out = self . layer2 ( out ) out = out . reshape ( out . size ( 0 ), - 1 ) logits = self . linear_1 ( out . view ( - 1 , 7 * 7 * 64 )) probas = F . softmax ( logits , dim = 1 ) return logits , probas model = ConvNetPyTorch ( num_classes ) . to ( device ) What you don't see is: Fit/train ( model.train() ) Evaluate with given metric ( model.eval() ) To add dropout after the nn.ReLU() layer (or even after the fully connected in any of these examples) a dropout function will be used, e.g. nn.Dropout(0.5) Sometimes another fully connected (dense) layer with, say, ReLU activation, is added right before the final fully connected layer. Tensorflow Below is a ConvNet defined with the Layers library and Estimators API in TensorFlow ( Ref ). This is a snippet with only the model definition parts - see the References for the full code example. import tensorflow as tf # Create the neural network def convNetTensorFlow ( x_dict , n_classes , reuse , is_training ): \"\"\"Adapted from: https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network.py \"\"\" # Define a scope for reusing the variables with tf . variable_scope ( 'ConvNet' , reuse = reuse ): # TF Estimator input is a dict, in case of multiple inputs x = x_dict [ 'images' ] # MNIST data input is a 1-D vector of 784 features (28*28 pixels) # Reshape to match picture format [Height x Width x Channel] # Tensor input become 4-D: [Batch Size, Height, Width, Channel] x = tf . reshape ( x , shape = [ - 1 , 28 , 28 , 1 ]) # Convolution Layer with 32 filters and a kernel size of 5 conv1 = tf . layers . conv2d ( x , 32 , 3 , activation = tf . nn . relu ) # Max Pooling (down-sampling) with strides of 2 and kernel size of 2 conv1 = tf . layers . max_pooling2d ( conv1 , 2 , 2 ) # Convolution Layer with 64 filters and a kernel size of 3 conv2 = tf . layers . conv2d ( conv1 , 64 , 3 , activation = tf . nn . relu ) # Max Pooling (down-sampling) with strides of 2 and kernel size of 2 conv2 = tf . layers . max_pooling2d ( conv2 , 2 , 2 ) # Flatten the data to a 1-D vector for the fully connected layer fc1 = tf . contrib . layers . flatten ( conv2 ) # Output layer, class prediction logits = tf . layers . dense ( fc1 , n_classes , activation = None ) return logits \"\"\"...[snipped for brevity]\"\"\" # Build the Estimator model = tf . estimator . Estimator ( model_fn ) What you don't see is: Fit/train ( model.train() ) Evaluate with given metric ( model.evaluate() ) To add dropout after the tf.layers.conv2d() layer (or even after the fully connected in any of these examples) a dropout function will be used, e.g. tf.layers.dropout(inputs=net_layer, rate=0.5, training=is_training) Sometimes another fully connected (dense) layer with, say, ReLU activation, is added right before the final fully connected layer. For more see tensorflow in the References below. Cognitive Toolkit (CNTK) Below is a ConvNet defined with the Layer API in CNTK ( Ref ). This is a snippet with only the model definition parts - see the References for the full code example (Note: as of this writing CNTK is Windows or Linux only) import cntk as C def convNetCNTK ( features , num_output_classes ): \"\"\"https://cntk.ai/pythondocs/CNTK_103D_MNIST_ConvolutionalNeuralNetwork.html\"\"\" with C . layers . default_options ( init = C . glorot_uniform (), activation = C . relu ): model = C . layers . Sequential ([ C . layers . For ( range ( 2 ), lambda i : [ C . layers . Convolution (( 3 , 3 ), [ 32 , 64 ][ i ], pad = True ), C . layers . MaxPooling (( 2 , 2 ), strides = ( 2 , 2 )) ]), C . layers . Dense ( 64 ), C . layers . Dense ( out_dims , activation = None ) ]) return model ( features ) What you don't see is: Fit/train ( trainer = C.Trainer() and trainer.train_minibatch() ) Evaluate with given metric ( out = C.softmax() and out.eval() ) To add dropout after the C.layers.Convolution() layer (or even after the fully connected in any of these examples) a dropout function will be used, e.g. C.layers.Dropout(0.5) . Sometimes another fully connected (dense) layer with, say, ReLU activation, is added right before the final fully connected layer. Conclusion No real conclusion except to say these frameworks do pretty much the same sorts of things and all have different API layers, high-level to low-level. I did find Keras to be the easiest and just as fast as TensorFlow - albeit with a lot abstracted away, which is generally good if one needs a quick jumping off point. The benchmarks are not official in full code sample notebook. The full code samples are in this Jupyter Notebook . Certainly some room for improvement in code and benchmarking so if you have any ideas, please leave a comment. References Samples adapted in this post: Keras code sample with Sequential model Ref PyTorch code sample with Sequential container Ref TensorFlow code sample with Layers and Estimators APIs Ref and ConvNets Tutorial at this Doc CNTK code sample with Layer API Doc A great book from which I took some of the concepts written in this post: Book and Code Even more nice code samples: Kaggle Keras code sample: https://www.kaggle.com/tonypoe/keras-cnn-example?scriptVersionId=589403 Keras example: http://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/ PyTorch example: https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/convolutional_neural_network/main.py CNTK example: https://cntk.ai/pythondocs/CNTK_201B_CIFAR-10_ImageHandsOn.html TensorFlow Estimators example: https://jhui.github.io/2017/03/14/TensorFlow-Estimator/ Thanks for reading. Appendix Nice explanation of tensor layouts (PyTorch vs. TensorFlow) in a PyTorch forum post by Mamy Ratsimbazafy ( Post ): Furthermore there might be a difference due to the Tensor layouts: PyTorch use NCHW and Tensorflow uses NHWC, NCHW was the first layout supported by CuDNN but presents a big challenge for optimization (due to access patterns in convolutions, memory coalescing and such \u2026). NHWC is easier to optimize for convolutions but suffer in linear layers iirc because you have to physically transpose/permute the dimensions. Furthermore, due to it\u2019s dynamic nature, PyTorch allocate new memory at each new batch while Tensorflow can just reuse previous memory locations since size is known in advance. Memory is THE bottleneck in Deep Learning not CPU, the big challenge is how to feed data fast enough to the CPU and GPU to get the maximum GFLOPS throughput. /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */ var disqus_config = function () { this.page.url = 'https://michhar.github.io/convolutional-in-layers-and-sequences/'; // Replace PAGE_URL with your page's canonical URL variable this.page.identifier = 'happycat1'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable }; (function() { // DON'T EDIT BELOW THIS LINE var d = document, s = d.createElement('script'); s.src = 'https://michhar.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); Please enable JavaScript to view the comments powered by Disqus.","title":"Convolutional Neural Networks in Four Deep Learning Frameworks by Example"},{"location":"convolutional-in-layers-and-sequences/#introduction","text":"I've found recently that the Sequential classes and Layer/Layers modules are names used across Keras, PyTorch, TensorFlow and CNTK - making it a little confusing to switch from one framework to another. I was also curious how easy it would be to use these modules/APIs in each framework to define the same Convolutional neural network ( ConvNet ). Let's get through some terminology, first. You can skip to the Code if you are already familiar with ConvNets on images. Note, the code originates from projects working with MNIST handwritten digits dataset. The neural network architecture used in this post is as follows. Convolutional layer Max pooling layer Convolutional layer Max pooling layer Fully connected or dense layer with 10 outputs and softmax activation (to get probabilities) A convolutional layer creates a feature map (using a filter or kernel , which I like to refer to as a \"flashlight\", shinning on the image and stepping through with a sliding window of 1 unit, that's a stride of 1, by the way). A good reference for this is in the CNTK Tutorial . Source A pooling layer is a way to subsample an input feature map, or output from the convolutional layer that has already extracted salient features from an image in our case. Source A fully connected layer is defined such that every input unit is connected to every output unit much like the multilayer perceptron . Source Not represented in the code below, but important nonetheless, is dropout. Dropout removes a percentage of the neuron connections - helping to prevent overfitting by reducing the feature space for convolutional and, especially, dense layers. Source Remember, the power of a convolutional layer is that we don't have to do much upfront raw image processing. The layer(s) will subsequently find the most salient features for us. In this post you will find ConvNets defined for four frameworks with adaptations to create easier comparisons (please leave comments as needed). The full example code can be found as a Jupyter notebook - Ref .","title":"Introduction"},{"location":"convolutional-in-layers-and-sequences/#the-frameworks","text":"","title":"The Frameworks"},{"location":"convolutional-in-layers-and-sequences/#keras","text":"Below is a ConvNet defined with the Sequential model in Keras ( Ref ). This is a snippet with only the model definition parts - see the References for the full code example. \"\"\" Adapted from: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py \"\"\" import keras from keras.models import Sequential from keras.layers import Dense , Flatten from keras.layers import Conv2D , MaxPooling2D model = Sequential () model . add ( Conv2D ( 32 , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), padding = 'same' , activation = 'relu' , input_shape = input_shape )) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Conv2D ( 64 , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), padding = 'same' , activation = 'relu' )) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Flatten ()) model . add ( Dense ( num_classes , activation = 'softmax' )) sgd = SGD ( lr = 0.05 , decay = 1e-6 , momentum = 0.9 , nesterov = True ) model . compile ( loss = 'categorical_crossentropy' , optimizer = sgd , metrics = [ 'accuracy' ]) What you don't see is: Fit/train ( model.fit() ) Evaluate with given metric ( model.evaluate() ) To add dropout after the Convolution2D() layer (or after the fully connected in any of these examples) a dropout function will be used, e.g., Dropout(0.5) Sometimes another fully connected (dense) layer with, say, ReLU activation, is added right before the final fully connected layer.","title":"Keras"},{"location":"convolutional-in-layers-and-sequences/#pytorch","text":"Below is a ConvNet defined with the Sequential container in PyTorch ( Ref ). This is a snippet with only the model definition parts - see the References for the full code example. import torch import torch.nn as nn import torch.nn.functional as F class ConvNetPyTorch ( nn . Module ): \"\"\"Adapted from: https://github.com/rasbt/deep-learning-book/blob/master/code/model_zoo/pytorch_ipynb/convnet.ipynb \"\"\" def __init__ ( self , num_classes = 10 ): super ( ConvNetPyTorch , self ) . __init__ () self . layer1 = nn . Sequential ( # 28x28x1 => 28x28x32 nn . Conv2d ( in_channels = 1 , out_channels = 32 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), # (1(28-1) - 28 + 3) / 2 = 1 nn . ReLU (), # 28x28x32 => 14x14x32 nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 ), padding = 0 )) # (2(14-1) - 28 + 2) = 0 self . layer2 = nn . Sequential ( # 14x14x32 => 14x14x64 nn . Conv2d ( in_channels = 32 , out_channels = 64 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), # (1(14-1) - 14 + 3) / 2 = 1 nn . ReLU (), # 14x14x64 => 7x7x64 nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 ), padding = 0 )) # (2(7-1) - 14 + 2) = 0 self . linear_1 = nn . Linear ( 7 * 7 * 64 , num_classes ) def forward ( self , x ): out = self . layer1 ( x ) out = self . layer2 ( out ) out = out . reshape ( out . size ( 0 ), - 1 ) logits = self . linear_1 ( out . view ( - 1 , 7 * 7 * 64 )) probas = F . softmax ( logits , dim = 1 ) return logits , probas model = ConvNetPyTorch ( num_classes ) . to ( device ) What you don't see is: Fit/train ( model.train() ) Evaluate with given metric ( model.eval() ) To add dropout after the nn.ReLU() layer (or even after the fully connected in any of these examples) a dropout function will be used, e.g. nn.Dropout(0.5) Sometimes another fully connected (dense) layer with, say, ReLU activation, is added right before the final fully connected layer.","title":"PyTorch"},{"location":"convolutional-in-layers-and-sequences/#tensorflow","text":"Below is a ConvNet defined with the Layers library and Estimators API in TensorFlow ( Ref ). This is a snippet with only the model definition parts - see the References for the full code example. import tensorflow as tf # Create the neural network def convNetTensorFlow ( x_dict , n_classes , reuse , is_training ): \"\"\"Adapted from: https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network.py \"\"\" # Define a scope for reusing the variables with tf . variable_scope ( 'ConvNet' , reuse = reuse ): # TF Estimator input is a dict, in case of multiple inputs x = x_dict [ 'images' ] # MNIST data input is a 1-D vector of 784 features (28*28 pixels) # Reshape to match picture format [Height x Width x Channel] # Tensor input become 4-D: [Batch Size, Height, Width, Channel] x = tf . reshape ( x , shape = [ - 1 , 28 , 28 , 1 ]) # Convolution Layer with 32 filters and a kernel size of 5 conv1 = tf . layers . conv2d ( x , 32 , 3 , activation = tf . nn . relu ) # Max Pooling (down-sampling) with strides of 2 and kernel size of 2 conv1 = tf . layers . max_pooling2d ( conv1 , 2 , 2 ) # Convolution Layer with 64 filters and a kernel size of 3 conv2 = tf . layers . conv2d ( conv1 , 64 , 3 , activation = tf . nn . relu ) # Max Pooling (down-sampling) with strides of 2 and kernel size of 2 conv2 = tf . layers . max_pooling2d ( conv2 , 2 , 2 ) # Flatten the data to a 1-D vector for the fully connected layer fc1 = tf . contrib . layers . flatten ( conv2 ) # Output layer, class prediction logits = tf . layers . dense ( fc1 , n_classes , activation = None ) return logits \"\"\"...[snipped for brevity]\"\"\" # Build the Estimator model = tf . estimator . Estimator ( model_fn ) What you don't see is: Fit/train ( model.train() ) Evaluate with given metric ( model.evaluate() ) To add dropout after the tf.layers.conv2d() layer (or even after the fully connected in any of these examples) a dropout function will be used, e.g. tf.layers.dropout(inputs=net_layer, rate=0.5, training=is_training) Sometimes another fully connected (dense) layer with, say, ReLU activation, is added right before the final fully connected layer. For more see tensorflow in the References below.","title":"Tensorflow"},{"location":"convolutional-in-layers-and-sequences/#cognitive-toolkit-cntk","text":"Below is a ConvNet defined with the Layer API in CNTK ( Ref ). This is a snippet with only the model definition parts - see the References for the full code example (Note: as of this writing CNTK is Windows or Linux only) import cntk as C def convNetCNTK ( features , num_output_classes ): \"\"\"https://cntk.ai/pythondocs/CNTK_103D_MNIST_ConvolutionalNeuralNetwork.html\"\"\" with C . layers . default_options ( init = C . glorot_uniform (), activation = C . relu ): model = C . layers . Sequential ([ C . layers . For ( range ( 2 ), lambda i : [ C . layers . Convolution (( 3 , 3 ), [ 32 , 64 ][ i ], pad = True ), C . layers . MaxPooling (( 2 , 2 ), strides = ( 2 , 2 )) ]), C . layers . Dense ( 64 ), C . layers . Dense ( out_dims , activation = None ) ]) return model ( features ) What you don't see is: Fit/train ( trainer = C.Trainer() and trainer.train_minibatch() ) Evaluate with given metric ( out = C.softmax() and out.eval() ) To add dropout after the C.layers.Convolution() layer (or even after the fully connected in any of these examples) a dropout function will be used, e.g. C.layers.Dropout(0.5) . Sometimes another fully connected (dense) layer with, say, ReLU activation, is added right before the final fully connected layer.","title":"Cognitive Toolkit (CNTK)"},{"location":"convolutional-in-layers-and-sequences/#conclusion","text":"No real conclusion except to say these frameworks do pretty much the same sorts of things and all have different API layers, high-level to low-level. I did find Keras to be the easiest and just as fast as TensorFlow - albeit with a lot abstracted away, which is generally good if one needs a quick jumping off point. The benchmarks are not official in full code sample notebook. The full code samples are in this Jupyter Notebook . Certainly some room for improvement in code and benchmarking so if you have any ideas, please leave a comment.","title":"Conclusion"},{"location":"convolutional-in-layers-and-sequences/#references","text":"Samples adapted in this post: Keras code sample with Sequential model Ref PyTorch code sample with Sequential container Ref TensorFlow code sample with Layers and Estimators APIs Ref and ConvNets Tutorial at this Doc CNTK code sample with Layer API Doc A great book from which I took some of the concepts written in this post: Book and Code Even more nice code samples: Kaggle Keras code sample: https://www.kaggle.com/tonypoe/keras-cnn-example?scriptVersionId=589403 Keras example: http://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/ PyTorch example: https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/convolutional_neural_network/main.py CNTK example: https://cntk.ai/pythondocs/CNTK_201B_CIFAR-10_ImageHandsOn.html TensorFlow Estimators example: https://jhui.github.io/2017/03/14/TensorFlow-Estimator/ Thanks for reading.","title":"References"},{"location":"convolutional-in-layers-and-sequences/#appendix","text":"Nice explanation of tensor layouts (PyTorch vs. TensorFlow) in a PyTorch forum post by Mamy Ratsimbazafy ( Post ): Furthermore there might be a difference due to the Tensor layouts: PyTorch use NCHW and Tensorflow uses NHWC, NCHW was the first layout supported by CuDNN but presents a big challenge for optimization (due to access patterns in convolutions, memory coalescing and such \u2026). NHWC is easier to optimize for convolutions but suffer in linear layers iirc because you have to physically transpose/permute the dimensions. Furthermore, due to it\u2019s dynamic nature, PyTorch allocate new memory at each new batch while Tensorflow can just reuse previous memory locations since size is known in advance. Memory is THE bottleneck in Deep Learning not CPU, the big challenge is how to feed data fast enough to the CPU and GPU to get the maximum GFLOPS throughput. /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */ var disqus_config = function () { this.page.url = 'https://michhar.github.io/convolutional-in-layers-and-sequences/'; // Replace PAGE_URL with your page's canonical URL variable this.page.identifier = 'happycat1'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable }; (function() { // DON'T EDIT BELOW THIS LINE var d = document, s = d.createElement('script'); s.src = 'https://michhar.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); Please enable JavaScript to view the comments powered by Disqus.","title":"Appendix"},{"location":"data-science-story-part1/","text":"tl;dr : My path into data science involved dabbling in a new programming language, stalking GitHub for a popular machine learning package, doing some networking and finding some other people to teach. Posted: 2017-05-15 Last week, I had the amazing opportunity of co-presenting a talk called \"Navigating the AI Revolution\" at Microsoft's developer conference, //build 2017. I spoke about questions we can answer with machine learning and mentioned a little bit about my path into data science . Afterwards, many folks approached me about this path and asked for some more guidance and advice. I thought I'd begin by expanding on my story of how I started out. My hope is that something in here is helpful to you. Before I was a data scientist I was a developer. TBH I'm now a developer again and getting to do more data science than ever before. One of my favorite articles of late states that the real prerequisite for machine learning is not math, but rather data analysis skills like data viz and wrangling (link here ). But what's next, i.e., how did I begin actually learning data science... Tip: Start doing data visualization and/or data cleaning At the beginning, this historic tweet by Josh Wills was both funny and helpful to me -- giving me perspective. Before I presented on AI to 700 people at //build, before I taught an ML package at a workshop, before I had read docs and gone through the tutorials and before I had trolled YouTube for ML videos, I had attended the Open Data Science Conference in the Fall of 2015. I networked, as we do , and met a group of devs/data scientists/physicists/biologists interested in data science like me (a great, diverse group of super smart people!) and immediately got involved in a Python workshop group (GitHub organization ). I got asked to help them give a workshop on ML, scikit-learn and tensorflow they were planning for the following year because I had a some basic Python materials they could use. Tip: Network with those in your field interested in data science Most of what I had learned was from one single package, scikit-learn which I chose based on its popularity and GitHub activity. I also studied videos on scikit-learn like this one by Jake VanderPlas and this one by Olivier Grisel. After some YouTube'ing and reading the docs, plus talking about it with other data scientists in my workshop-planning group, I was ready to teach the basics to others . The old adage is true, the best way to learn is to teach . Scikit-learn is a Python machine learning library with incredible docs that not only explain the package, actually taught me something useful once I had watched a couple of basic videos like the ones just mentioned (check out this page from the docs). The development of this package is now led by Andreas M\u00fcller, a lecturer at the Data Science Institute at Columbia University. He's also a big proponent of women as contributors to scikit-learn and open source, openly encouraging more and more to contribute. Tip: Pick a package to learn that is popular and active In May of 2016, I co-instructed a workshop (close to 200 folks, mostly women) at a Google office in Mountain View, CA. They thought it was funny when I said \"I'm now in the belly of the beast\" (I'm a Microsofty). I was still a new data scientist, but I knew how to use a powerful ML tool to understand some simple data (the iris dataset ) and could communicate that to others. I didn't start out teaching or creating courses by any means. I started out by being curious about analytics, for many years doing genomics and proteomics. I started dabbling in Python as necessary and a little R for my analyses and visualizations. I chose to write some teaching material because I wanted to learn Python better (you can start out small even with a readme or two on GitHub) and off I went. Tip: Pick a language like R or Python and plan to create some training material on something analytical for work or fun All of this certainly required experience in a new language, Python, adopted by the data science community along with R as the go-to programming language for its intuitive syntax, readability, and existing math libraries, among other reasons. I had been dabbling in Python for a few years, doing basic math and web programming. I really grokked Python, however, after I had created a Python course of my own (find it here ) \u2014 see a theme here? :)","title":"On Being a Data Scientist"},{"location":"deploy-with-azureml-cli-boldly/","text":"Posted: 2018-03-11 UPDATE 2018-09-27 : Please refer to this excellent new documentation on the newly released AML CLI Doc . Context I was looking for an easy way to deploy a machine learning model I'd trained for classification, built with Microsoft Cogntive Toolkit (CNTK), a deep learning framework. I wanted still to test locally with the Python code I wrote, then dockerize and test my image locally, as well. If the local image ran, I wished to, then, deploy the tested, dockerized service to a cluster for a realtime scoring endpoint (with just a handful of commands if possible - and, indeed, it was). This post is mainly about the commands to use for deploying with the new, in Preview, Azure ML CLI, however for example scoring files and schema with CNTK, see the References below. Prerequisites AzureML CLI (Install Using the CLI in this Doc ) Docker installed (for local service testing) - Ref A scoring script (see References for examples) Any other necessary files like labels or necessary pip installs in a requirements.txt Note: The following was all done in Jupyter on a Linux Ubuntu Data Science Virutal Machine ( Doc ) Overview Save for writing the actual code and installing the prerequisites, all can be done with the CLI, even running a quick service test call. The general story goes as follows. There is a local testing grounds and a remote cluster deployment in the overview outline. To Begin Write a scoring script that has run and init methods, with a main method to make the service payload schema (an example Ref ). The scoring script is packaged up for use later, but has a dual purpose of generating a schema for the service. Run this script to generate the schema. Package and deploy this script to make prediction service. Write a conda dependencies and/or pip install requirements file (this will have the reference to the CNTK wheel to install cntk into the docker image - we'll talk about in a second) Register three Environment Providers (for the cluster deployment) Create a Model Management Account in Azure There's an option to use one master, do-it-all, command or run separate commands as done here. The separate commands perform the following. For a Local Deployment test (always a good idea) Set up the local Environment in Azure and switch to it Register a model (the ML model, e.g. a saved CNTK model in a Protobuf based format) Create a manifest for all requirements to build an image (e.g. model, dependencies and can include multiple models) Create a docker image with the environment and pertinent files Create and deploy the service using the docker image For the Remote Cluster deployment Set up the remote cluster Environment in Azure and switch to it Create and deploy the service with the same image as from local deployment The Command Sequence After creating the scoring file, score.py here, and placing all necessary package installs into a requirements.txt file (for a Python package manager to use) we can begin our deployment. Deploy locally to test For most of the commands as reference see this Doc , however some more specific instructions are here that may be useful for a specialized framework model as can be made with CNTK or TensorFlow, for example. Other commands around setup are found in this Doc . These commands can be run in a Jupyter notebook, hence the bang, \"!\", preceding the command. If these are run on the command line please remove the \"!\". TIP: If on the Data Science Virtual Machine which has this CLI, you may need to run these commands a little differently (replace az with {sys.executable} -m azure.cli ). e.g. in a Jupyter code cell: # Get Azure ML CLI help on DSVM import sys ! {sys.executable} -m azure.cli ml -h Log into Azure Simply: # This will send a code to prompt for password through the browser ! az login Or if you've created a couple of system variables with the username and password: # Using two system variable, the user-defined Azure username and password ! az login --username \" $AZ_USER \" --password \" $AZ_PASS \" Register three Environment Providers To start the setup process, you need to register a few environment providers by entering the following commands ! az provider register -n Microsoft.MachineLearningCompute ! az provider register -n Microsoft.ContainerRegistry ! az provider register -n Microsoft.ContainerService Create a Model Management Account in Azure # ! az ml account modelmanagement create -l [Azure region, e.g. eastus2] -n [your account name] -g [resource group name] --sku-instances [number of instances, e.g. 1] --sku-name [Pricing tier for example S1] ! az ml account modelmanagement create -l westeurope -n happymodelmgmt -g happyprojrg --sku-instances 1 --sku-name S1 # az ml account modelmanagement set -n [your account name] -g [resource group it was created in] ! az ml account modelmanagement set -n happymodelmgmt -g happyprojrg Set up the local Environment in Azure and switch to it # az ml env setup -l [Azure Region, e.g. eastus2] -n [your environment name] [-g [existing resource group]] ! printf 'y' | az ml env setup -l \"West Europe\" -n localenv -g happyprojrg # az ml env show -n [environment name] -g [resource group] ! az ml env show -n localenv -g happyprojrg # az ml env set -n [environment name] -g [resource group] ! az ml env set -n localenv -g happyprojrg Register a model (the ML model, e.g. a saved CNTK model in a Protobuf based format) # Get help on this ! az ml model register --help This will output the model ID: # az ml model register --model [path to model file] --name [model name] ! az ml model register --model happy_classifier_cntk.model --name happy_classifier_cntk.registered.model # Show the registered models ! az ml model list -o table Create a manifest for all requirements to build an image # Get help on this ! az ml manifest create --help After having the requirements file (user generated list of pip installable packages needed) and the service schema file (representing the json payload for the service call which is created by running the main method in score.py mentioned above, e.g., python score.py ), one can create the manifest to hold this information along with other requirements. # az ml manifest create --manifest-name [your new manifest name] --model-id [model id] -f [path to code file] -r [runtime for the image, e.g. spark-py] -p [pip installs, e.g. requirements.txt] -d [extra files, e.g. a label file] -s [service schema. e.g. service_schema.json] --verbose --debug # Note must have requirements file and manifest name mustn't have underscores but rather '.' or '-' ! az ml manifest create --manifest-name happyclassifiermanifest --model-id [ model id from register command ] -r python -p requirements.txt -d target_set.txt -f score.py -s service_schema.json #--verbose --debug ! az ml manifest show -i [ manifest id ] Create a docker image with the environment and pertinent files # Ensure correct permissions for docker and add user to docker group ! sudo chmod -R ugo+rwx /var/run/ ! sudo usermod -aG docker [ your current user ] # Get help on this ! az ml image create --help This will produce an image ID: # az ml image create -n [image name] --manifest-id [the manifest ID] ! az ml image create -n happyclassifierimage --manifest-id [ manifest id ] # Get the usage in order to pull the image ! az ml image usage -i [ image id ] (Optional) Test the docker image # To log in as a docker user ! az ml env get-credentials -g happyprojrg -n localenv # Log in to docker and pull down image from ACR, then run ! docker login -u [ username ] -p [ password ] [ loginServer ] ! docker pull [ image name from usage command ] ! docker run [ image name from usage command ] Create and deploy the service using the docker image # az ml service create realtime --image-id [image id] -n [service name] ! printf 'y' | az ml service create realtime --image-id [ image id ] -n localhappywebservice # Get logs from creation in case something went wrong ! az ml service logs realtime -i localhappywebservice # az ml service usage realtime -i [service id] ! az ml service usage realtime -i localhappywebservice Test it # az ml service run realtime -i <service id> -d \"{\\\"input_df\\\": [{\\\"sepal length\\\": 3.0, \\\"sepal width\\\": 3.6, \\\"petal width\\\": 1.3, \\\"petal length\\\":0.25}]}\" # Note the removal in the json payload of the \"u\" or unicode designation from docs ! az ml service run realtime -i localhappywebservice -d \"{\\\"request_package\\\": {\\\"url\\\": \\\"https://contents.mediadecathlon.com/p350121/2000x2000/sq/mountaineering_boots_-_blue_standard_sizes41_42_43_44_45_46_simond_8324356_350121.jpg?k=362304aaf6fecd4b2c8750987a2fb104\\\"}}\" Deploy to a cluster Follow this Doc for more information on cluster deployment. Below are the pertinent commands working at of 2018-03-11. Set up the remote cluster Environment in Azure and switch to it # az ml env setup --cluster -n [your environment name] -l [Azure region e.g. eastus2] [-g [resource group]] ! printf 'n\\nY' | az ml env setup --cluster -n clusterenv -l westeurope -g happyprojrg # Is the environment ready? ! az ml env show -g happyprojrg -n clusterenv # Set the environment to the remote cluster ! az ml env set -g happyprojrg -n clusterenv # Set to same model management account as local ! az ml account modelmanagement set -n happymodelmgmt -g happyprojrg Create and deploy the service with the same image as from local deployment # One command to do it all from \"scratch\" # ! az ml service create realtime --model-file happy_classifier_cntk.model -f score.py -n remotehappywebservice -s service_schema.json -r python -p requirements.txt -d target_set.txt --verbose --debug # Remotely deployed kubernetes cluster for predicting and scoring new images with the model ! az ml service create realtime --image-id [ image id ] -n remotehappywebservice Test it ! az ml service run realtime -i [ service id ] -d \"{\\\"request_package\\\": {\\\"url\\\": \\\"https://contents.mediadecathlon.com/p350121/2000x2000/sq/mountaineering_boots_-_blue_standard_sizes41_42_43_44_45_46_simond_8324356_350121.jpg?k=362304aaf6fecd4b2c8750987a2fb104\\\"}}\" # Even though successful, might still take some time to deprovision everything in Azure ! az ml service delete realtime --id [ service id ] Finis! References Important Notes There are different input data type options for sending up to the service and you can specify this when you generate the schema for the service call. Install the Azure ML CLI into the system Python if using a DSVM and the main Python in a local setup with (from this Doc ): ! sudo pip install -r https://aka.ms/az-ml-o16n-cli-requirements-file When creating the image with the az ml cli, remember to include all files necessary with the -d flag such as any label or data files. Avoid using the -c flag for the conda dependencies file for the time being. If particluar installs are needed, a requirements.txt file can be used with the pip installable packages specified and this files should go after a -p flag. Overview Overview of Azure ML model management Doc Deployment walkthrough Ref More on Deployment Microsoft Blog on deploying from Azure ML Workbench and the Azure ML CLI Ref Setting up with the Azure ML CLI for deployment Doc Non-CLI deployment methods (AML alternative) Ref Scoring File and Schema Creation References Example of schema generation Doc Example of the scoring file showing a CNTK model and serializing an image as a PANDAS data type for input data to service Ref Example of the scoring file showing a scikit-learn model and a STANDARD data type (json) for input data to service Ref After creating a run and init methods as in the links above, plus a schema file, begin with \"Register a model\" found in this Doc Sample code from Azure on GitHub: Ref Docker Docker Docs Ref","title":"Deploying a Machine Learning Model Easily with Azure ML CLI"},{"location":"deploy-with-azureml-cli-boldly/#context","text":"I was looking for an easy way to deploy a machine learning model I'd trained for classification, built with Microsoft Cogntive Toolkit (CNTK), a deep learning framework. I wanted still to test locally with the Python code I wrote, then dockerize and test my image locally, as well. If the local image ran, I wished to, then, deploy the tested, dockerized service to a cluster for a realtime scoring endpoint (with just a handful of commands if possible - and, indeed, it was). This post is mainly about the commands to use for deploying with the new, in Preview, Azure ML CLI, however for example scoring files and schema with CNTK, see the References below.","title":"Context"},{"location":"deploy-with-azureml-cli-boldly/#prerequisites","text":"AzureML CLI (Install Using the CLI in this Doc ) Docker installed (for local service testing) - Ref A scoring script (see References for examples) Any other necessary files like labels or necessary pip installs in a requirements.txt Note: The following was all done in Jupyter on a Linux Ubuntu Data Science Virutal Machine ( Doc )","title":"Prerequisites"},{"location":"deploy-with-azureml-cli-boldly/#overview","text":"Save for writing the actual code and installing the prerequisites, all can be done with the CLI, even running a quick service test call. The general story goes as follows. There is a local testing grounds and a remote cluster deployment in the overview outline.","title":"Overview"},{"location":"deploy-with-azureml-cli-boldly/#to-begin","text":"Write a scoring script that has run and init methods, with a main method to make the service payload schema (an example Ref ). The scoring script is packaged up for use later, but has a dual purpose of generating a schema for the service. Run this script to generate the schema. Package and deploy this script to make prediction service. Write a conda dependencies and/or pip install requirements file (this will have the reference to the CNTK wheel to install cntk into the docker image - we'll talk about in a second) Register three Environment Providers (for the cluster deployment) Create a Model Management Account in Azure There's an option to use one master, do-it-all, command or run separate commands as done here. The separate commands perform the following.","title":"To Begin"},{"location":"deploy-with-azureml-cli-boldly/#for-a-local-deployment-test-always-a-good-idea","text":"Set up the local Environment in Azure and switch to it Register a model (the ML model, e.g. a saved CNTK model in a Protobuf based format) Create a manifest for all requirements to build an image (e.g. model, dependencies and can include multiple models) Create a docker image with the environment and pertinent files Create and deploy the service using the docker image","title":"For a Local Deployment test (always a good idea)"},{"location":"deploy-with-azureml-cli-boldly/#for-the-remote-cluster-deployment","text":"Set up the remote cluster Environment in Azure and switch to it Create and deploy the service with the same image as from local deployment","title":"For the Remote Cluster deployment"},{"location":"deploy-with-azureml-cli-boldly/#the-command-sequence","text":"After creating the scoring file, score.py here, and placing all necessary package installs into a requirements.txt file (for a Python package manager to use) we can begin our deployment.","title":"The Command Sequence"},{"location":"deploy-with-azureml-cli-boldly/#deploy-locally-to-test","text":"For most of the commands as reference see this Doc , however some more specific instructions are here that may be useful for a specialized framework model as can be made with CNTK or TensorFlow, for example. Other commands around setup are found in this Doc . These commands can be run in a Jupyter notebook, hence the bang, \"!\", preceding the command. If these are run on the command line please remove the \"!\". TIP: If on the Data Science Virtual Machine which has this CLI, you may need to run these commands a little differently (replace az with {sys.executable} -m azure.cli ). e.g. in a Jupyter code cell: # Get Azure ML CLI help on DSVM import sys ! {sys.executable} -m azure.cli ml -h Log into Azure Simply: # This will send a code to prompt for password through the browser ! az login Or if you've created a couple of system variables with the username and password: # Using two system variable, the user-defined Azure username and password ! az login --username \" $AZ_USER \" --password \" $AZ_PASS \" Register three Environment Providers To start the setup process, you need to register a few environment providers by entering the following commands ! az provider register -n Microsoft.MachineLearningCompute ! az provider register -n Microsoft.ContainerRegistry ! az provider register -n Microsoft.ContainerService Create a Model Management Account in Azure # ! az ml account modelmanagement create -l [Azure region, e.g. eastus2] -n [your account name] -g [resource group name] --sku-instances [number of instances, e.g. 1] --sku-name [Pricing tier for example S1] ! az ml account modelmanagement create -l westeurope -n happymodelmgmt -g happyprojrg --sku-instances 1 --sku-name S1 # az ml account modelmanagement set -n [your account name] -g [resource group it was created in] ! az ml account modelmanagement set -n happymodelmgmt -g happyprojrg Set up the local Environment in Azure and switch to it # az ml env setup -l [Azure Region, e.g. eastus2] -n [your environment name] [-g [existing resource group]] ! printf 'y' | az ml env setup -l \"West Europe\" -n localenv -g happyprojrg # az ml env show -n [environment name] -g [resource group] ! az ml env show -n localenv -g happyprojrg # az ml env set -n [environment name] -g [resource group] ! az ml env set -n localenv -g happyprojrg Register a model (the ML model, e.g. a saved CNTK model in a Protobuf based format) # Get help on this ! az ml model register --help This will output the model ID: # az ml model register --model [path to model file] --name [model name] ! az ml model register --model happy_classifier_cntk.model --name happy_classifier_cntk.registered.model # Show the registered models ! az ml model list -o table Create a manifest for all requirements to build an image # Get help on this ! az ml manifest create --help After having the requirements file (user generated list of pip installable packages needed) and the service schema file (representing the json payload for the service call which is created by running the main method in score.py mentioned above, e.g., python score.py ), one can create the manifest to hold this information along with other requirements. # az ml manifest create --manifest-name [your new manifest name] --model-id [model id] -f [path to code file] -r [runtime for the image, e.g. spark-py] -p [pip installs, e.g. requirements.txt] -d [extra files, e.g. a label file] -s [service schema. e.g. service_schema.json] --verbose --debug # Note must have requirements file and manifest name mustn't have underscores but rather '.' or '-' ! az ml manifest create --manifest-name happyclassifiermanifest --model-id [ model id from register command ] -r python -p requirements.txt -d target_set.txt -f score.py -s service_schema.json #--verbose --debug ! az ml manifest show -i [ manifest id ] Create a docker image with the environment and pertinent files # Ensure correct permissions for docker and add user to docker group ! sudo chmod -R ugo+rwx /var/run/ ! sudo usermod -aG docker [ your current user ] # Get help on this ! az ml image create --help This will produce an image ID: # az ml image create -n [image name] --manifest-id [the manifest ID] ! az ml image create -n happyclassifierimage --manifest-id [ manifest id ] # Get the usage in order to pull the image ! az ml image usage -i [ image id ] (Optional) Test the docker image # To log in as a docker user ! az ml env get-credentials -g happyprojrg -n localenv # Log in to docker and pull down image from ACR, then run ! docker login -u [ username ] -p [ password ] [ loginServer ] ! docker pull [ image name from usage command ] ! docker run [ image name from usage command ] Create and deploy the service using the docker image # az ml service create realtime --image-id [image id] -n [service name] ! printf 'y' | az ml service create realtime --image-id [ image id ] -n localhappywebservice # Get logs from creation in case something went wrong ! az ml service logs realtime -i localhappywebservice # az ml service usage realtime -i [service id] ! az ml service usage realtime -i localhappywebservice Test it # az ml service run realtime -i <service id> -d \"{\\\"input_df\\\": [{\\\"sepal length\\\": 3.0, \\\"sepal width\\\": 3.6, \\\"petal width\\\": 1.3, \\\"petal length\\\":0.25}]}\" # Note the removal in the json payload of the \"u\" or unicode designation from docs ! az ml service run realtime -i localhappywebservice -d \"{\\\"request_package\\\": {\\\"url\\\": \\\"https://contents.mediadecathlon.com/p350121/2000x2000/sq/mountaineering_boots_-_blue_standard_sizes41_42_43_44_45_46_simond_8324356_350121.jpg?k=362304aaf6fecd4b2c8750987a2fb104\\\"}}\"","title":"Deploy locally to test"},{"location":"deploy-with-azureml-cli-boldly/#deploy-to-a-cluster","text":"Follow this Doc for more information on cluster deployment. Below are the pertinent commands working at of 2018-03-11. Set up the remote cluster Environment in Azure and switch to it # az ml env setup --cluster -n [your environment name] -l [Azure region e.g. eastus2] [-g [resource group]] ! printf 'n\\nY' | az ml env setup --cluster -n clusterenv -l westeurope -g happyprojrg # Is the environment ready? ! az ml env show -g happyprojrg -n clusterenv # Set the environment to the remote cluster ! az ml env set -g happyprojrg -n clusterenv # Set to same model management account as local ! az ml account modelmanagement set -n happymodelmgmt -g happyprojrg Create and deploy the service with the same image as from local deployment # One command to do it all from \"scratch\" # ! az ml service create realtime --model-file happy_classifier_cntk.model -f score.py -n remotehappywebservice -s service_schema.json -r python -p requirements.txt -d target_set.txt --verbose --debug # Remotely deployed kubernetes cluster for predicting and scoring new images with the model ! az ml service create realtime --image-id [ image id ] -n remotehappywebservice Test it ! az ml service run realtime -i [ service id ] -d \"{\\\"request_package\\\": {\\\"url\\\": \\\"https://contents.mediadecathlon.com/p350121/2000x2000/sq/mountaineering_boots_-_blue_standard_sizes41_42_43_44_45_46_simond_8324356_350121.jpg?k=362304aaf6fecd4b2c8750987a2fb104\\\"}}\" # Even though successful, might still take some time to deprovision everything in Azure ! az ml service delete realtime --id [ service id ] Finis!","title":"Deploy to a cluster"},{"location":"deploy-with-azureml-cli-boldly/#references","text":"Important Notes There are different input data type options for sending up to the service and you can specify this when you generate the schema for the service call. Install the Azure ML CLI into the system Python if using a DSVM and the main Python in a local setup with (from this Doc ): ! sudo pip install -r https://aka.ms/az-ml-o16n-cli-requirements-file When creating the image with the az ml cli, remember to include all files necessary with the -d flag such as any label or data files. Avoid using the -c flag for the conda dependencies file for the time being. If particluar installs are needed, a requirements.txt file can be used with the pip installable packages specified and this files should go after a -p flag. Overview Overview of Azure ML model management Doc Deployment walkthrough Ref More on Deployment Microsoft Blog on deploying from Azure ML Workbench and the Azure ML CLI Ref Setting up with the Azure ML CLI for deployment Doc Non-CLI deployment methods (AML alternative) Ref Scoring File and Schema Creation References Example of schema generation Doc Example of the scoring file showing a CNTK model and serializing an image as a PANDAS data type for input data to service Ref Example of the scoring file showing a scikit-learn model and a STANDARD data type (json) for input data to service Ref After creating a run and init methods as in the links above, plus a schema file, begin with \"Register a model\" found in this Doc Sample code from Azure on GitHub: Ref Docker Docker Docs Ref","title":"References"},{"location":"ell/","text":"Building ELL for macOS Out of Microsoft Research and in early preview, is the Embedded Learning Library, a toolkit for deploying pre-made, small models to a resource contrained system, like a Raspberry Pi 3. The models are designed to run without any connection to the cloud resulting in a truly disconnected device use case. Walkthrough My macOS specs: As the writing of this article, release 2.3.0 of the Embedded Learning Library (ELL) was successfully built. On macOS, to build, follow this tutorial: https://github.com/Microsoft/ELL/blob/master/INSTALL-Mac.md , except for the llvm install (see below) and ensure that you are using the full path to llvm when using cmake , as in: cmake -DLLVM_DIR=/usr/local/opt/llvm/lib/cmake/llvm .. The following is based on reference (1) (see References ). Get llvm 3.9.x (need 3.9 to work with ELL): brew install --with-toolchain llvm Get a list of local version and the active version: brew search llvm Optional (in case you don't get llvm 3.9.x): brew update brew upgrade Find the binaries: (brew --prefix llvm)/bin Added to ~/.bashrc file: # For building ELL export LDFLAGS = \"-L/usr/local/opt/llvm/lib -Wl,-rpath,/usr/local/opt/llvm/lib\" export CPPFLAGS = \"-I/usr/local/opt/llvm/include -I/usr/local/opt/llvm/include/c++/v1/\" export CC = /usr/local/opt/llvm/bin/clang export CXX = /usr/local/opt/llvm/bin/clang++ source ~/.bashrc To build the python bindings ensure you run make with the right flag: make _ELL_python My Troubleshooting Getting the right C and C++ compiler specified - these must be the llvm versions for ELL to build. If something goes awry and the build continuously fails, the last resort is to delete the whole build folder and then recreate it and follow the rest of the instructions. Make sure using the brew installed packages. These should be the ones in the /usr/local/opt . Follow tutorial instructions for using the right LLVM module at their troubleshooting References https://embeddedartistry.com/blog/2017/2/20/installing-clangllvm-on-osx","title":"Building and Running ELL on MacOS for Object Detection Part 1"},{"location":"ell/#building-ell-for-macos","text":"Out of Microsoft Research and in early preview, is the Embedded Learning Library, a toolkit for deploying pre-made, small models to a resource contrained system, like a Raspberry Pi 3. The models are designed to run without any connection to the cloud resulting in a truly disconnected device use case.","title":"Building ELL for macOS"},{"location":"ell/#walkthrough","text":"My macOS specs: As the writing of this article, release 2.3.0 of the Embedded Learning Library (ELL) was successfully built. On macOS, to build, follow this tutorial: https://github.com/Microsoft/ELL/blob/master/INSTALL-Mac.md , except for the llvm install (see below) and ensure that you are using the full path to llvm when using cmake , as in: cmake -DLLVM_DIR=/usr/local/opt/llvm/lib/cmake/llvm .. The following is based on reference (1) (see References ). Get llvm 3.9.x (need 3.9 to work with ELL): brew install --with-toolchain llvm Get a list of local version and the active version: brew search llvm Optional (in case you don't get llvm 3.9.x): brew update brew upgrade Find the binaries: (brew --prefix llvm)/bin Added to ~/.bashrc file: # For building ELL export LDFLAGS = \"-L/usr/local/opt/llvm/lib -Wl,-rpath,/usr/local/opt/llvm/lib\" export CPPFLAGS = \"-I/usr/local/opt/llvm/include -I/usr/local/opt/llvm/include/c++/v1/\" export CC = /usr/local/opt/llvm/bin/clang export CXX = /usr/local/opt/llvm/bin/clang++ source ~/.bashrc To build the python bindings ensure you run make with the right flag: make _ELL_python","title":"Walkthrough"},{"location":"ell/#my-troubleshooting","text":"Getting the right C and C++ compiler specified - these must be the llvm versions for ELL to build. If something goes awry and the build continuously fails, the last resort is to delete the whole build folder and then recreate it and follow the rest of the instructions. Make sure using the brew installed packages. These should be the ones in the /usr/local/opt . Follow tutorial instructions for using the right LLVM module at their troubleshooting","title":"My Troubleshooting"},{"location":"ell/#references","text":"https://embeddedartistry.com/blog/2017/2/20/installing-clangllvm-on-osx","title":"References"},{"location":"how-i-built-pytorch-gpu/","text":"tl;dr : Notes on building PyTorch 1.0 Preview and other versions from source including LibTorch, the PyTorch C++ API for fast inference with a strongly typed, compiled language. So fast. Posted: 2018-11-10 Introduction I'd like to share some notes on building PyTorch from source from various releases using commit ids. This process allows you to build from any commit id, so you are not limited to a release number only. I've used this to build PyTorch with LibTorch for Linux amd64 with an NVIDIA GPU and Linux aarch64 (e.g. NVIDIA Jetson TX2). Instructions Create a shell script with the following contents (this being only an example) and refer to rest of post for possible changes you may have to make. # Post 1.0rc1 for a few fixes I needed PYTORCH_COMMIT_ID = \"8619230\" # Clone, checkout specific commit and build for GPU with CUDA support git clone https://github.com/pytorch/pytorch.git && \\ cd pytorch && git checkout ${ PYTORCH_COMMIT_ID } && \\ git submodule update --init --recursive && \\ pip3 install pyyaml == 3 .13 && \\ pip3 install -r requirements.txt && \\ USE_OPENCV = 1 \\ BUILD_TORCH = ON \\ CMAKE_PREFIX_PATH = \"/usr/bin/\" \\ LD_LIBRARY_PATH = /usr/local/cuda/lib64:/usr/local/lib: $LD_LIBRARY_PATH \\ CUDA_BIN_PATH = /usr/local/cuda/bin \\ CUDA_TOOLKIT_ROOT_DIR = /usr/local/cuda/ \\ CUDNN_LIB_DIR = /usr/local/cuda/lib64 \\ CUDA_HOST_COMPILER = cc \\ USE_CUDA = 1 \\ USE_NNPACK = 1 \\ CC = cc \\ CXX = c++ \\ TORCH_CUDA_ARCH_LIST = \"3.5 5.2 6.0 6.1+PTX\" \\ TORCH_NVCC_FLAGS = \"-Xfatbin -compress-all\" \\ python3 setup.py bdist_wheel # Install the Python wheel (includes LibTorch) pip3 install dist/*.whl # Clean up resources rm -fr pytorch Note, the size of the binary/wheel can be up to 180 MB. Build flag meanings USE_OPENCV=1 - build with OpenCV support BUILD_TORCH=ON - build LibTorch (C++ API) CMAKE_PREFIX_PATH=\"/usr/bin/\" - where to find Python LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/lib:$LD_LIBRARY_PATH - build lib paths CUDA_BIN_PATH=/usr/local/cuda/bin - where to find current CUDA CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda/ - where to find current CUDA Toolkit CUDNN_LIB_DIR=/usr/local/cuda/lib64 - where to find cuDNN install CUDA_HOST_COMPILER=cc - sets the host compiler to be used by nvcc USE_CUDA=1 - compile with CUDA support USE_NNPACK=1 - compile with cuDNN CC=cc - which C compiler to use for PyTorch build CXX=c++ - which C++ compiler to use for PyTorch build TORCH_CUDA_ARCH_LIST=\"3.5 5.2 6.0 6.1+PTX\" - GPU architectures to accomodate TORCH_NVCC_FLAGS=\"-Xfatbin -compress-all\" - extra nvcc (NVIDIA CUDA compiler driver) flags Changes to script that may be necessary Update pip3 to pip as necessary (However, it's recommended to build with Python 3 system installs) Update CMAKE_PREFIX_PATH to your bin where Python lives Update PYTORCH_COMMIT_ID to one you wish to use. Official release commit ids are v0.3.1 - 2b47480 (which I still needed for a project) v0.4.0 - 3749c58 v0.4.1 - a24163a v1.0rc1 - ff608a9 If compiling on macOS, update to the following: CC=clang CXX=clang++ CUDA_HOST_COMPILER=clang To compile without CUDA support (e.g. on CPU-only), update to the following: USE_CUDA=0 USE_NNPACK=0 Conclusion Given the right hardware (Linux amd64 or even aarch64 like a TX2) - the above script will work to build PyTorch and LibTorch. Leave a comment if you wish - issues or suggestions welcome. References PyTorch official build instructions PyTorch official Dockerfile Micheleen's GPU VM Dockerfile with a PyTorch+LibTorch build included NVCC from NVIDIA Docs Thank Yous To PyTorch GitHub Issues with great activity and insights ( https://github.com/pytorch/pytorch/issues ) and the official PyTorch Forums ( https://discuss.pytorch.org/ ). /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */ var disqus_config = function () { this.page.url = 'https://michhar.github.io/how-i-built-pytorch-gpu/'; // Replace PAGE_URL with your page's canonical URL variable this.page.identifier = 'happycat2'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable }; (function() { // DON'T EDIT BELOW THIS LINE var d = document, s = d.createElement('script'); s.src = 'https://michhar.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); Please enable JavaScript to view the comments powered by Disqus.","title":"Building PyTorch with LibTorch From Source with CUDA Support"},{"location":"how-i-built-pytorch-gpu/#introduction","text":"I'd like to share some notes on building PyTorch from source from various releases using commit ids. This process allows you to build from any commit id, so you are not limited to a release number only. I've used this to build PyTorch with LibTorch for Linux amd64 with an NVIDIA GPU and Linux aarch64 (e.g. NVIDIA Jetson TX2).","title":"Introduction"},{"location":"how-i-built-pytorch-gpu/#instructions","text":"Create a shell script with the following contents (this being only an example) and refer to rest of post for possible changes you may have to make. # Post 1.0rc1 for a few fixes I needed PYTORCH_COMMIT_ID = \"8619230\" # Clone, checkout specific commit and build for GPU with CUDA support git clone https://github.com/pytorch/pytorch.git && \\ cd pytorch && git checkout ${ PYTORCH_COMMIT_ID } && \\ git submodule update --init --recursive && \\ pip3 install pyyaml == 3 .13 && \\ pip3 install -r requirements.txt && \\ USE_OPENCV = 1 \\ BUILD_TORCH = ON \\ CMAKE_PREFIX_PATH = \"/usr/bin/\" \\ LD_LIBRARY_PATH = /usr/local/cuda/lib64:/usr/local/lib: $LD_LIBRARY_PATH \\ CUDA_BIN_PATH = /usr/local/cuda/bin \\ CUDA_TOOLKIT_ROOT_DIR = /usr/local/cuda/ \\ CUDNN_LIB_DIR = /usr/local/cuda/lib64 \\ CUDA_HOST_COMPILER = cc \\ USE_CUDA = 1 \\ USE_NNPACK = 1 \\ CC = cc \\ CXX = c++ \\ TORCH_CUDA_ARCH_LIST = \"3.5 5.2 6.0 6.1+PTX\" \\ TORCH_NVCC_FLAGS = \"-Xfatbin -compress-all\" \\ python3 setup.py bdist_wheel # Install the Python wheel (includes LibTorch) pip3 install dist/*.whl # Clean up resources rm -fr pytorch Note, the size of the binary/wheel can be up to 180 MB.","title":"Instructions"},{"location":"how-i-built-pytorch-gpu/#build-flag-meanings","text":"USE_OPENCV=1 - build with OpenCV support BUILD_TORCH=ON - build LibTorch (C++ API) CMAKE_PREFIX_PATH=\"/usr/bin/\" - where to find Python LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/lib:$LD_LIBRARY_PATH - build lib paths CUDA_BIN_PATH=/usr/local/cuda/bin - where to find current CUDA CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda/ - where to find current CUDA Toolkit CUDNN_LIB_DIR=/usr/local/cuda/lib64 - where to find cuDNN install CUDA_HOST_COMPILER=cc - sets the host compiler to be used by nvcc USE_CUDA=1 - compile with CUDA support USE_NNPACK=1 - compile with cuDNN CC=cc - which C compiler to use for PyTorch build CXX=c++ - which C++ compiler to use for PyTorch build TORCH_CUDA_ARCH_LIST=\"3.5 5.2 6.0 6.1+PTX\" - GPU architectures to accomodate TORCH_NVCC_FLAGS=\"-Xfatbin -compress-all\" - extra nvcc (NVIDIA CUDA compiler driver) flags","title":"Build flag meanings"},{"location":"how-i-built-pytorch-gpu/#changes-to-script-that-may-be-necessary","text":"Update pip3 to pip as necessary (However, it's recommended to build with Python 3 system installs) Update CMAKE_PREFIX_PATH to your bin where Python lives Update PYTORCH_COMMIT_ID to one you wish to use. Official release commit ids are v0.3.1 - 2b47480 (which I still needed for a project) v0.4.0 - 3749c58 v0.4.1 - a24163a v1.0rc1 - ff608a9 If compiling on macOS, update to the following: CC=clang CXX=clang++ CUDA_HOST_COMPILER=clang To compile without CUDA support (e.g. on CPU-only), update to the following: USE_CUDA=0 USE_NNPACK=0","title":"Changes to script that may be necessary"},{"location":"how-i-built-pytorch-gpu/#conclusion","text":"Given the right hardware (Linux amd64 or even aarch64 like a TX2) - the above script will work to build PyTorch and LibTorch. Leave a comment if you wish - issues or suggestions welcome.","title":"Conclusion"},{"location":"how-i-built-pytorch-gpu/#references","text":"PyTorch official build instructions PyTorch official Dockerfile Micheleen's GPU VM Dockerfile with a PyTorch+LibTorch build included NVCC from NVIDIA Docs","title":"References"},{"location":"how-i-built-pytorch-gpu/#thank-yous","text":"To PyTorch GitHub Issues with great activity and insights ( https://github.com/pytorch/pytorch/issues ) and the official PyTorch Forums ( https://discuss.pytorch.org/ ). /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */ var disqus_config = function () { this.page.url = 'https://michhar.github.io/how-i-built-pytorch-gpu/'; // Replace PAGE_URL with your page's canonical URL variable this.page.identifier = 'happycat2'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable }; (function() { // DON'T EDIT BELOW THIS LINE var d = document, s = d.createElement('script'); s.src = 'https://michhar.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); Please enable JavaScript to view the comments powered by Disqus.","title":"Thank Yous"},{"location":"how-i-setup-cntk-on-macos/","text":"tl;dr : Notes on building CNTK (Cognitive Toolkit) on macOS. Posted: 2018-05-16 Following: https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-CNTK-on-Linux with some modifications. Ensure XCode is installed Proceed with steps in article with the following modifications. My System (by the way) g++ --version : Apple LLVM version 9.0.0 (clang-900.0.39.2) Target: x86_64-apple-darwin17.3.0 Thread model: posix InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin MKL For https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-CNTK-on-Linux#mkl (MKL install) use the following: Create the directory for MKL: sudo mkdir /usr/local/mklmil Create a file build_mklml.sh and add the following to it. sudo wget https://github.com/intel/mkl-dnn/releases/download/v0.14/mklml_mac_2018.0.3.20180406.tgz && \\ sudo tar -xzf mklml_mac_2018.0.3.20180406.tgz -C /usr/local/mklml && \\ wget --no-verbose -O - https://github.com/01org/mkl-dnn/archive/v0.12.tar.gz | tar -xzf - && \\ cd mkl-dnn-0.12 && \\ ln -s /usr/local external && \\ mkdir -p build && \\ cd build && \\ cmake .. && \\ make && \\ make install && \\ cd ../.. && \\ rm -rf mkl-dnn-0.12 Modify build_mklml.sh to be executible ( chmod +x build_mklml.sh ) and run ( ./build_mklml.sh ). OpenMPI This is not a modification, just a code snippet for a bash script. wget https://www.open-mpi.org/software/ompi/v1.10/downloads/openmpi-1.10.3.tar.gz && \\ tar -xzvf ./openmpi-1.10.3.tar.gz && \\ cd openmpi-1.10.3 && \\ ./configure --prefix = /usr/local/mpi && \\ make -j all && \\ sudo make install Protobuf Skipped the apt-get commands. This is not a modification, just a code snippet for a bash script. wget https://github.com/google/protobuf/archive/v3.1.0.tar.gz && \\ tar -xzf v3.1.0.tar.gz && \\ cd protobuf-3.1.0 && \\ ./autogen.sh && \\ ./configure CFLAGS = -fPIC CXXFLAGS = -fPIC --disable-shared --prefix = /usr/local/protobuf-3.1.0 && \\ make -j $( nproc ) && \\ sudo make install ZLIB I did not need to apt-get or download/install this library as it was already installed, but if not on your system and the correct version, follow the instructions in the article. LIBZIP This is not a modification, just a code snippet for a bash script. wget http://nih.at/libzip/libzip-1.1.2.tar.gz && \\ tar -xzvf ./libzip-1.1.2.tar.gz && \\ cd libzip-1.1.2 && \\ ./configure && \\ make -j all && \\ sudo make install Boost Skipped the apt-get commands. Ran as a bash script: It can take some time.","title":"Building CNTK on MacOS"},{"location":"how-i-setup-cntk-on-macos/#my-system-by-the-way","text":"g++ --version : Apple LLVM version 9.0.0 (clang-900.0.39.2) Target: x86_64-apple-darwin17.3.0 Thread model: posix InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin","title":"My System (by the way)"},{"location":"how-i-setup-cntk-on-macos/#mkl","text":"For https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-CNTK-on-Linux#mkl (MKL install) use the following: Create the directory for MKL: sudo mkdir /usr/local/mklmil Create a file build_mklml.sh and add the following to it. sudo wget https://github.com/intel/mkl-dnn/releases/download/v0.14/mklml_mac_2018.0.3.20180406.tgz && \\ sudo tar -xzf mklml_mac_2018.0.3.20180406.tgz -C /usr/local/mklml && \\ wget --no-verbose -O - https://github.com/01org/mkl-dnn/archive/v0.12.tar.gz | tar -xzf - && \\ cd mkl-dnn-0.12 && \\ ln -s /usr/local external && \\ mkdir -p build && \\ cd build && \\ cmake .. && \\ make && \\ make install && \\ cd ../.. && \\ rm -rf mkl-dnn-0.12 Modify build_mklml.sh to be executible ( chmod +x build_mklml.sh ) and run ( ./build_mklml.sh ).","title":"MKL"},{"location":"how-i-setup-cntk-on-macos/#openmpi","text":"This is not a modification, just a code snippet for a bash script. wget https://www.open-mpi.org/software/ompi/v1.10/downloads/openmpi-1.10.3.tar.gz && \\ tar -xzvf ./openmpi-1.10.3.tar.gz && \\ cd openmpi-1.10.3 && \\ ./configure --prefix = /usr/local/mpi && \\ make -j all && \\ sudo make install","title":"OpenMPI"},{"location":"how-i-setup-cntk-on-macos/#protobuf","text":"Skipped the apt-get commands. This is not a modification, just a code snippet for a bash script. wget https://github.com/google/protobuf/archive/v3.1.0.tar.gz && \\ tar -xzf v3.1.0.tar.gz && \\ cd protobuf-3.1.0 && \\ ./autogen.sh && \\ ./configure CFLAGS = -fPIC CXXFLAGS = -fPIC --disable-shared --prefix = /usr/local/protobuf-3.1.0 && \\ make -j $( nproc ) && \\ sudo make install","title":"Protobuf"},{"location":"how-i-setup-cntk-on-macos/#zlib","text":"I did not need to apt-get or download/install this library as it was already installed, but if not on your system and the correct version, follow the instructions in the article.","title":"ZLIB"},{"location":"how-i-setup-cntk-on-macos/#libzip","text":"This is not a modification, just a code snippet for a bash script. wget http://nih.at/libzip/libzip-1.1.2.tar.gz && \\ tar -xzvf ./libzip-1.1.2.tar.gz && \\ cd libzip-1.1.2 && \\ ./configure && \\ make -j all && \\ sudo make install","title":"LIBZIP"},{"location":"how-i-setup-cntk-on-macos/#boost","text":"Skipped the apt-get commands. Ran as a bash script: It can take some time.","title":"Boost"},{"location":"how-to-bot-on-mac/","text":"Posted: 2016-11-07 Command Line Emulator for the Bot Framework - interacting with ocrbot on Shakespeare UPDATE: The Command Line Emulator has been replaced with a full-fledged emulator which is cross-platform (info and install here ). tl;dr : I built a simple OCR bot using the Bot Framework (BF) from Microsoft and the Cognitive Services Computer Vision API. I chose the Node.js Bot Builder SDK from BF. I tested with the BF's unix-compatible emulator (black box above). It was nice and straightforward the whole way through. All of the instructions are here , but feel free to keep reading. There's really almost too much to say about chat bots: definitions, bot builders, history ( more ), warnings and cautionary tales ( more ), guidelines, delightful stories, sad stories, etc. For our purposes, other than a few considerations, I'll sum it up with: they've been around for decades and will be for many more. However, let's cover some vocabulary to start. A blog post here details the different types of chat bots, progressing from simple to ones that can hold down a conversation. These are the bot types discussed and an example to illustrate: Notifier - sends a one-way message e.g. ping me with today's weather forecast at the start of the day (\"push\" bot i.e. bot initiates) Reactor - replies when I send a message, but does not remember anything e.g. send me the weather forecast when I ask for it (\"pull bot\" i.e. I initiate), but don't remember me or what I ask for Responder - replies and remembers me and my message history e.g. send me today's weather forecast, use my user name on this channel, and remember what cities I choose Conversationalist - replies, remembers me and my message history, knows what service I'm on, if there are others there, and when I come and go e.g. send me today's weather forecast, use my user name on this channel, remember what cities I choose, format it nicely for this channel, and if the conversation is old, archive it and send as email Bot builders lower the activation barrier for developing bots and the MS Bot Framework (BF) Bot Builder SDKs give us a wealth of methods for building dialog and user prompts, making the creation of effective waterfalls really easy. Along with the SDKs, the BF provides free emulator tools, a Windows-compatible desktop app and a Mac OS X / Linux-compatible console app (more information on emulators here ). I know you've been waiting to dive into the code, so let's begin... There are two choices on bot builder functions for testing locally. We can use the ConsoleConnector which simply and directly allows us to run our Node.js code on the command line. Using the bot builder SDK our code is pretty concise (see more examples on the Core Concepts page for the BF here ): var builder = require ( 'botbuilder' ); // Create a bot and connect it to the console var connector = new builder . ConsoleConnector (). listen (); var bot = new builder . UniversalBot ( connector ); Interacting with ocrbot could look like: It's simple, but we don't get to see the actual JSON that gets passed to the bot and the JSON passed back. If we want to be able to see the message and also write code that can be used for production later, the Bot Framework Emulator is the way to go. Note, this is the beginning of my server.js Node.js file - see my ocrbot github repo for the complete project and code and the included lab file for more instructions on doing this at home. We replace ConsoleConnector with ChatConnector , for a full deployment-compatible setup, as follows: var restify = require ( 'restify' ); var builder = require ( 'botbuilder' ); // Create bot var connector = new builder . ChatConnector ( botConnectorOptions ); var bot = new builder . UniversalBot ( connector ); // Setup Restify Server var server = restify . createServer (); // Handle Bot Framework messages server . post ( '/api/messages' , connector . listen ()); // Serve a static web page - for testing deployment server . get ( /.*/ , restify . serveStatic ({ 'directory' : '.' , 'default' : 'index.html' })); server . listen ( process . env . port || process . env . PORT || 3978 , function () { console . log ( '%s listening to %s' , server . name , server . url ); }); One of the major reasons I used github to host this project is that (and outlined in a later blog, TBD) it afforded me the ability to do a continuous deployment directly from the repo. Any change I push up, immediately reflects in my bot on whichever channel I'm on - it was actually pretty astonishing to see a quick typo fix show up so immediately when chatting to my bot at the same time as pushing up the change. But I'll save this for the deployment article to come. I've always been a fan of command terminals, so even on a Windows machine I'd probably choose to download and use the BF Command Terminal Emulator (download instructions are here ). Honestly, I enjoy the simplicity and control a command prompt affords. And now I can develop bots in the exact same way agnostic of OS. So, I decided to create a bot I could give image links to and if there was text in the picture it would reply with that text. Pretty simple and straightforward since I knew about the free subscriptions to Microsoft Cognitive Services and in particular the Vision APIs. I went to the Cognitive Services main page and clicked on My Account and signed up (or I could have signed up this way with MS, github, or LinkedIn accounts). After that I had a secret key for the service. Now to splice that into my bot. So, I borrowed much of my code from Samuele Resca's blog post (excellent blog, btw). I placed these helper methods in with the server.js code above: //========================================================= // URL Helpers //========================================================= var extractUrl = function _extractUrl ( message ) { if ( message . type !== \"message\" ) return ; if ( typeof message . attachments !== \"undefined\" && message . attachments . length > 0 ) { return message . attachments [ 0 ]. contentUrl ; } if ( typeof message . text !== \"\" ) { return _findUrl ( message . text ); } return \"\" ; }; function _findUrl ( text ) { var source = ( text || '' ). toString (); var matchArray ; // Regular expression to find FTP, HTTP(S) and email URLs. var regexToken = /(((http|https?):\\/\\/)[\\-\\w@:%_\\+.~#?,&\\/\\/=]+)/g ; // Iterate through any URLs in the text. if (( matchArray = regexToken . exec ( source )) !== null ) { var token = matchArray [ 0 ]; return token ; } return \"\" ; } Then I set up a config file as shown in the repository containing my Computer Vision API key (as well as placeholders for the future app id and app password I get after the deployment step - in a followup article TBD). So, don't worry about the app id and app password for now. If you are doing this at home, replace the process.env.VISION_API_KEY with a string holding your key (this is a separate file I called configuration.js ), see instructions in this lab file . var _config = { // Ignore this part for now... CHAT_CONNECTOR : { APP_ID : process . env . MICROSOFT_APP_ID , //You can obtain an APP ID and PASSWORD here: https://dev.botframework.com/bots/new APP_PASSWORD : process . env . MICROSOFT_APP_PASSWORD }, // Replace the API_KEY below with yours from the free trial COMPUTER_VISION_SERVICE : { API_URL : \"https://api.projectoxford.ai/vision/v1.0/\" , API_KEY : process . env . VISION_API_KEY //You can obtain an COGNITIVE SERVICE API KEY: https://www.microsoft.com/cognitive-services/en-us/pricing } }; exports . CONFIGURATIONS = _config ; Then back in my server.js file with the main code I set up my vision OCR functions to read the image text (basically call out the the Computer Vision service with an image url) and then process the text: //========================================================= // Vision Service //========================================================= var request = require ( \"request\" ); var readImageText = function _readImageText ( url , callback ) { var options = { method : 'POST' , url : config . CONFIGURATIONS . COMPUTER_VISION_SERVICE . API_URL + \"ocr/\" , headers : { 'ocp-apim-subscription-key' : config . CONFIGURATIONS . COMPUTER_VISION_SERVICE . API_KEY , 'content-type' : 'application/json' }, body : { url : url , language : \"en\" }, json : true }; request ( options , callback ); }; var extractText = function _extractText ( bodyMessage ) { if ( typeof bodyMessage . regions === \"undefined\" ) return \"\" ; var regs = bodyMessage . regions ; if ( typeof regs [ 0 ] !== \"undefined\" && regs [ 0 ]. lines . length > 0 ) { text = \"\" ; var lines = regs [ 0 ]. lines ; // For all lines in image ocr result // grab the text in the words array for ( i = 0 ; i < lines . length ; i ++ ) { var words = lines [ i ]. words ; for ( j = 0 ; j < words . length ; j ++ ) { text += \" \" + words [ j ]. text + \" \" ; } } return text ; } return \"Sorry, I can't find text in it :( !\" ; }; Finally, we tie it all together in a user dialog: //========================================================= // Bots Dialogs //========================================================= bot . dialog ( '/' , function ( session ) { // Use our url helper method var extractedUrl = extractUrl ( session . message ); // Nothing returned? Ask for an image url again... if ( extractedUrl === \"\" ) { session . send ( \"Hello. I'm an OCRBot. Please give me an image link and I'll look for words.\" ); } // Get a valid url? Send it off to the Vision API for OCR... readImageText ( extractedUrl , function ( error , response , body ) { session . send ( extractText ( body )); }) }); We could have added a waterfall here to say confirm they want to process this image or perhaps added a built-in prompt that allows uploading an image attachment. Perhaps in future releases. Now we test. On Windows one would double click on the BFEmulator.exe directly and on Mac we use the Mono framework software to run BFEmulator.exe from the command line. Pretty easy, peasy. Let's take this image: The emulator experience will look a little something like: Command Line Emulator for the Bot Framework - interacting with ocrbot locally on stats In conclusion, to get started now, check out the github repo and start playing with the code and emulator. For more instructions check out this lab . Chat later!","title":"Building an OCR Chat Bot with the Microsoft Bot Framework on my Mac"},{"location":"javascript-and-python-have-a-party/","text":"Posted: 2017-02-18 A little exchange Python : \"I want your silly game to have a starting value of 100 for life points for all players.\" Javascript : \"I read you loud and clear. Let's take that starting life and play this silly game. Alfred and Wallace are on the same side battling orcs. Alfred decides to give life to Wallace because Wallace appears to be about to bravely charge onto the battle field and may need it. Done. That's all the game can do right now because I just started making it.\" Python : \"Let's see...oh wow. Your game code actually worked. Alfred now has 99 life points and Wallace has 101. Good job us.\" I went on a hunt to find the cleanest and most succinct way to pass a variable from Python to Javascript and then back to Python in a Jupyter notebook. I wanted a proof of principle upon which I could later base my D3 graphics using Python data as pandas dataframes. So, this silly example can be translated into code as follows. First, things are actually easy for us because we are in a Jupyter notebook living in a browser, utilizing all sorts of widgets and HTML elements already. We can tap into that (and actually the DOM) to get some of the functionality we require. So, in Python we can use the IPython.display module with the HTML function allowing us to embed an element for later use. # Python from IPython.display import HTML pystartlife = str ( 100 ) HTML ( \"<div id='textid'>\" + pystartlife + \"</div>\" ) We've created a Python variable, pystartlife , and embedded it as a div element, literally just using raw HTML. Now we use the Javascript magics ( %%javascript ) to create a Javascript coding environment for the next cell. (BTW there are magics for many more languages - very cool.) In the Javascript, now, we grab the div element with the Python variable from the document or webpage and play our game. We also write the game. (This game is based on a Node.js tutorial by thenewboston on YouTube - thank you Bucky!). %% javascript // Get the python variable from the DOM var startlife = document . getElementById ( 'textid' ). innerHTML ; // Define a User class with a method function User () { this . name = '' ; this . life = Number ( startlife ); this . giveLife = function giveLife ( targetPlayer ) { targetPlayer . life += 1 ; this . life -= 1 ; } } // Use class var Alfred = new User (); var Wallace = new User (); // Names were blank so give them name values Alfred . name = 'Alfred' ; Wallace . name = 'Wallace' ; // Let's play a game! // Let Alfred give life to Wallace Alfred . giveLife ( Wallace ); // Save these variables back to python variables to work with later IPython . notebook . kernel . execute ( 'Alfred_life=\"' + Alfred . life + '\";' ); IPython . notebook . kernel . execute ( 'Wallace_life=\"' + Wallace . life + '\";' ); We grab the Python variable now embedded with document.getElementById('idname').innerHTML , a DOM function and attribute which grabs the div by its id (not name) and takes what is in the text part of the div . Then we define a class in Javascript that contains an altruistic method for giving life. We create some users and give them names. We then \"play\" the game and allow one to give life to another with the method in the User class - how generous of Alfred! Lastly, the lovely magic part, we use this sneaky Javascript function from the IPython class that executes Python statements. We could execute any Python statement really in this way, e.g., IPython.notebook.kernel.execute('print(\"Hello World!\")'); We include our life values (Javascript variables) into this executable string and, well, execute it. And to see if the magic worked, we run a simple couple of print statements back in Python: # Python print ( Alfred_life ) print ( Wallace_life ) With low and behold and output of: 99 and 101. Way to pay it forward, Alfred! You can find this code and more in this Jupyter notebook.","title":"Javascript and Python Meet through Magic and IPython"},{"location":"javascript-and-python-have-a-party/#a-little-exchange","text":"Python : \"I want your silly game to have a starting value of 100 for life points for all players.\" Javascript : \"I read you loud and clear. Let's take that starting life and play this silly game. Alfred and Wallace are on the same side battling orcs. Alfred decides to give life to Wallace because Wallace appears to be about to bravely charge onto the battle field and may need it. Done. That's all the game can do right now because I just started making it.\" Python : \"Let's see...oh wow. Your game code actually worked. Alfred now has 99 life points and Wallace has 101. Good job us.\" I went on a hunt to find the cleanest and most succinct way to pass a variable from Python to Javascript and then back to Python in a Jupyter notebook. I wanted a proof of principle upon which I could later base my D3 graphics using Python data as pandas dataframes. So, this silly example can be translated into code as follows. First, things are actually easy for us because we are in a Jupyter notebook living in a browser, utilizing all sorts of widgets and HTML elements already. We can tap into that (and actually the DOM) to get some of the functionality we require. So, in Python we can use the IPython.display module with the HTML function allowing us to embed an element for later use. # Python from IPython.display import HTML pystartlife = str ( 100 ) HTML ( \"<div id='textid'>\" + pystartlife + \"</div>\" ) We've created a Python variable, pystartlife , and embedded it as a div element, literally just using raw HTML. Now we use the Javascript magics ( %%javascript ) to create a Javascript coding environment for the next cell. (BTW there are magics for many more languages - very cool.) In the Javascript, now, we grab the div element with the Python variable from the document or webpage and play our game. We also write the game. (This game is based on a Node.js tutorial by thenewboston on YouTube - thank you Bucky!). %% javascript // Get the python variable from the DOM var startlife = document . getElementById ( 'textid' ). innerHTML ; // Define a User class with a method function User () { this . name = '' ; this . life = Number ( startlife ); this . giveLife = function giveLife ( targetPlayer ) { targetPlayer . life += 1 ; this . life -= 1 ; } } // Use class var Alfred = new User (); var Wallace = new User (); // Names were blank so give them name values Alfred . name = 'Alfred' ; Wallace . name = 'Wallace' ; // Let's play a game! // Let Alfred give life to Wallace Alfred . giveLife ( Wallace ); // Save these variables back to python variables to work with later IPython . notebook . kernel . execute ( 'Alfred_life=\"' + Alfred . life + '\";' ); IPython . notebook . kernel . execute ( 'Wallace_life=\"' + Wallace . life + '\";' ); We grab the Python variable now embedded with document.getElementById('idname').innerHTML , a DOM function and attribute which grabs the div by its id (not name) and takes what is in the text part of the div . Then we define a class in Javascript that contains an altruistic method for giving life. We create some users and give them names. We then \"play\" the game and allow one to give life to another with the method in the User class - how generous of Alfred! Lastly, the lovely magic part, we use this sneaky Javascript function from the IPython class that executes Python statements. We could execute any Python statement really in this way, e.g., IPython.notebook.kernel.execute('print(\"Hello World!\")'); We include our life values (Javascript variables) into this executable string and, well, execute it. And to see if the magic worked, we run a simple couple of print statements back in Python: # Python print ( Alfred_life ) print ( Wallace_life ) With low and behold and output of: 99 and 101. Way to pay it forward, Alfred! You can find this code and more in this Jupyter notebook.","title":"A little exchange"},{"location":"jupyter-and-beaker-make-a-case/","text":"tl;dr : Once you learn how to use one kind of notebook system, the knowledge will transfer easily to another. Here, we're discussing two, Jupyter and Beaker. Because Jupyter is much more mature of a project, it'd probably be the best place to start. But for those with that extreme sense of adventure and/or use Python 2 a lot with other languages, give the Beaker notebooks a chance. Loads of potential for data scientists there. Posted: 2017-02-19 What's a notebook? Have you ever taught programming and wished to have the class notes, sample code and exercises with instructions all in one place? Have you ever heavily commented your code and wished it was more readable? Have you used R Markdown and wished to run individual code chunks with only a button or keyboard shortcut? Have you ever wished to use multiple programming languages in the same place, same document? So, the story begins with my wish for a better way. When I discovered notebooks, at first, I felt strange programming in a browser until I discovered I could annotate the code with pleasant, easy-to-read text and for some reason that opened up a whole world. I began documenting my research work more, creating clear and rich teaching aids, and enhancing my work to share with others in a reproducible way or at least with clear instructions and notes in nice looking text rather than sometimes hard-to-read comments within the code (which I still do of course). It was the annotations that made it worth my time to learn. There are several notebook systems out there and they all seem to behave, at their core, the same way in that I can run interactive code cells and document my work in pleasant-to-read formats. They do vary in their use cases, such as RStudio's notebook being more geared towards the R programmer (although it has extension packages for other languages now) or the Beaker notebooks for combining multiple languages into a workflow. A sample of notebook \"providers\" is as follows. Beaker Zeppelin Spark Jupyter Jupyterlab (preview) RStudio and more Introducing our players Beaker , a polyglot notebook system, is based on IPython (amongst other things) and Jupyter , supporting over 40 programming languages, is based on IPython (amongst other things). They both allow multiple languages from within the same notebook and both run on top of Python. I found I was able to install either one without the command line so they seemed pretty easy to get going on (Jupyter did require one command in the terminal to start which was a simple task). They are both open source projects and being built on IPython have similar notebook interfaces so it'd be easy to switch over from one to another once you get the hang of notebooks. Differences and distinguishing factors discussed below. Jupyter: customizable and sometimes magic A Python 3 flavored Jupyter notebook with a \"grade3\" theme (theme from Kyle Dunovan's jupyter-themes repo) The Jupyter project is much more mature than the Beaker project and thus has a reliable and almost totally bug-free experience (nothing is completely bug-free). It's pretty much the classic notebook system, but gives the us the ability to use it for reproducible research, publish papers, do presentations with live code, create blogs (not this one, although it's in markdown at least), and the list goes on. It's a mature project with many add-ons and features available. Returning to basecamp, Jupyter notebook setups can be simple and basic, one language supported and the basic theme, or much more complex, supporting several languages chosen from a drop-down menu and having extensions to check spelling and perhaps a custom theme to pretty it up. Out of the box, they simply work with the default Python. It's a very transparent system. What you add on is done by you, but you must take care of what that add-on requires. Jupyter notebooks are meant to be simple, useful and clean (I've seen and made many of messes so I aim for this). An Anaconda install gives us Jupyter notebooks automatically. Whichever is the default Python, becomes the default Python version for the notebook (basically whatever is first in our PATH if we have 2 and 3). We could also install with pip , Python's package manager. Jupyter, when using the python kernel, can incorporate \"magics\" or other languages within the same notebook (and sometimes passing variables back and forth like with R kernel and rpy2 or javascript). Some cell magics are listed here (the \"%%\" is literally the syntax we use in a notebook cell to designate): %%fortran %%cython %%javascript %%html %%bash %%latex %%perl %%python2 %%python3 %%ruby %%R others (incl. incubator projects like sparkmagic , which created magics within it, in the context of working with spark clusters) These languages, of course, must be on the system hosting the notebook. In addition, the Jupyter project reports over 40 languages supported, but this does not mean they all have magics and can be run from an IPython notebook (IPython, here, referring to the Python kernel, but it can also refer to a previous Python notebook project). Also, custom kernels for languages not supported can be made according to the Jupyter docs. One customization I really love is nbextensions (more here ) which adds functionality to a Jupyter notebooks such as a table of contents, section numbering, highlighting, and spellcheck to name a few. Personally, I found the TOC and spellcheck very, very useful as I get lost easily and spell quite horribly. Another customization is around adding a theme, but more on that below. A really nifty meta-feature is that GitHub renders static versions of IPython/Jupyter notebooks (.ipynb files which are just JSON) which makes viewing your work and the work of others very easy from GitHub. You can find the Jupyter project on GitHub at https://github.com/jupyter/notebook . Beaker: a true polyglot Python 2 and JavaScript with D3 sharing variables (entire code sample is in the D3 Demo notebook that comes with Beaker) A Beaker notebook is different from a Jupyter notebook in that it can easily pass data from one cell to the next even if the code in each cell is in a different programming language. This is the big selling point of Beaker notebooks. Literally, we can share one variable from Python to Javascript, for example, by just prefacing it with beaker. . Woah. This opens up a realm of possibilities. Beaker notebooks give us more default functionality and ease-of-use than a Jupyter notebook at the expense of being less transparent. If all you need is Python 2, they are super easy and very user-friendly. Also, Beaker starts up with tons of sample code, called Demos, at your fingertips for most if not all of the supported languages. Beaker, so far, out-of-the-box, supports* 17 languages: Clojure C++ Python2 (called IPython by Beaker) Python3 R SQL JavaScript Scala Node Torch Julia Java Kdb Groovy HTML Ruby TeX * You still need to have the backend interpreter or compiler (just like in Jupyter) and certain plugins in most cases to connect it up to Beaker. On the origins of Beaker, in their own words... Beaker is built on many fantastic open source projects including Angular, Bootstrap, CometD, Gradle, Guice, IPython, Jackson, Jetty, Nginx, Rserve, and others for which we are very grateful. Beaker, too, is open source as a \"base\" or something to run locally, host oneself, or use with Docker. You can check it out on their GiHhub repo at: https://github.com/twosigma/beaker-notebook . Beaker has better notebooks management features (such as listing your open notebooks with time stamps). The UI looks a bit nicer as well. Aside: Those who like to see their files listed, however, should try Jupyterlab which feels more like RStudio than a notebook system. It has nice management features as well, but more around transparency into the file system and has the ability to open different file formats plus a classic interpreter. It's out of the scope of this post for sure. And some may not see this tiny little note in a screenshot of a guide for what you can put in the text parts right on their GitHub readme, but they totally mention Donald Knuth, one of my favorite people ever. +1. Installing it Neither Beaker, nor Jupyter, require the command line for installation. An install of Anaconda for Python includes Jupyter notebooks. To run it however, one will need to type jupyter notebook from the command line, but that's really it (you can also install Jupyter from the command line with pip ). The tricky part sometimes for Jupyter is getting other kernels (support for other languages) installed. But my other de facto language is R and I simply used the conda compatible R-Essentials , which gives me the R kernel option (Yay!!) (and by far the easiest way to get the R kernel working that I've found - see this blog for more on R-Essentials). I gave up getting R to work in Beaker after toying around for an hour or so (granted that wasn't a long time and smarter folks could probably get it working) running up against an undocumented R package called Rserve, a dependency for R in Beaker. It appears Beaker by default expects, as it says here , a Python 2 conda install (which is weird I thought due to Python 2 becoming a legacy language soon). So, when I tried it with my Python-3-only conda install, I had bad luck running an IPython cell, although Python 3 cells worked. I did solve the IPython cell issue according to some pretty easy-to-follow advice on their wiki about specifying the IPython path and adding a Python path pointing to a Python 2 install (in a config file called beaker.pref.json ). Beaker's wiki is, in general, very helpful I've found thus far. Themes: design can win people over A Jupyter notebook with the mm_dark_theme (theme by Milos Miljkovic and found on GitHub) Out of the box, we get a couple of themes for our notebooks with Beaker, Default and Ambiance. With Jupyter, we can add any theme we'd like or like to create in a few ways, but my favorite and simplest is just adding a custom.js and custom.css to a folder called ~/.jupyter/custom/ (create one if it's not there). The ~ refers to the user's base directory, often used in Unix systems. For the Jupyter notebook above, I used a dark theme which is found here , but you can find them all over place on GitHub. The installs will vary so I stick with the custom.js from the mm_dark_theme project/repository and just switch out the css files (probably not the best practice mind you, but I always give credit to the creators of the css, or stylesheets). Yes, the Jupyter method to include a theme is one or two steps more complicated, but it's truly custom and themes can be important for various reasons, like emphasizing the graphs and text to non-technical folk in a more pleasant background color than white and nicer fonts. I do have to admit I was pleasantly surprised by having a theme choice in Beaker and then how easy it was to switch them. Competition or no? So, is it a competition? I'd say not so much given we don't really have an apples to apples situation. While Beaker may be trying to fill the gaps or make its niche, I'd say it's in fact creating a whole new experience for data scientists which could be extraordinary if they'd only make some small adjustments (such as make the deployment Python version agnostic). I hope these two projects, and the others, continue to complement each other and grow even better. Questions I have, but didn't answer here Do I really need to pass variables from one language to another or are magics in Jupyter sufficient for incorporating the same languages into one notebook? Why would I choose Jupyterlab over Jupyter notebooks? Is it a step forward or simply a divergence? RStudio does similar things these days and with XRPython we can embed python in a notebook-eqsue environment \u2014 and might I add tidyverse. Why not use that? Which one is the best for Spark? Thanks for reading.","title":"The Notebook Superhero -- Is It Always a Contest?"},{"location":"jupyter-and-beaker-make-a-case/#whats-a-notebook","text":"Have you ever taught programming and wished to have the class notes, sample code and exercises with instructions all in one place? Have you ever heavily commented your code and wished it was more readable? Have you used R Markdown and wished to run individual code chunks with only a button or keyboard shortcut? Have you ever wished to use multiple programming languages in the same place, same document? So, the story begins with my wish for a better way. When I discovered notebooks, at first, I felt strange programming in a browser until I discovered I could annotate the code with pleasant, easy-to-read text and for some reason that opened up a whole world. I began documenting my research work more, creating clear and rich teaching aids, and enhancing my work to share with others in a reproducible way or at least with clear instructions and notes in nice looking text rather than sometimes hard-to-read comments within the code (which I still do of course). It was the annotations that made it worth my time to learn. There are several notebook systems out there and they all seem to behave, at their core, the same way in that I can run interactive code cells and document my work in pleasant-to-read formats. They do vary in their use cases, such as RStudio's notebook being more geared towards the R programmer (although it has extension packages for other languages now) or the Beaker notebooks for combining multiple languages into a workflow. A sample of notebook \"providers\" is as follows. Beaker Zeppelin Spark Jupyter Jupyterlab (preview) RStudio and more","title":"What's a notebook?"},{"location":"jupyter-and-beaker-make-a-case/#introducing-our-players","text":"Beaker , a polyglot notebook system, is based on IPython (amongst other things) and Jupyter , supporting over 40 programming languages, is based on IPython (amongst other things). They both allow multiple languages from within the same notebook and both run on top of Python. I found I was able to install either one without the command line so they seemed pretty easy to get going on (Jupyter did require one command in the terminal to start which was a simple task). They are both open source projects and being built on IPython have similar notebook interfaces so it'd be easy to switch over from one to another once you get the hang of notebooks. Differences and distinguishing factors discussed below.","title":"Introducing our players"},{"location":"jupyter-and-beaker-make-a-case/#jupyter-customizable-and-sometimes-magic","text":"A Python 3 flavored Jupyter notebook with a \"grade3\" theme (theme from Kyle Dunovan's jupyter-themes repo) The Jupyter project is much more mature than the Beaker project and thus has a reliable and almost totally bug-free experience (nothing is completely bug-free). It's pretty much the classic notebook system, but gives the us the ability to use it for reproducible research, publish papers, do presentations with live code, create blogs (not this one, although it's in markdown at least), and the list goes on. It's a mature project with many add-ons and features available. Returning to basecamp, Jupyter notebook setups can be simple and basic, one language supported and the basic theme, or much more complex, supporting several languages chosen from a drop-down menu and having extensions to check spelling and perhaps a custom theme to pretty it up. Out of the box, they simply work with the default Python. It's a very transparent system. What you add on is done by you, but you must take care of what that add-on requires. Jupyter notebooks are meant to be simple, useful and clean (I've seen and made many of messes so I aim for this). An Anaconda install gives us Jupyter notebooks automatically. Whichever is the default Python, becomes the default Python version for the notebook (basically whatever is first in our PATH if we have 2 and 3). We could also install with pip , Python's package manager. Jupyter, when using the python kernel, can incorporate \"magics\" or other languages within the same notebook (and sometimes passing variables back and forth like with R kernel and rpy2 or javascript). Some cell magics are listed here (the \"%%\" is literally the syntax we use in a notebook cell to designate): %%fortran %%cython %%javascript %%html %%bash %%latex %%perl %%python2 %%python3 %%ruby %%R others (incl. incubator projects like sparkmagic , which created magics within it, in the context of working with spark clusters) These languages, of course, must be on the system hosting the notebook. In addition, the Jupyter project reports over 40 languages supported, but this does not mean they all have magics and can be run from an IPython notebook (IPython, here, referring to the Python kernel, but it can also refer to a previous Python notebook project). Also, custom kernels for languages not supported can be made according to the Jupyter docs. One customization I really love is nbextensions (more here ) which adds functionality to a Jupyter notebooks such as a table of contents, section numbering, highlighting, and spellcheck to name a few. Personally, I found the TOC and spellcheck very, very useful as I get lost easily and spell quite horribly. Another customization is around adding a theme, but more on that below. A really nifty meta-feature is that GitHub renders static versions of IPython/Jupyter notebooks (.ipynb files which are just JSON) which makes viewing your work and the work of others very easy from GitHub. You can find the Jupyter project on GitHub at https://github.com/jupyter/notebook .","title":"Jupyter:  customizable and sometimes magic"},{"location":"jupyter-and-beaker-make-a-case/#beaker-a-true-polyglot","text":"Python 2 and JavaScript with D3 sharing variables (entire code sample is in the D3 Demo notebook that comes with Beaker) A Beaker notebook is different from a Jupyter notebook in that it can easily pass data from one cell to the next even if the code in each cell is in a different programming language. This is the big selling point of Beaker notebooks. Literally, we can share one variable from Python to Javascript, for example, by just prefacing it with beaker. . Woah. This opens up a realm of possibilities. Beaker notebooks give us more default functionality and ease-of-use than a Jupyter notebook at the expense of being less transparent. If all you need is Python 2, they are super easy and very user-friendly. Also, Beaker starts up with tons of sample code, called Demos, at your fingertips for most if not all of the supported languages. Beaker, so far, out-of-the-box, supports* 17 languages: Clojure C++ Python2 (called IPython by Beaker) Python3 R SQL JavaScript Scala Node Torch Julia Java Kdb Groovy HTML Ruby TeX * You still need to have the backend interpreter or compiler (just like in Jupyter) and certain plugins in most cases to connect it up to Beaker. On the origins of Beaker, in their own words... Beaker is built on many fantastic open source projects including Angular, Bootstrap, CometD, Gradle, Guice, IPython, Jackson, Jetty, Nginx, Rserve, and others for which we are very grateful. Beaker, too, is open source as a \"base\" or something to run locally, host oneself, or use with Docker. You can check it out on their GiHhub repo at: https://github.com/twosigma/beaker-notebook . Beaker has better notebooks management features (such as listing your open notebooks with time stamps). The UI looks a bit nicer as well. Aside: Those who like to see their files listed, however, should try Jupyterlab which feels more like RStudio than a notebook system. It has nice management features as well, but more around transparency into the file system and has the ability to open different file formats plus a classic interpreter. It's out of the scope of this post for sure. And some may not see this tiny little note in a screenshot of a guide for what you can put in the text parts right on their GitHub readme, but they totally mention Donald Knuth, one of my favorite people ever. +1.","title":"Beaker:  a true polyglot"},{"location":"jupyter-and-beaker-make-a-case/#installing-it","text":"Neither Beaker, nor Jupyter, require the command line for installation. An install of Anaconda for Python includes Jupyter notebooks. To run it however, one will need to type jupyter notebook from the command line, but that's really it (you can also install Jupyter from the command line with pip ). The tricky part sometimes for Jupyter is getting other kernels (support for other languages) installed. But my other de facto language is R and I simply used the conda compatible R-Essentials , which gives me the R kernel option (Yay!!) (and by far the easiest way to get the R kernel working that I've found - see this blog for more on R-Essentials). I gave up getting R to work in Beaker after toying around for an hour or so (granted that wasn't a long time and smarter folks could probably get it working) running up against an undocumented R package called Rserve, a dependency for R in Beaker. It appears Beaker by default expects, as it says here , a Python 2 conda install (which is weird I thought due to Python 2 becoming a legacy language soon). So, when I tried it with my Python-3-only conda install, I had bad luck running an IPython cell, although Python 3 cells worked. I did solve the IPython cell issue according to some pretty easy-to-follow advice on their wiki about specifying the IPython path and adding a Python path pointing to a Python 2 install (in a config file called beaker.pref.json ). Beaker's wiki is, in general, very helpful I've found thus far.","title":"Installing it"},{"location":"jupyter-and-beaker-make-a-case/#themes-design-can-win-people-over","text":"A Jupyter notebook with the mm_dark_theme (theme by Milos Miljkovic and found on GitHub) Out of the box, we get a couple of themes for our notebooks with Beaker, Default and Ambiance. With Jupyter, we can add any theme we'd like or like to create in a few ways, but my favorite and simplest is just adding a custom.js and custom.css to a folder called ~/.jupyter/custom/ (create one if it's not there). The ~ refers to the user's base directory, often used in Unix systems. For the Jupyter notebook above, I used a dark theme which is found here , but you can find them all over place on GitHub. The installs will vary so I stick with the custom.js from the mm_dark_theme project/repository and just switch out the css files (probably not the best practice mind you, but I always give credit to the creators of the css, or stylesheets). Yes, the Jupyter method to include a theme is one or two steps more complicated, but it's truly custom and themes can be important for various reasons, like emphasizing the graphs and text to non-technical folk in a more pleasant background color than white and nicer fonts. I do have to admit I was pleasantly surprised by having a theme choice in Beaker and then how easy it was to switch them.","title":"Themes:  design can win people over"},{"location":"jupyter-and-beaker-make-a-case/#competition-or-no","text":"So, is it a competition? I'd say not so much given we don't really have an apples to apples situation. While Beaker may be trying to fill the gaps or make its niche, I'd say it's in fact creating a whole new experience for data scientists which could be extraordinary if they'd only make some small adjustments (such as make the deployment Python version agnostic). I hope these two projects, and the others, continue to complement each other and grow even better.","title":"Competition or no?"},{"location":"jupyter-and-beaker-make-a-case/#questions-i-have-but-didnt-answer-here","text":"Do I really need to pass variables from one language to another or are magics in Jupyter sufficient for incorporating the same languages into one notebook? Why would I choose Jupyterlab over Jupyter notebooks? Is it a step forward or simply a divergence? RStudio does similar things these days and with XRPython we can embed python in a notebook-eqsue environment \u2014 and might I add tidyverse. Why not use that? Which one is the best for Spark? Thanks for reading.","title":"Questions I have, but didn't answer here"},{"location":"learning-from-learning-yolov3/","text":"Lessons from YOLO v3 Implementations in PyTorch UPDATE 2020-06-06 : YOLO v4 has been recently published and implemented in PyTorch and can be found at https://github.com/Tianxiaomo/pytorch-YOLOv4 . tl:dr : YOLO (for \"you only look once\") v3 is a relatively recent (April 2018) architecture design for object detection. PyTorch (recently merged with Caffe2 and production as of November 2018) is a very popular deep learning library with Python and C++ bindings for both training and inference that is differentiated from Tensorflow by having a dynamic graph. This post is about my lessons working on PyTorch YOLO v3 and a little insight into creating a good YOLO v3 custom model on custom data ( We love you COCO, but we have our own interets, now. ). Posted: 2019-11-23 Quick Links Original YOLO v3 paper Original PyTorch codebase Ayoosh Kathuria's original blog post on implementing YOLO v3 in PyTorch Lessons Anchor boxes (and briefly how YOLO works) In order to understand the anchors or anchor boxes, a little background is needed on the YOLO v3 algorithm (sources are the original YOLO and YOLO v3 papers). In full-sized YOLO v3 there are 9 anchor boxes specified in total as can be seen in the cfg files on the PyTorch repo. [[94, 89], [188, 190], [322, 308], [401, 401], [483, 475], [555, 539], [634, 646], [771, 765], [960, 866]] There are 3 scales at which YOLO \"sees\" an image when passes through the network (these correspond to the three yolo layers). Note, this allows YOLO to see big, medium and small sized objects all at once. At each of the three scales, the image is broken in to a grid of 13x13 squares or cells (remember, our input image is converted to a 416x416 square in this implementation before running through the network). For each cell in a 13x13 grid, three anchor boxes are used (this corresponds to the three anchor boxes from above). In other words, each cell has three anchor boxes overlayed on it and this happens at three different scales (all within the same pass through the network, even! Hence, \"you only look once\" :-) ). So, when we list the array of 9 anchor boxes from above, the first three width/heights ( [94, 89], [188, 190], [322, 308] ), belong to the first scaling process, the second three ( [401, 401], [483, 475], [555, 539] ) to the second scaling process and, as follows, the final three to the third scaling process ( [634, 646], [771, 765], [960, 866] ). Each set of three width/heights correspond to the width/heights of the three bounding boxes used for each grid cell at each of the three scales. To round out this story, the three anchor boxes are used to predict whether there is an object there (object/no object). The grid cell is used to predict classes. These are combined at the end of the network to figure out the shape of objects (bounding boxes) from anchor boxes and their classes from grid cells. This diagram shows this very well (anchor boxes on top path and grid cell predictions on bottom path): Image source With this all being said, the lesson is to always calculate the anchor boxes on each new dataset before training. The sizes of labeled objects (which determines sizes of anchor boxes) will be crucial to a good training experiment and well as inference which uses the same anchor box sizes. Anchor boxes are calculated using Kmeans clustering for every new dataset as is shown in code here (adapted from a Keras implementation of YOLO v3). Transfer learning In transfer learning we begin with a base model which gives us the weight values to start our training. Objects from the training set of the base model, upon which the base model was trained, gets us closer to a new learned network for objects in the real world. So, instead of starting with random weights to begin our training we begin from a \"smarter\" set of values. One tidbit I learned was to skip making batch normalization (BN) layers trainable. I recently learned from A refresher on batch (re-)normalization that: \"When the mini-batch mean (\u00b5B) and mini-batch standard deviation (\u03c3B) diverge from the mean and standard deviation over the entire training set too often, BatchNorm breaks.\" And that there are perils in hyperparameter tuning in conjunction with retraining BN layers and a few extra steps required to fix this (with a technique call batch renormalization) - so for simplicity sake, I left out retraining on BN layers, but look at batch renormalization techniques in the post above for addressing the complex issue if you wish. How to allow layers in a PyTorch model to be trainable (minus BNs). # Freeze layers according to user specification stop_layer = layers_length - args.unfreeze # Freeze up until this layer cntr = 0 for name, param in model.named_parameters(): if cntr < stop_layer: param.requires_grad = False else: if 'batch_norm' not in name: print(\"Parameter has gradients tracked.\") param.requires_grad = True else: param.requires_grad = False cntr+=1 Finetuning How much of network to \"open up\" or set as trainable (the parameters that is)? - it's recommended at times to open it more (likely all of the parameters in fine-tuning phase) if the object or objects are very different from any COCO classes, which is called domain adaptation (NB: the yolov3.weights base model from darknet is trained on COCO dataset). So, for instance, if the base model has never seen a caterpillar before (not in COCO), you may want to let more layers be trainable. How to allow even more layers in the PyTorch model to be trainable (could set stop_layer to 0 to train whole network): # \"unfreeze\" refers to the last number of layers to tune (allow gradients to be tracked - backprop) stop_layer = layers_length - (args.unfreeze * 2) # Freeze up to this layer (open up more than first phase) \"\"\"...[same as above section]\"\"\" Another learning is that if the network is not converging, try opening up all of the layers during fine-tuning. Data augmentation Some of these I learned the hard way, others from the wonderful PyTorch forums and StackOverflow. Be careful of conversions from a 0-255 to a 0-1 range as you don't want to do that more than once in code. Keep this simple at first with only the resize and normalization. Try with several types of augmentation next, increasing in complexity with each experiment. Start with just resize and standard pixel intensity normalize. (NB: the transforms operate on PIL images, then convert to numpy 3D array and finally to torch.tensor() ) custom_transforms = Sequence([YoloResizeTransform(inp_dim), Normalize()]) Then get fancier with hue, saturation and brightness shifts, for example (look in cfg for the amounts if following along in code ). custom_transforms = Sequence([RandomHSV(hue=hue, saturation=saturation, brightness=exposure), YoloResizeTransform(inp_dim), Normalize()]) Where Normalize is a pixel intensity normalization (here, not to unit norm because we do that elsewhere) (based on accepted answer on StackOverflow ): class Normalize(object): \"\"\"Pixel-intensity normalize the input numpy image\"\"\" def __init__(self): self.channels = 3 def __call__(self, img, bboxes): \"\"\" Args: img : numpy array Image to be scaled. Returns: img : numpy array normalize image. \"\"\" arr = img.astype('float') # Do not touch the alpha channel for i in range(self.channels): minval = arr[...,i].min() maxval = arr[...,i].max() if minval != maxval: arr[...,i] -= minval # Don't divide by 255 because already doing elsewhere arr[...,i] *= ((maxval-minval)) return arr, bboxes A great option for augmentation is to double or triple the size of a dataset with a library like imgaug which can handle bounding boxes and polygons now. Learning rate schedulers There are some great learning rate schedulers to decrease learning rate with training on a schedule or automatically in the torch.optim.lr_scheduler and set of methods therein. The following is more of an implementation detail, but nonetheless, found it helpful to not make the mistake. Place the learning rate scheduler at the level of the epoch update, not the inner loop over batches of data (where the optimizer is). YOLO Glossary YOLOv3: You Only Look Once v3. Improvments over v1, v2 and YOLO9000 which include Ref : Predicts more bounding boxes per image (hence a bit slower than previous YOLO architectures) Detections at 3 scales Addressed issue of detecting small objects New loss function (cross-entropy replaces squared error terms) Can perform multi-label classification (no more mutually exclusive labels) Performance on par with other architectures (a bit faster than SSD, even, in many cases) Tiny-YOLOv3: A reduced network architecture for smaller models designed for mobile, IoT and edge device scenarios Anchors: There are 5 anchors per box. The anchor boxes are designed for a specific dataset using K-means clustering, i.e., a custom dataset must use K-means clustering to generate anchor boxes. It does not assume the aspect ratios or shapes of the boxes. Ref Loss: using nn.MSELoss (for loss confidence) or mean squared error IOU: intersection over union between predicted bounding boxes and ground truth boxes References 37 Reasons why your Neural Network is not working imgaug augmentation Python library Real-time object detection with YOLO A refresher on batch (re-)normalization /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */ var disqus_config = function () { this.page.url = 'https://michhar.github.io/learning-from-learning-yolov3/'; // Replace PAGE_URL with your page's canonical URL variable this.page.identifier = 'happycat3'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable }; (function() { // DON'T EDIT BELOW THIS LINE var d = document, s = d.createElement('script'); s.src = 'https://michhar.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); Please enable JavaScript to view the comments powered by Disqus.","title":"Lessons from YOLO v3 Implementations in PyTorch"},{"location":"learning-from-learning-yolov3/#lessons-from-yolo-v3-implementations-in-pytorch","text":"UPDATE 2020-06-06 : YOLO v4 has been recently published and implemented in PyTorch and can be found at https://github.com/Tianxiaomo/pytorch-YOLOv4 . tl:dr : YOLO (for \"you only look once\") v3 is a relatively recent (April 2018) architecture design for object detection. PyTorch (recently merged with Caffe2 and production as of November 2018) is a very popular deep learning library with Python and C++ bindings for both training and inference that is differentiated from Tensorflow by having a dynamic graph. This post is about my lessons working on PyTorch YOLO v3 and a little insight into creating a good YOLO v3 custom model on custom data ( We love you COCO, but we have our own interets, now. ). Posted: 2019-11-23","title":"Lessons from YOLO v3 Implementations in PyTorch"},{"location":"learning-from-learning-yolov3/#quick-links","text":"Original YOLO v3 paper Original PyTorch codebase Ayoosh Kathuria's original blog post on implementing YOLO v3 in PyTorch","title":"Quick Links"},{"location":"learning-from-learning-yolov3/#lessons","text":"","title":"Lessons"},{"location":"learning-from-learning-yolov3/#anchor-boxes-and-briefly-how-yolo-works","text":"In order to understand the anchors or anchor boxes, a little background is needed on the YOLO v3 algorithm (sources are the original YOLO and YOLO v3 papers). In full-sized YOLO v3 there are 9 anchor boxes specified in total as can be seen in the cfg files on the PyTorch repo. [[94, 89], [188, 190], [322, 308], [401, 401], [483, 475], [555, 539], [634, 646], [771, 765], [960, 866]] There are 3 scales at which YOLO \"sees\" an image when passes through the network (these correspond to the three yolo layers). Note, this allows YOLO to see big, medium and small sized objects all at once. At each of the three scales, the image is broken in to a grid of 13x13 squares or cells (remember, our input image is converted to a 416x416 square in this implementation before running through the network). For each cell in a 13x13 grid, three anchor boxes are used (this corresponds to the three anchor boxes from above). In other words, each cell has three anchor boxes overlayed on it and this happens at three different scales (all within the same pass through the network, even! Hence, \"you only look once\" :-) ). So, when we list the array of 9 anchor boxes from above, the first three width/heights ( [94, 89], [188, 190], [322, 308] ), belong to the first scaling process, the second three ( [401, 401], [483, 475], [555, 539] ) to the second scaling process and, as follows, the final three to the third scaling process ( [634, 646], [771, 765], [960, 866] ). Each set of three width/heights correspond to the width/heights of the three bounding boxes used for each grid cell at each of the three scales. To round out this story, the three anchor boxes are used to predict whether there is an object there (object/no object). The grid cell is used to predict classes. These are combined at the end of the network to figure out the shape of objects (bounding boxes) from anchor boxes and their classes from grid cells. This diagram shows this very well (anchor boxes on top path and grid cell predictions on bottom path): Image source With this all being said, the lesson is to always calculate the anchor boxes on each new dataset before training. The sizes of labeled objects (which determines sizes of anchor boxes) will be crucial to a good training experiment and well as inference which uses the same anchor box sizes. Anchor boxes are calculated using Kmeans clustering for every new dataset as is shown in code here (adapted from a Keras implementation of YOLO v3).","title":"Anchor boxes (and briefly how YOLO works)"},{"location":"learning-from-learning-yolov3/#transfer-learning","text":"In transfer learning we begin with a base model which gives us the weight values to start our training. Objects from the training set of the base model, upon which the base model was trained, gets us closer to a new learned network for objects in the real world. So, instead of starting with random weights to begin our training we begin from a \"smarter\" set of values. One tidbit I learned was to skip making batch normalization (BN) layers trainable. I recently learned from A refresher on batch (re-)normalization that: \"When the mini-batch mean (\u00b5B) and mini-batch standard deviation (\u03c3B) diverge from the mean and standard deviation over the entire training set too often, BatchNorm breaks.\" And that there are perils in hyperparameter tuning in conjunction with retraining BN layers and a few extra steps required to fix this (with a technique call batch renormalization) - so for simplicity sake, I left out retraining on BN layers, but look at batch renormalization techniques in the post above for addressing the complex issue if you wish. How to allow layers in a PyTorch model to be trainable (minus BNs). # Freeze layers according to user specification stop_layer = layers_length - args.unfreeze # Freeze up until this layer cntr = 0 for name, param in model.named_parameters(): if cntr < stop_layer: param.requires_grad = False else: if 'batch_norm' not in name: print(\"Parameter has gradients tracked.\") param.requires_grad = True else: param.requires_grad = False cntr+=1","title":"Transfer learning"},{"location":"learning-from-learning-yolov3/#finetuning","text":"How much of network to \"open up\" or set as trainable (the parameters that is)? - it's recommended at times to open it more (likely all of the parameters in fine-tuning phase) if the object or objects are very different from any COCO classes, which is called domain adaptation (NB: the yolov3.weights base model from darknet is trained on COCO dataset). So, for instance, if the base model has never seen a caterpillar before (not in COCO), you may want to let more layers be trainable. How to allow even more layers in the PyTorch model to be trainable (could set stop_layer to 0 to train whole network): # \"unfreeze\" refers to the last number of layers to tune (allow gradients to be tracked - backprop) stop_layer = layers_length - (args.unfreeze * 2) # Freeze up to this layer (open up more than first phase) \"\"\"...[same as above section]\"\"\" Another learning is that if the network is not converging, try opening up all of the layers during fine-tuning.","title":"Finetuning"},{"location":"learning-from-learning-yolov3/#data-augmentation","text":"Some of these I learned the hard way, others from the wonderful PyTorch forums and StackOverflow. Be careful of conversions from a 0-255 to a 0-1 range as you don't want to do that more than once in code. Keep this simple at first with only the resize and normalization. Try with several types of augmentation next, increasing in complexity with each experiment. Start with just resize and standard pixel intensity normalize. (NB: the transforms operate on PIL images, then convert to numpy 3D array and finally to torch.tensor() ) custom_transforms = Sequence([YoloResizeTransform(inp_dim), Normalize()]) Then get fancier with hue, saturation and brightness shifts, for example (look in cfg for the amounts if following along in code ). custom_transforms = Sequence([RandomHSV(hue=hue, saturation=saturation, brightness=exposure), YoloResizeTransform(inp_dim), Normalize()]) Where Normalize is a pixel intensity normalization (here, not to unit norm because we do that elsewhere) (based on accepted answer on StackOverflow ): class Normalize(object): \"\"\"Pixel-intensity normalize the input numpy image\"\"\" def __init__(self): self.channels = 3 def __call__(self, img, bboxes): \"\"\" Args: img : numpy array Image to be scaled. Returns: img : numpy array normalize image. \"\"\" arr = img.astype('float') # Do not touch the alpha channel for i in range(self.channels): minval = arr[...,i].min() maxval = arr[...,i].max() if minval != maxval: arr[...,i] -= minval # Don't divide by 255 because already doing elsewhere arr[...,i] *= ((maxval-minval)) return arr, bboxes A great option for augmentation is to double or triple the size of a dataset with a library like imgaug which can handle bounding boxes and polygons now.","title":"Data augmentation"},{"location":"learning-from-learning-yolov3/#learning-rate-schedulers","text":"There are some great learning rate schedulers to decrease learning rate with training on a schedule or automatically in the torch.optim.lr_scheduler and set of methods therein. The following is more of an implementation detail, but nonetheless, found it helpful to not make the mistake. Place the learning rate scheduler at the level of the epoch update, not the inner loop over batches of data (where the optimizer is).","title":"Learning rate schedulers"},{"location":"learning-from-learning-yolov3/#yolo-glossary","text":"YOLOv3: You Only Look Once v3. Improvments over v1, v2 and YOLO9000 which include Ref : Predicts more bounding boxes per image (hence a bit slower than previous YOLO architectures) Detections at 3 scales Addressed issue of detecting small objects New loss function (cross-entropy replaces squared error terms) Can perform multi-label classification (no more mutually exclusive labels) Performance on par with other architectures (a bit faster than SSD, even, in many cases) Tiny-YOLOv3: A reduced network architecture for smaller models designed for mobile, IoT and edge device scenarios Anchors: There are 5 anchors per box. The anchor boxes are designed for a specific dataset using K-means clustering, i.e., a custom dataset must use K-means clustering to generate anchor boxes. It does not assume the aspect ratios or shapes of the boxes. Ref Loss: using nn.MSELoss (for loss confidence) or mean squared error IOU: intersection over union between predicted bounding boxes and ground truth boxes","title":"YOLO Glossary"},{"location":"learning-from-learning-yolov3/#references","text":"37 Reasons why your Neural Network is not working imgaug augmentation Python library Real-time object detection with YOLO A refresher on batch (re-)normalization /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */ var disqus_config = function () { this.page.url = 'https://michhar.github.io/learning-from-learning-yolov3/'; // Replace PAGE_URL with your page's canonical URL variable this.page.identifier = 'happycat3'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable }; (function() { // DON'T EDIT BELOW THIS LINE var d = document, s = d.createElement('script'); s.src = 'https://michhar.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); Please enable JavaScript to view the comments powered by Disqus.","title":"References"},{"location":"masks_to_polygons_and_back/","text":"A Monarch butterfly courtesy of National Geographic Kids tl:dr : Masks are areas of interest in an image set to one color, or pixel value, surrounded by a contrast color or colors. In this technical how-to, I use the OpenCV Python binding and Shapely library to create a mask, convert it to shapes as polygons, and then back to a masked image - noting some interesting properties of OpenCV and useful tricks with these libraries. Posted: 2017-09-28 All of the code below can be found in this Python jupyter notebook. Why are masks and polygons important? Imagine you'd like to identify all of the pixels in a brain scan that correspond to a certain feature of the brain - maybe identify the location and contours of a mass. Creating a mask, or highlighting just the feature's pixels on a backdrop of one contrast color, would be a good start, then understanding the shape of that masked feature as identified as polygons would give more information and perhaps help a doctor better understand any abnormalities. But say we had a machine that only identified shapes and we wanted the masked image. We could do so with the following process and code. Finally, and this is how I began exploring this topic, if one wanted to create a trained machine learning model for semantic image segmentation, or essentially classifying groups of pixels, a masked image and its class label (is this greenery or human-made structure?) plus shapes would be very nice to have for training. My end goal was to turn a masked image (image with pixels of interest set to zero) into some polygon shapes and then back again using a couple of popular tools in Python. This was motivated by a real customer engagement around semantic image segmentation and I thought it might be useful to someone in the future. Lesson 1: opencv reads in as BGR and matplotlib reads in a RGB , just in case that is ever an issue. I tested this as follows: img_file = 'monarch.jpg' # Matplotlib reads as RGB img_plt = plt . imread ( img_file ) plt . imshow ( img_plt ) # Read as unchanged so that the transparency is not ignored as it would normally be by default # Reads as BGR img_cv2 = cv2 . imread ( img_file , cv2 . IMREAD_UNCHANGED ) plt . imshow ( img_cv2 ) # Convert opencv BGR back to RGB # See https://www.scivision.co/numpy-image-bgr-to-rgb/ for more conversions rgb = img_cv2 [ ... ,:: - 1 ] plt . imshow ( rgb ) With the following results: Matplotlib RGB: OpenCV BGR: OpenCV BGR converted back: Let's define our helper functions and not worry too much about the details right now (the original source of these helpers was a Kaggle post by Konstantin Lopuhin here - you'll need to be logged into Kaggle to see it). Helper to create MultiPolygon s from a masked image as numpy array: def mask_to_polygons ( mask , epsilon = 10. , min_area = 10. ): \"\"\"Convert a mask ndarray (binarized image) to Multipolygons\"\"\" # first, find contours with cv2: it's much faster than shapely image , contours , hierarchy = cv2 . findContours ( mask , cv2 . RETR_CCOMP , cv2 . CHAIN_APPROX_NONE ) if not contours : return MultiPolygon () # now messy stuff to associate parent and child contours cnt_children = defaultdict ( list ) child_contours = set () assert hierarchy . shape [ 0 ] == 1 # http://docs.opencv.org/3.1.0/d9/d8b/tutorial_py_contours_hierarchy.html for idx , ( _ , _ , _ , parent_idx ) in enumerate ( hierarchy [ 0 ]): if parent_idx != - 1 : child_contours . add ( idx ) cnt_children [ parent_idx ] . append ( contours [ idx ]) # create actual polygons filtering by area (removes artifacts) all_polygons = [] for idx , cnt in enumerate ( contours ): if idx not in child_contours and cv2 . contourArea ( cnt ) >= min_area : assert cnt . shape [ 1 ] == 1 poly = Polygon ( shell = cnt [:, 0 , :], holes = [ c [:, 0 , :] for c in cnt_children . get ( idx , []) if cv2 . contourArea ( c ) >= min_area ]) all_polygons . append ( poly ) all_polygons = MultiPolygon ( all_polygons ) return all_polygons Helper to create masked image as numpy array from MultiPolygon s: def mask_for_polygons ( polygons , im_size ): \"\"\"Convert a polygon or multipolygon list back to an image mask ndarray\"\"\" img_mask = np . zeros ( im_size , np . uint8 ) if not polygons : return img_mask # function to round and convert to int int_coords = lambda x : np . array ( x ) . round () . astype ( np . int32 ) exteriors = [ int_coords ( poly . exterior . coords ) for poly in polygons ] interiors = [ int_coords ( pi . coords ) for poly in polygons for pi in poly . interiors ] cv2 . fillPoly ( img_mask , exteriors , 1 ) cv2 . fillPoly ( img_mask , interiors , 0 ) return img_mask Lesson 2: If you want a more \"blocky\" polygon representation to save space or memory use the approximation method (add to the mask_to_polygons method). # Approximate contours for a smaller polygon array to save on memory contours = [ cv2 . approxPolyDP ( cnt , epsilon , True ) for cnt in contours ] We read the image in again: # Read in image unchanged img = cv2 . imread ( img_file , cv2 . IMREAD_UNCHANGED ) # View plt . imshow ( img , cmap = 'gray' , interpolation = 'bicubic' ) We convert to a luminance image with Scikit-Image's rgb2gray flattening it in the channel dimension: # Convert to a luminance image or an array which is the same size as # the input array, but with the channel dimension removed - flattened BW = rgb2gray ( img ) # View plt . imshow ( BW , cmap = 'gray' , interpolation = 'bicubic' ) With this result: Lesson 3: For a quick mask we can use OpenCV's convertScaleAbs function and it also is needed for the helper As far as this pre-processing step goes, cv2.convertScaleAbs converts the image to an 8-bit unsigned integer with 1 channel, essentially flattening it and getting it ready to create some polygons (actually MultiPolygon s in this case). # Convert to CV_8UC1 for creating polygons with shapely # CV_8UC1 is an 8-bit unsigned integer with 1 channel BW = cv2 . convertScaleAbs ( BW ) # View plt . imshow ( BW , cmap = 'gray' , interpolation = 'bicubic' ) With this result: Now let's let our functions do the real work! Convert to polygons and then back to a masked image: # Get the polygons using shapely polys = mask_to_polygons ( BW , min_area = 50 ) # Convert the polygons back to a mask image to validate that all went well mask = mask_for_polygons ( polys , BW . shape [: 2 ]) # View - you'll see some loss in detail compared to the before-polygon # image if min_area is high - go ahead and try different numbers! plt . imshow ( mask , cmap = 'gray' , interpolation = 'nearest' ) Final result: Notice the slight loss of detail - this is because we are removing really tiny polygons (see min_area parameter). I leave it up to you to download this Python jupyter notebook and try using the RGB image for masking and creating Polygons and back. Do the results change? Try this on your own images and have fun!","title":"Shapely Shapes and OpenCV Visions"},{"location":"my-new-static-site-generator-hobby/","text":"tl:dr : A little post on using static site generators to overlay a website on top of a GitHub repo for displaying docs, portfolios/products, and blogs. Posted : 2017-06-18 I recently discovered I can have a static website for each of my GitHub repositories, which seems like overkill for 35+ repos. That being said, it certainly would be nice to attractively present some of my content I've painstakingly written in Markdown files to showcase my work. Using static site generators, like Jekyll or MkDocs (and others \u2014 good article here ) combined with a templating engine like Jinga, opens up a lovely world of having human-friendly interfaces on top of my repos that I want to share. The complexity of the sites can get quite intricate \u2014 to my hearts content. Usage of the Jekyll Webjeda theme for a course listing site Markup languages like Markdown are pretty easy to write in and create text files with headings, lists, tables and such (GitHub made a nice guide here ). However, if you like reStructuredText or HTML better there are generators out there for you. There's a great site to shop for a Jekyll theme (where I began this journey), here . They are mostly slanted towards blog writers as that was the reason for the genesis of Jekyll by the founder of GitHub (thanks Tom Preston-Werner! Find out more about Jekyll on this blog). There is of course the use of GitHub Pages to simply render the repo's README markdown file by clicking on Settings and scrolling down to GitHub Pages , then selecting a theme directly at that point. Basically, I'm just scratching the surface here on another way. Also, the generators I chose all look good on hand-held devices - an important aspect to consider. These are some scenarios for the site generators I'm introducing based on research and actual work I've done: Chalk Blogs Things listed by dates Webjeda Cards Blogs Portfolios Modular presentations e.g. product cards MkDocs Documentation Guides or tutorials With the following information, hopefully you can begin to successfully build sites based on these generators. I hope to fork them at some point for contributing back and encourage you to do so if you figure out something useful. Clarification on project setups: some themes or static site generators that use repositories for content, have a branch for building the site (source) and a branch (usually called gh-pages ) for the sites deployment files (MkDocs sites do this). Some themes just work under master or have a mirrored dev branch. Chalk Chalk demo site Chalk is a high quality, completely customizable, performant and 100% free blog template for Jekyll. Creator's Profile: https://github.com/nielsenramon Chalk is my favorite Jekyll theme for blogging. It's simple and clean in its look. However, Chalk doesn't support the standard way of working with Jekyll on GitHub pages due to custom plugins. There's a little bit more complexity around building the site with these plugins, but all of the scripts are provided so it's actually quite easy in the long run to build and deploy to GitHub Pages or another hosting service. Check out these sites: Demo site Chalk GitHub repo Recommended tweaks: Use the dark code highlighting theme inside of the light Chalk theme for a Sublime Editor-like effect. Modify an import at the bottom of the /_assets/stylesheets/light.scss : @import \"modules/highlights-dark\"; Add an icon to the post listing page (main page) by editing the index.html with a div tag: < div class = \"article-list-tags\" > < img src = \"{{ post.logo }}\" alt = \"post logo\" href = \"{{ post.url | relative_url }}\" width = \"30%\" align = \"center\" ></ img > </ div > And adding the post.logo to the YAML heading on the post as in: --- ... comments: true description: A short chatterbot dev story logo: \"../resources/images/ocrbot_local.png\" tags: ... --- A tweaked Chalk-based site with icons on main page Webjeda Cards Webjeda demo site Webjeda Cards is a Bootstrap based jekyll theme for portfolio, photography or any kind of blog. Creator's Profile: https://github.com/sharu725 This Jekyll theme is fantastic for more modular postings (like course or product listings). It could also be a great place to show off your work like photos you've taken with a nice write-up. Perhaps, this could be the main page of your site and you could link to all of the repos you'd like to share from this card layout design. Setup note: you can of course simply use the master branch here for building and deploying, but it's always nice to have a separate branch we often see called gh-pages for the sites actual deployed content. Check out these sites: Demo site Webjeda Cards GitHub repo Recommended tweaks: In index.html added a variable to automatically pull in the \"Read\" button name (so, instead of just \"Read\" it's a custom label on the button). This is done with the post.label : < div class = \"panel-body\" > < h3 class = \"panel-title pull-left\" > {{ post.title }} </ h3 >< span class = \"post-meta pull-right\" >< small ></ small ></ span > < a href = \"{{ post.url | prepend: site.baseurl }}\" class = \"btn btn-primary btn-sm pull-right mt10\" > Go to {{post.label}} </ a > </ div > Then, in the post you'll have a YAML header with a label variable like: --- layout: post title: Microsoft R Server and SQL Server R Services Labs categories: mrs img: hikeclouds.jpg label: Labs resource: --- MkDocs Docs for one of my projects using the readthedocs theme MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Creators' profile: https://github.com/mkdocs This is a great alternative to a Sphinx build for a readthedocs style documentation page. It's very easy to setup and use. There are other builtin themes as well. It's fairly pre-baked, but very good for what it's good for. Documentation for MkDocs","title":"Overlaying a Website ontop of a GitHub Repository"},{"location":"my-new-static-site-generator-hobby/#chalk","text":"Chalk demo site Chalk is a high quality, completely customizable, performant and 100% free blog template for Jekyll. Creator's Profile: https://github.com/nielsenramon Chalk is my favorite Jekyll theme for blogging. It's simple and clean in its look. However, Chalk doesn't support the standard way of working with Jekyll on GitHub pages due to custom plugins. There's a little bit more complexity around building the site with these plugins, but all of the scripts are provided so it's actually quite easy in the long run to build and deploy to GitHub Pages or another hosting service. Check out these sites: Demo site Chalk GitHub repo Recommended tweaks: Use the dark code highlighting theme inside of the light Chalk theme for a Sublime Editor-like effect. Modify an import at the bottom of the /_assets/stylesheets/light.scss : @import \"modules/highlights-dark\"; Add an icon to the post listing page (main page) by editing the index.html with a div tag: < div class = \"article-list-tags\" > < img src = \"{{ post.logo }}\" alt = \"post logo\" href = \"{{ post.url | relative_url }}\" width = \"30%\" align = \"center\" ></ img > </ div > And adding the post.logo to the YAML heading on the post as in: --- ... comments: true description: A short chatterbot dev story logo: \"../resources/images/ocrbot_local.png\" tags: ... --- A tweaked Chalk-based site with icons on main page","title":"Chalk"},{"location":"my-new-static-site-generator-hobby/#webjeda-cards","text":"Webjeda demo site Webjeda Cards is a Bootstrap based jekyll theme for portfolio, photography or any kind of blog. Creator's Profile: https://github.com/sharu725 This Jekyll theme is fantastic for more modular postings (like course or product listings). It could also be a great place to show off your work like photos you've taken with a nice write-up. Perhaps, this could be the main page of your site and you could link to all of the repos you'd like to share from this card layout design. Setup note: you can of course simply use the master branch here for building and deploying, but it's always nice to have a separate branch we often see called gh-pages for the sites actual deployed content. Check out these sites: Demo site Webjeda Cards GitHub repo Recommended tweaks: In index.html added a variable to automatically pull in the \"Read\" button name (so, instead of just \"Read\" it's a custom label on the button). This is done with the post.label : < div class = \"panel-body\" > < h3 class = \"panel-title pull-left\" > {{ post.title }} </ h3 >< span class = \"post-meta pull-right\" >< small ></ small ></ span > < a href = \"{{ post.url | prepend: site.baseurl }}\" class = \"btn btn-primary btn-sm pull-right mt10\" > Go to {{post.label}} </ a > </ div > Then, in the post you'll have a YAML header with a label variable like: --- layout: post title: Microsoft R Server and SQL Server R Services Labs categories: mrs img: hikeclouds.jpg label: Labs resource: ---","title":"Webjeda Cards"},{"location":"my-new-static-site-generator-hobby/#mkdocs","text":"Docs for one of my projects using the readthedocs theme MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Creators' profile: https://github.com/mkdocs This is a great alternative to a Sphinx build for a readthedocs style documentation page. It's very easy to setup and use. There are other builtin themes as well. It's fairly pre-baked, but very good for what it's good for. Documentation for MkDocs","title":"MkDocs"},{"location":"notebooks1-post/","text":"Posted: 2016-10-07 I had a great time interacting with some of the most motivated people I've ever met at the Data Science Summit in Atlanta, GA, on September 26 and 27 th . The attendees showed up to learn new skills in the analytics domain. A colleague and I delivered two courses for the price of one (well, at the very least they were in the same room - choose your own adventure) and we saw people so motivated, they worked for a full 3 hour session, heads down, and with an intense drive. The courses were self-paced and self-guided with us there as proctors. The other course in the room was on Microsoft R Server and SQL Server R Services, however my expertise lay in Python for Data Science , a course I created and that was recently released on the landing page of https://notebooks.azure.com . This course's audience includes those: New to Python Experienced with Python, but not data science In need of a refresher Hoping to learn a little about the Python 2 to 3 change In need of a handy modular reference ... Azure Notebooks was released the first day of the courses and used for the 13-module Python for Data Science course covering, for those new to Python, Basics Data Structures Functional Programming Sorting and Pattern Matching Object Oriented Programming Basic Difference from 2 to 3 and for the DS, Numerical Computing Data Analysis with pandas I Data Analysis with pandas II Machine Learning I - ML Basics and Data Exploration Machine Learning II - Supervised and Unsupervised Learning Machine Learning III - Parameter Tuning and Model Evaluation Visualization This course includes the gambit. From list comprehension: # Solution to list comprehension exercise # Solution to list comprehension exercise letters = list ( 'thepurposeoflife' ) # Place your list comprehension below output = [ x . upper () if x is 'e' else x for x in letters ] print ( '' . join ( output )) Which gets you: thEpurposEoflifE To anomaly detection with a one-class SVM in scikit-learn : import numpy as np import matplotlib.pyplot as plt import matplotlib.font_manager from sklearn import svm from sklearn.datasets import load_iris from sklearn.cross_validation import train_test_split # Iris data iris = load_iris () X , y = iris . data , iris . target # Taking only two columns... labels = iris . feature_names [ 1 : 3 ] X = X [:, 1 : 3 ] # split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 0 ) # make some outliers X_weird = np . random . uniform ( low =- 2 , high = 9 , size = ( 20 , 2 )) # fit the model clf = svm . OneClassSVM ( nu = 0.1 , kernel = \"rbf\" , gamma = 1 , random_state = 0 ) clf . fit ( X_train ) # predict labels y_pred_train = clf . predict ( X_train ) y_pred_test = clf . predict ( X_test ) y_pred_outliers = clf . predict ( X_weird ) Which when plotted in matplotlib results in revealing some decision boundaries around if a value is weird or not from the iris dataset (and some made-up values for testing): You can find the Notebooks offering at https://notebooks.azure.com and the Python course on the main page with the tutorials near the bottom and titled Fundamentals of Data Science with Python . It's free, just like the product, Azure Notebooks. The course is actually a Library . It was a really fun and challenging to create and I'm so very glad to have it released into the wild. It contains exercises as well, some simple and some for those who enjoy a challenge themselves. For example, in this exercise a student is asked to create a Wallet for your friend Amy, let Amy earn some money and then let Amy spend some of that money. class Wallet : '''The Wallet class holds cash for a owner.''' def __init__ ( self , owner , cash ): self . owner = owner self . cash = cash # Spend method def spend ( self , amount ): '''The spend method removes a given amount of money from the wallet.''' self . cash -= amount # Earn method def earn ( self , amount ): '''The earn method adds a given amount of money to the wallet.''' self . cash += amount I can't wait to add more modules. Python will attack big data in the next one with dask . Stay tuned. My colleague and I also made use of the \"stickies\" teacher's aid. We handed out blue and red sticky notes to all of the attendees and so when they had a question or got stuck, they could simply place a red sticky on the front of their laptop and we could see the \"flag\" and come to help. This way the attendees didn't need to raise a hand at all, creating a situation where everyone had a chance to get questions answered or help when they needed without a tired arm or feeling as if they were disrupting the self-paced classroom. The Jupyter notebooks were new to about 90% of the attendees so an in promptu tutorial was given at the beginning to familiarize the attendees with the learning framework. An introduction to Python-flavored Jupyter notebooks is provided on my GitHub on this notebook . If you are looking for an R-flavored one, I have one here . Please enjoy and know there are tons of resources on the net regarding Jupyter notebooks and learning Python or R (Jupyter actually is a portmanteau of Julia, Python and R). I plan to write a short blog on using them and multi-tenant Jupyter notebook frameworks for teaching in another blog post. Many posts have been promised so I better end this one.","title":"Python for Data Science Goes Into the Wild"},{"location":"ocrbot-gets-attached/","text":"tl;dr : Chatterbots are trending bigtime! Here, we continue the story of OCRBot, a word recognizing chatbot. OCRBot's new ability to get image text from attachments is revealed (adding to it's existing ability to take image web links). So, we can snap a picture of some text and OCRbot finds the words. This could lead us into even more exciting realms like text to speech or translation. Posted: 2017-03-01 For OCR, commonly a k-nearest neighbors classifier is used for character recognition For the first two stories, see Part 1 and Part 2 . What's the wow factor? Have you ever just wanted the text extracted from, perhaps a page in a book or a funny comic that's sitting around in an image? Or maybe it'd be helpful to take a snapshot of a menu item wherein the font is a bit too small or you forgot your reading glasses, however, it's easy to read on your phone as plain text. Now, if you send the OCRBot an image (jpeg, png, gif, bmp are its known formats), on one of its supported conversational platforms like Skype or Slack, you'll get that text you crave. OCRBot on Skype - using a photo I just took of my favorite coaster sitting on my coffee table currently This is not only fun and useful it could be the precursor to adding text to speech, or TTS, to the bot itself as we've got a Cognitive Servies API for that (Bing Speech). You can, of course at this point even, pipe this text into one of your TTS apps already on your device. (Re)Introducing OCR OCR, or optical character recognition, is the electronic or mechnanical recognition of text in images and the conversion into machine-encoded representaions. The text could be handwritten, typed or printed. The data is an image such as scanned passports or financial data, business cards, postal mail, or any document as an image that someone wishes to digitize. Sometimes, OCR uses the human mind as the intelligence algorithms. An example is the use of reCAPTCHA as a crowdsourcing effort for a two-fold purpose: verification that the entity logging in somewhere is not a bot and crowdsourcing the recognition of hard to read text for the archiving of say 13 million articles from the NY Times starting with articles from 1851 which was accomplished in 2011 along with all of the books on Google Books. OCR is performed through pattern recognition with components often pulled from AI and computer vision. The process usually takes the form of: pre-processing (fixing skew, noise reduction, conversion to black-and-white, and the like) - see figure below; character or word recognition through feature extraction and, often, a k-nearest neighbors classification (see figure below), one of the simplest of ML algorithms; post-processing such as validation through seeing co-occurrences of words (words that usually go together like \"ice cream\" instead of \"ice pizza\"). See the Wikipedia article on OCR for more. In the Cognitive Services OCR, we have the idea of word detection within bounding boxes, combining pre-processing and several ML algorithms such as the feature extraction mentioned above, another set of algorithms for classification (including convolutional neural networks) and validation through simillar means as above, plus using a vocabulary and other techniques. Pre-processing example: An example of image pre-processing for character recognition: fixing skew, binarisation, despekling, and line removal k-nearest neighbor example: Classifying the orange letter as a blue D or green P. Note, that if k is 3, the orange letter is classified as a blue D, but with a k of 7 it is classified as a green P. The structure of the data can cause for a tricky problem in k-NN Nowadays, especially on devices like smart phones, the OCR model used to do this conversion to text is done in the cloud through an API. This is how the Computer Vision API for OCR under the Cognitive Services umbrealla on Azure gets it done. And for those who like a little history, note that OCR was known to be used in devices as early as 1914 with Emanuel Goldberg's invention of a machine that could read characters and convert them into standard telegraph code like Morse code (see this Wikipedia article for more history). Skip ahead to today and we have optical word recognition (commonly called OCR) used for typewritten text and others like intelligent character recognition (ICR) for handwritten or cursive text. How has OCRBot's code changed From the original OCRBot OCRBot began with the ability to take a web link of an image with text and give us back the actual text in Post 1 . Now we've updated OCRBot quite a bit to also accept images as attachments to the conversation. Simply check out the server.js at github link here for the new changes to OCRBot of which there are quite a few. Certainly an overhaul of sorts, one of which was the incorporation of promises... A Promise... What is a Promise? Let's start with a promise of a promise that I like (ok, I had to go there) from an article by Marc Harter found here . Promises are a compelling alternative to callbacks when dealing with asynchronous code. [...] Callbacks are the simplest possible mechanism for asynchronous code in JavaScript. Unfortunately, raw callbacks sacrifice the control flow, exception handling, and function semantics familiar from synchronous code. Promises provide a way to get those things back. See this article if you are unfamiliar with the meaning of asychronous. Key concepts (don't worry if this doesn't quite make sense yet) of a promise are as follows (paraphrased from info in Harter's article). A promise (one way to think about them), is a value representing an asynchronous operation that is only fufilled once, either being resolved or rejected (we'll see in code soon). then , a reserved word for promises, allows anyone with access to the promise to be able to consume it regardless of if the asynchronous operation is done or not . One way to think about them is as a function which unwraps the asynchronous operation's result from the promise. then returns a promise. catch is often used after a then , series of then s, or some nested then s to handle errors both implicitly and explicitly if the ned arises. We can still use return to model syncrhonous functions by returning a promise or any other value that and then signaling the next then , possibly giving this value to the next onFufilled . onFufilled and onRejected are the callbacks and handlers that then handles to signal what to do with the results of a promise. Here's a simple representation in code of the logic changes a promise provides. var promise = readFile () promise . then ( console . log , console . error ) In this example, readFile does, in fact, need to return a promise. This object can now be used with any then that has access to the promise. You'll notice that the then has two bits to it. If the promise is fufilled in readFile the first ( console.log ) chunk of code is called, and if rejected, the second chunk is called ( console.error ). We can create raw promises in the following way. In this example, we are using the fs library's readFile method, using reject to pass on the result of the promise in the case of an error, and resolve if it is fufilled or simply not rejected, wrapping all of this up in a function which returns the promise to be used by a then . Then, the next then that consumes this promise-returning function \"unwraps\" that logic. function readFileAsync ( file , encoding ) { return new Promise ( function ( resolve , reject ) { fs . readFile ( file , encoding , function ( err , data ) { if ( err ) return reject ( err ) // rejects the promise with `err` as the reason resolve ( data ) // fulfills the promise with `data` as the value }) }) } readFileAsync ( 'myfile.txt' ). then ( console . log , console . error ) In a similar way, the OCRBot's code now includes promises and this looks like the raw promise above with the clever use of a catch to catch any unhandled errors and give some information back to the app and user. var fileDownload = new Promise ( function ( resolve , reject ) { var check = checkRequiresToken ( msg ); if ( check == true ) { resolve ( requestWithToken ( attachment . contentUrl )); } else { resolve ( request_promise ( attachment . contentUrl )); } } ); fileDownload . then ( function ( response ) { readImageText ( response , attachment . contentType , function ( error , response , body ) { session . send ( extractText ( body )); }); }). catch ( function ( err , reply ) { console . log ( 'Error with attachment: ' , { statusCode : err . statusCode , message : err }); session . send ( \"Error with attachment or reading image with %s\" , err ); }); Promises are quite amazing, leading to resolving many issues around Node.js code such as too many nested callbacks (aka \"callback hell\") and clean error handling. I hope you got a bit of that enthusiasm from reading through this section. The end of this chapter and OCRBot's next adventure I'm actually not sure what will be in store for OCRBot next. There are so many fantastic \"smarts\" we could add or clever functionality. It'll have to wait and be revealed when OCRBot returns to this blog.","title":"OCRBot Gets Attached"},{"location":"ocrbot-gets-attached/#whats-the-wow-factor","text":"Have you ever just wanted the text extracted from, perhaps a page in a book or a funny comic that's sitting around in an image? Or maybe it'd be helpful to take a snapshot of a menu item wherein the font is a bit too small or you forgot your reading glasses, however, it's easy to read on your phone as plain text. Now, if you send the OCRBot an image (jpeg, png, gif, bmp are its known formats), on one of its supported conversational platforms like Skype or Slack, you'll get that text you crave. OCRBot on Skype - using a photo I just took of my favorite coaster sitting on my coffee table currently This is not only fun and useful it could be the precursor to adding text to speech, or TTS, to the bot itself as we've got a Cognitive Servies API for that (Bing Speech). You can, of course at this point even, pipe this text into one of your TTS apps already on your device.","title":"What's the wow factor?"},{"location":"ocrbot-gets-attached/#reintroducing-ocr","text":"OCR, or optical character recognition, is the electronic or mechnanical recognition of text in images and the conversion into machine-encoded representaions. The text could be handwritten, typed or printed. The data is an image such as scanned passports or financial data, business cards, postal mail, or any document as an image that someone wishes to digitize. Sometimes, OCR uses the human mind as the intelligence algorithms. An example is the use of reCAPTCHA as a crowdsourcing effort for a two-fold purpose: verification that the entity logging in somewhere is not a bot and crowdsourcing the recognition of hard to read text for the archiving of say 13 million articles from the NY Times starting with articles from 1851 which was accomplished in 2011 along with all of the books on Google Books. OCR is performed through pattern recognition with components often pulled from AI and computer vision. The process usually takes the form of: pre-processing (fixing skew, noise reduction, conversion to black-and-white, and the like) - see figure below; character or word recognition through feature extraction and, often, a k-nearest neighbors classification (see figure below), one of the simplest of ML algorithms; post-processing such as validation through seeing co-occurrences of words (words that usually go together like \"ice cream\" instead of \"ice pizza\"). See the Wikipedia article on OCR for more. In the Cognitive Services OCR, we have the idea of word detection within bounding boxes, combining pre-processing and several ML algorithms such as the feature extraction mentioned above, another set of algorithms for classification (including convolutional neural networks) and validation through simillar means as above, plus using a vocabulary and other techniques. Pre-processing example: An example of image pre-processing for character recognition: fixing skew, binarisation, despekling, and line removal k-nearest neighbor example: Classifying the orange letter as a blue D or green P. Note, that if k is 3, the orange letter is classified as a blue D, but with a k of 7 it is classified as a green P. The structure of the data can cause for a tricky problem in k-NN Nowadays, especially on devices like smart phones, the OCR model used to do this conversion to text is done in the cloud through an API. This is how the Computer Vision API for OCR under the Cognitive Services umbrealla on Azure gets it done. And for those who like a little history, note that OCR was known to be used in devices as early as 1914 with Emanuel Goldberg's invention of a machine that could read characters and convert them into standard telegraph code like Morse code (see this Wikipedia article for more history). Skip ahead to today and we have optical word recognition (commonly called OCR) used for typewritten text and others like intelligent character recognition (ICR) for handwritten or cursive text.","title":"(Re)Introducing OCR"},{"location":"ocrbot-gets-attached/#how-has-ocrbots-code-changed","text":"","title":"How has OCRBot's code changed"},{"location":"ocrbot-gets-attached/#from-the-original-ocrbot","text":"OCRBot began with the ability to take a web link of an image with text and give us back the actual text in Post 1 . Now we've updated OCRBot quite a bit to also accept images as attachments to the conversation. Simply check out the server.js at github link here for the new changes to OCRBot of which there are quite a few. Certainly an overhaul of sorts, one of which was the incorporation of promises...","title":"From the original OCRBot"},{"location":"ocrbot-gets-attached/#a-promise","text":"What is a Promise? Let's start with a promise of a promise that I like (ok, I had to go there) from an article by Marc Harter found here . Promises are a compelling alternative to callbacks when dealing with asynchronous code. [...] Callbacks are the simplest possible mechanism for asynchronous code in JavaScript. Unfortunately, raw callbacks sacrifice the control flow, exception handling, and function semantics familiar from synchronous code. Promises provide a way to get those things back. See this article if you are unfamiliar with the meaning of asychronous. Key concepts (don't worry if this doesn't quite make sense yet) of a promise are as follows (paraphrased from info in Harter's article). A promise (one way to think about them), is a value representing an asynchronous operation that is only fufilled once, either being resolved or rejected (we'll see in code soon). then , a reserved word for promises, allows anyone with access to the promise to be able to consume it regardless of if the asynchronous operation is done or not . One way to think about them is as a function which unwraps the asynchronous operation's result from the promise. then returns a promise. catch is often used after a then , series of then s, or some nested then s to handle errors both implicitly and explicitly if the ned arises. We can still use return to model syncrhonous functions by returning a promise or any other value that and then signaling the next then , possibly giving this value to the next onFufilled . onFufilled and onRejected are the callbacks and handlers that then handles to signal what to do with the results of a promise. Here's a simple representation in code of the logic changes a promise provides. var promise = readFile () promise . then ( console . log , console . error ) In this example, readFile does, in fact, need to return a promise. This object can now be used with any then that has access to the promise. You'll notice that the then has two bits to it. If the promise is fufilled in readFile the first ( console.log ) chunk of code is called, and if rejected, the second chunk is called ( console.error ). We can create raw promises in the following way. In this example, we are using the fs library's readFile method, using reject to pass on the result of the promise in the case of an error, and resolve if it is fufilled or simply not rejected, wrapping all of this up in a function which returns the promise to be used by a then . Then, the next then that consumes this promise-returning function \"unwraps\" that logic. function readFileAsync ( file , encoding ) { return new Promise ( function ( resolve , reject ) { fs . readFile ( file , encoding , function ( err , data ) { if ( err ) return reject ( err ) // rejects the promise with `err` as the reason resolve ( data ) // fulfills the promise with `data` as the value }) }) } readFileAsync ( 'myfile.txt' ). then ( console . log , console . error ) In a similar way, the OCRBot's code now includes promises and this looks like the raw promise above with the clever use of a catch to catch any unhandled errors and give some information back to the app and user. var fileDownload = new Promise ( function ( resolve , reject ) { var check = checkRequiresToken ( msg ); if ( check == true ) { resolve ( requestWithToken ( attachment . contentUrl )); } else { resolve ( request_promise ( attachment . contentUrl )); } } ); fileDownload . then ( function ( response ) { readImageText ( response , attachment . contentType , function ( error , response , body ) { session . send ( extractText ( body )); }); }). catch ( function ( err , reply ) { console . log ( 'Error with attachment: ' , { statusCode : err . statusCode , message : err }); session . send ( \"Error with attachment or reading image with %s\" , err ); }); Promises are quite amazing, leading to resolving many issues around Node.js code such as too many nested callbacks (aka \"callback hell\") and clean error handling. I hope you got a bit of that enthusiasm from reading through this section.","title":"A Promise..."},{"location":"ocrbot-gets-attached/#the-end-of-this-chapter-and-ocrbots-next-adventure","text":"I'm actually not sure what will be in store for OCRBot next. There are so many fantastic \"smarts\" we could add or clever functionality. It'll have to wait and be revealed when OCRBot returns to this blog.","title":"The end of this chapter and OCRBot's next adventure"},{"location":"ocrbot-makes-a-connection/","text":"Posted: 2016-11-15 A short conversation with OCRBot on Skype - using the Bot Framework on the Azure Cloud Update: November 16, 2016 Microsoft announced the Azure Bot Service in Preview. tl;dr : In Part 1 I built an OCR bot using the Bot Framework (BF) from Microsoft and the Cognitive Services Computer Vision API and conversed with it using the command line on my Mac. In this HowTo article I deploy the OCR bot to the cloud. If you keep reading you'll learn the technical know-how to take the bot code and turn it into deployed Skype bot. \"Hello, you've reached [name of company]. How can I help you today?\" --says the mechanical voice on the other end of the phone We know right away that this is a bot. But bots can be much more than a question and answer machine that deals in natural language. In fact, it doesn't have to be language intelligence at all. I could, for instance, send my bot an audio clip of my favorite song and it could send back the name of the singer. Or I could send my bot an image and it could tell me what the scene is like, who is in it, what other objects are there, etc. I could even leave intelligence out of it and use a bot to order a sandwich. Bots are just apps. In this case our bot uses optical character recognition (OCR) to extract text from images. All of the code is here . It almost goes without saying, but since the sky is really the limit, it's a good idea to be thoughtful in our creation and usage of these apps or bots. A current favorite quote is from Satya Nadella (from this article): A.I. must have algorithmic accountability so that humans can undo unintended harm. We must design these technologies for the expected and the unexpected. Now, let's continue our story of a chat bot, ocrbot. Ocrbot takes an image link as input and sends back the text found in that image, if any. I could imagine, then, doing more with that text (e.g. sentiment, key phrases) or extending this bot in other ways (e.g. speech, search). The Bot Framework gives me an easy way to connect my bot so that it's compatible and available on channels like Slack, Skype, Facebook Messenger, Twilio, and more. In the last post (\"Building an OCR Chat Bot with the Microsoft Bot Framework on my Mac\"), we met ocrbot and chatted with this bot locally (the upper path in the diagram below). This time we are going to deploy ocrbot to the cloud and communicate on a real channel (the lower path in the diagram below). My process for connecting ocrbot to the cloud To start, these are my subscriptions used: Github account (free) - for hosting code Azure account (free trial) - for continuous cloud deployment Microsoft account (free) - for Cognitive Services, BF and Skype Cognitive Services Computer Vision API key (free) - for OCR And these are my steps at a glance: The ocrbot gets forked GitHub Fork the repo (easier to start with the existing code) Update the README to say something useful for my purposes The ocrbot gets a web app service for continuous deployment Create a Web App in the Azure Portal for the bot's endpoint Choose GitHub as my deployment source Get the Cognitive Services Computer Vision API key Add some environment variables The ocrbot gets registered on the BF Fill out profile including url endpoint Record app ID and app password after configuration Update app service with the new app ID and password The ocrbot takes a test Test connection in BF Developer's Portal Test on Skype Update the bot's message on GitHub and observe the change mid-conversation The ocrbot gets forked on GitHub I logged into GitHub and navigated to the bot-education-ocrbot repository. Next, I forked the repository so that it would appear in my GitHub account profile. From there, I can now use it, push/pull and annotate with markdown text. Forking the ocrbot repository I like to change the README to say something specific to why I forked it like: Modifying README markdown file The ocrbot gets a web app service for continuous deployment Honestly, except for communicating on a channel with the bot, this is the coolest part in my opinion. I've set my bot up such that any change I commit or push to my GitHub repository, reflects immediately, even if I'm mid-conversation. So, since I'm using Microsoft's cloud, Azure, I signed into the Azure portal at https://portal.azure.com . I then added a Web App by clicking the \"+\" icon and searching for \"web app\" (also, found under \"Web and Mobile\"). Selecting Web App from portal menu I filled out all of the information and created the web app. I then went to my resources in the portal (the blue cube - first icon below the \"+\" icon on the left panel) and selected my newly created resource group. In that resource group I found my web app (labelled as an \"App Service\"). It opened what we call a \"blade\" and in that I navigated to \"Deployment options\" from which I can select different sources. In this instance I selected \"GitHub\" as in: Selecting GitHub from the web app deployment source blade (aka App Service) menu Using this wizard, I authorized with my GitHub account credentials (through GitHub launched within the wizard) for the web app to be able to pull in my code each time a change happens. I selected my project or repo (bot-education-ocrbot in this case) and clicked \"Ok.\" Continuous deployment deployed! The final setup step in this section was to add placeholder variables for the BF app ID and password that I obtain in the next section. This is going to make it so that the BF and my app can talk to each other. To do this I clicked on \"Application Settings\" under \"Settings\" (just below the \"Deployment options\"). This took me to a blade within which I scrolled down to \"App settings\" and entered in key-value pairs with filler text that correspond to the variable names in the configuration.js from my project (so, MICROSOFT_APP_ID and MICROSOFT_APP_PASSWORD ). I didn't need to do it right at that point, but thought it'd be a good idea so I didn't overlook later (a string on my finger): App environment variables which correspond to the environment variables in ocrbot's config file The actual values will be filled in in the next section. Also, in this settings blade, I created a variable corresponding to my Cognitive Services Computer Vision API key so I could use their OCR service. Therefore, I entered in a third variable, the VISION_API_KEY below my other two. I set it to my actual, real key from my subscription. To get this free key, btw, I simply went to the Cognitive Services APIs website , My Account (I used my Microsoft Account - used to be called Live ID - which is just my gmail account linked up; if I had an xbox, hotmail, or outlook.com account I would already have one), and signed up for a trial subscription for Computer Vision. It's just the free tier of a Microsoft service. Make sure to then save the settings in this blade. The ocrbot gets registered on the BF This part is pretty painless. Basically, we go to the Bot Framework Developer's Portal (same site where the docs and Bot Directory live) at https://dev.botframework.com , fill out a profile, do a small config and that's it. I called my bot, ocrbot (doesn't have to be unique) and gave it a public handle, ocrbot100 (has to be globally unique). My ocrbot's profile For the Configuration part, the messaging endpoint is the web app service URL (I went back to the Azure portal to grab this URL from the web app service - In \"Overview\") appended \"/api/messages\" to the end of it and changed http to https , all of which so that the Bot Connector can route the messages correctly (the Bot Connector is a component of the BF which handles many things including routing messages). For me this was something like: https://nameofwebapp.azurewebsites.net/api/messages . Also in the configuration part, a wizard took me through the process of getting the app ID and password and I just had to make sure to record the password in the pop-up and the app ID on the profile page. Yup, that's the same app ID and password I set up dummy environment variables earlier in the Azure portal. Now they will have real values. Except for pasting these values back into the Azure portal, the registration in the BF Developer's portal is done. So, I went ahead and did the pasting. The ocrbot takes a test Finally, the really fun part: here, I got to check my bot's connection and then have a real conversation. Back in the BF Dev Portal I went to \"Test connection to your bot\" and clicked on the \"Test\" button as shown here which pings my bot's messaging endpoint to confirm a connection. Testing in the Developer's Portal I finally and with excitement scrolled down on the page shown above and clicked on \"Add to Skype.\" After launching Skype (I had to make sure I was logged into Skype with the same Microsoft ID I was using in the Dev Portal) I tried sending some messages: a greeting and some image URLs from the web. I was curious to see if ocrbot liked Johnny Cash. Why not? ocrbot goes country - or at least reads country song lyrics from an image To test the nifty continuous deployment from GitHub, I changed ocrbot's message on GitHub and sync'd that repository in the Azure Portal (under the web app service and \"Deployment Options\"). This happened mid-conversation: ocrbot's message updated mid-conversation Well, that's it folks. To recap, ocrbot and I accomplished: Forking the original ocrbot repository from GitHub into my GitHub Deploying ocrbot as a web app service on Azure Registering ocrbot with the Bot Framework Taking ocrbot out for a spin on Skype Ocrbot stays busy in the next topics in this blog series: Ocrbot makes a friend (on Slack) Ocrbot gets attached (attachments) Ocrbot learns to talk (speech APIs) Ocrbot goes to the store (bring your own data)","title":"OCRBot Makes a Connection to the Cloud"},{"location":"ocrbot-makes-a-connection/#the-ocrbot-gets-forked-on-github","text":"I logged into GitHub and navigated to the bot-education-ocrbot repository. Next, I forked the repository so that it would appear in my GitHub account profile. From there, I can now use it, push/pull and annotate with markdown text. Forking the ocrbot repository I like to change the README to say something specific to why I forked it like: Modifying README markdown file","title":"The ocrbot gets forked on GitHub"},{"location":"ocrbot-makes-a-connection/#the-ocrbot-gets-a-web-app-service-for-continuous-deployment","text":"Honestly, except for communicating on a channel with the bot, this is the coolest part in my opinion. I've set my bot up such that any change I commit or push to my GitHub repository, reflects immediately, even if I'm mid-conversation. So, since I'm using Microsoft's cloud, Azure, I signed into the Azure portal at https://portal.azure.com . I then added a Web App by clicking the \"+\" icon and searching for \"web app\" (also, found under \"Web and Mobile\"). Selecting Web App from portal menu I filled out all of the information and created the web app. I then went to my resources in the portal (the blue cube - first icon below the \"+\" icon on the left panel) and selected my newly created resource group. In that resource group I found my web app (labelled as an \"App Service\"). It opened what we call a \"blade\" and in that I navigated to \"Deployment options\" from which I can select different sources. In this instance I selected \"GitHub\" as in: Selecting GitHub from the web app deployment source blade (aka App Service) menu Using this wizard, I authorized with my GitHub account credentials (through GitHub launched within the wizard) for the web app to be able to pull in my code each time a change happens. I selected my project or repo (bot-education-ocrbot in this case) and clicked \"Ok.\" Continuous deployment deployed! The final setup step in this section was to add placeholder variables for the BF app ID and password that I obtain in the next section. This is going to make it so that the BF and my app can talk to each other. To do this I clicked on \"Application Settings\" under \"Settings\" (just below the \"Deployment options\"). This took me to a blade within which I scrolled down to \"App settings\" and entered in key-value pairs with filler text that correspond to the variable names in the configuration.js from my project (so, MICROSOFT_APP_ID and MICROSOFT_APP_PASSWORD ). I didn't need to do it right at that point, but thought it'd be a good idea so I didn't overlook later (a string on my finger): App environment variables which correspond to the environment variables in ocrbot's config file The actual values will be filled in in the next section. Also, in this settings blade, I created a variable corresponding to my Cognitive Services Computer Vision API key so I could use their OCR service. Therefore, I entered in a third variable, the VISION_API_KEY below my other two. I set it to my actual, real key from my subscription. To get this free key, btw, I simply went to the Cognitive Services APIs website , My Account (I used my Microsoft Account - used to be called Live ID - which is just my gmail account linked up; if I had an xbox, hotmail, or outlook.com account I would already have one), and signed up for a trial subscription for Computer Vision. It's just the free tier of a Microsoft service. Make sure to then save the settings in this blade.","title":"The ocrbot gets a web app service for continuous deployment"},{"location":"ocrbot-makes-a-connection/#the-ocrbot-gets-registered-on-the-bf","text":"This part is pretty painless. Basically, we go to the Bot Framework Developer's Portal (same site where the docs and Bot Directory live) at https://dev.botframework.com , fill out a profile, do a small config and that's it. I called my bot, ocrbot (doesn't have to be unique) and gave it a public handle, ocrbot100 (has to be globally unique). My ocrbot's profile For the Configuration part, the messaging endpoint is the web app service URL (I went back to the Azure portal to grab this URL from the web app service - In \"Overview\") appended \"/api/messages\" to the end of it and changed http to https , all of which so that the Bot Connector can route the messages correctly (the Bot Connector is a component of the BF which handles many things including routing messages). For me this was something like: https://nameofwebapp.azurewebsites.net/api/messages . Also in the configuration part, a wizard took me through the process of getting the app ID and password and I just had to make sure to record the password in the pop-up and the app ID on the profile page. Yup, that's the same app ID and password I set up dummy environment variables earlier in the Azure portal. Now they will have real values. Except for pasting these values back into the Azure portal, the registration in the BF Developer's portal is done. So, I went ahead and did the pasting.","title":"The ocrbot gets registered on the BF"},{"location":"ocrbot-makes-a-connection/#the-ocrbot-takes-a-test","text":"Finally, the really fun part: here, I got to check my bot's connection and then have a real conversation. Back in the BF Dev Portal I went to \"Test connection to your bot\" and clicked on the \"Test\" button as shown here which pings my bot's messaging endpoint to confirm a connection. Testing in the Developer's Portal I finally and with excitement scrolled down on the page shown above and clicked on \"Add to Skype.\" After launching Skype (I had to make sure I was logged into Skype with the same Microsoft ID I was using in the Dev Portal) I tried sending some messages: a greeting and some image URLs from the web. I was curious to see if ocrbot liked Johnny Cash. Why not? ocrbot goes country - or at least reads country song lyrics from an image To test the nifty continuous deployment from GitHub, I changed ocrbot's message on GitHub and sync'd that repository in the Azure Portal (under the web app service and \"Deployment Options\"). This happened mid-conversation: ocrbot's message updated mid-conversation Well, that's it folks. To recap, ocrbot and I accomplished: Forking the original ocrbot repository from GitHub into my GitHub Deploying ocrbot as a web app service on Azure Registering ocrbot with the Bot Framework Taking ocrbot out for a spin on Skype Ocrbot stays busy in the next topics in this blog series: Ocrbot makes a friend (on Slack) Ocrbot gets attached (attachments) Ocrbot learns to talk (speech APIs) Ocrbot goes to the store (bring your own data)","title":"The ocrbot takes a test"},{"location":"setting-up-for-and-work-with-mlflow/","text":"tl;dr : MLflow is already powerful, yet simple, even in alpha release. It' integration with platforms for training and deployment, such as with AzureML, is incredibly helpful. After all, don't we all want to deploy eventually? Posted: 2019-09-15 Introduction to MLFlow Recently, I discovered MLflow on a hunt for a way to track my models and environment, parameters, training and deployments (that was a nice addition) in a single project and a single place. A few features that caught my eye: Run training and deployment from a remote GitHub repo Log the environment in which a model was trained (other logging and even a UI for tracking) Many deployment options (local and Azure ML among others) After trying MLflow, I discovered that I liked: The flexibility of running the project in several locations (local, Databricks , remote linux VMs, etc.) Error messaging is clear and includes a nice stacktrace The deployment was straighforward as the project for AzureML was generated for me with the files I needed Get the Model and Data Feel free to follow along or create your own project. This is an example of using MLFlow with an existing repo. Clone the YOLOv3 Keras example repo: git clone https://github.com/michhar/mlflow-keras-example.git cd into model directory: cd mlflow-keras-example/model_data/ Download the model, data (or create your own) and pointer file to data. Get a Keras-friendly YOLOv3 base model, converted directly from the Tiny YOLOv3 Darknet model (here, the Tiny weights are used - nice for constrained devices): Click to access through Azure Storage here (34MB) , download and place model in model_data subdirectory Get some sample data of lego minifigures with helmets and other head gear to train a model to detect what the figurine is wearing on its head, placing the uznipped folder in the voc subdirectory. Click to access through from here (173MB) or use your own data (according to instructions on this repo in Step 1 - you'll need to label as well). Get the list of images as a small text file with associated bounding boxes and class. Click to access from here and place it in the voc subdirectory Setup for MLflow Required Python packages: * mlflow * azure-cli (if deploying with AzureML) * To deploy with azureml one will need, also, an Azure Subscription. MLproject file is an excellent source of control over things. Optional, but recommended for the following reasons: * Points to conda dependencies file for building this environment before training * Parameters, types and defaults * Entrypoint command (with all options) - this is the master command that is run when the mlflow training is run on the command line A simplified MLproject file is as follows: name: My Awesome Project conda_env: smart_conda.yaml entry_points: main: parameters: parameter1: {type: str, default: \"voc/list_master.txt\"} parameter2: {type: float, default: 1e-2} parameter3: {type: int, default: 16} command: \"python train.py --parameter1 {parameter1} --parameter2 {parameter2} --parameter3 {parameter3}\" If there are defaults, none of these parameters need to exist on the command line when running with mlflow , however they may be overridden. Training To train, all you should need to do from within the cloned repo folder is (runs with default parameters in MLproject entrypoint command): mlfow run . Or if you want to modify a default parameter or two (use -P per parameter) like the number of epochs for the transfer learning stage ( frozen_epochs ) and network fine tuning stage ( fine_tune_epochs ) (note you'd use 100s to 1000s of epochs for these in the real world): mlflow run . -P frozen_epochs=5 -P fine_tune_epochs=3 Also, you can monitor the run through tensorboard which is part of a callback in the model.fit method (change logdir as appropriate). tensorboard --logdir logs/default An mlflow option for monitoring is with: mlflow ui The metric and model (as an artifact) is recorded by the following: # Added for MLflow mlflow . keras . log_model ( model , \"keras-yolo-model-frozen-pass\" ) mlflow . log_metric ( 'frozen_loss' , history . history [ 'val_loss' ][ - 1 ]) What does mlflow run actually do? As follows: Creates the conda environment based on the yolo_conda.yml file (or whatever you named the dependencies file) Runs the entrypoint script described in the MLproject file (this file is not mandatory, but makes customizing the training process much easier). Logs any artifacts specified in train.py (e.g. save model as an asset with mlflow.keras.log_model(model, \"keras-yolo-model-frozen-pass\") ) Logs any metrics specified in train.py (e.g. log loss with mlflow.log_metric('finetune_loss', model.loss) ) Artifacts If model is logged as in this training script, a folder should appear in the mlflow UI with information on the training run and the model itself. To find the model, look in the mlruns directory at the base of the project: ls mlruns/0/<run id given at end of successful run>/artifacts/<model key from train.py> E.g. For the final model after fine-tuning: ls mlruns/0/8237d734f1d94fd893368dd455565f2d/artifacts/keras-yolo-model It will be called something like model.h5 . Deploying with the Azure Machine Learning Integration Now, to get the package for the AzureML deployment CLI: pip install -r https://aka.ms/az-ml-o16n-cli-requirements-file (Corresponding to azure-cli-ml==0.1.0a27.post3 at the time of writing). See AzureML export option and CLI commands from mlflow for the detailed directions. For instance, to export to azureml -friendly deployment format/structure (and create neccessary files for this deployment type) the command will have the format: mlflow azureml export -m mlruns/<run folder>/<run id>/artifacts/<name of mlflow project> -o <name of new folder for azureml> E.g.: mlflow azureml export -m mlruns/0/8237d734f1d94fd893368dd455565f2d/artifacts/keras-yolo-model -o yolo-output Note, some additional libraries may need to be specified in the generated score.py 's init() and run() , such as keras here: def init (): global model import keras model = load_pyfunc ( \"model\" ) def run ( s ): import keras input_df = pd . read_json ( s , orient = \"records\" ) return get_jsonable_obj ( model . predict ( input_df )) Note: not including necessary packages is the most common source of error in deploying with AzureML Ensure yolo-output (or name used for mlflow generated file directory) has all necessary packages beyond mlflow , namely: numpy==1.14.2 matplotlib==2.2.2 Keras==2.2.2 tensorflow==1.8.0 Pillow==5.1.0 mlflow==0.5.2 The azureml CLI commands are: Note: one may need to register some environment providers in Azure. az provider register -n Microsoft.MachineLearningCompute az provider register -n Microsoft.ContainerRegistry az provider register -n Microsoft.ContainerService az login az ml env setup -l [Azure Region, e.g. eastus2] -n [your environment name] [-g [existing resource group]] az ml env set -n [environment name] -g [resource group] mlflow azureml deploy <parameters> See AzureML documentation for more information on the az ml commands for deployment or type az ml -h . Use a model management account (or create one). List them with: az ml account modelmanagement list Set one with: az ml account modelmanagement set -g [resource group] -n [model management name] For example, the commands used in this project to deploy locally are as follows Log in to Azure: az login Set up an environment: az ml env setup -l eastus2 -g localyoloenvrg -n localyoloenv This will take a few minutes. Choose the environment: az ml env set -g localyoloenvrg -n localyoloenv Deploy the project, now (locally, but linked to a few resources online): mlflow azureml deploy --model-path model -n yoloapp123 When done, clean up by deleting the resource group with: az group delete -g localyoloenvrg Setting up MLflow on Databricks Recommendations. Set this up and begin with a PyTorch tutorial with Mlflow server to ensure all is well before moving on to code. Note: Make sure the versions of packages on Databricks matches the versions of packages on the Linux VM hosting the Mlflow tracking server. Provision Databricks workspace on Azure - Premium tier in WestUS2 (Premium tier for access control so we can manage users) Provision a small, general-purpose Linux VM (Ubuntu 16.04 Server is good) for an ML tracking server to let us monitor, log and save artifacts from training experiments on Databricks: Use Password for authentication Open up traffic to 80 (HTTP) and 22 (SSH) Provision a new Storage Account or create a container for mlflow artifacts in an existing blob Storage Account. Follow directions for setting up the Mlflow tracking server on this new Linux VM at https://docs.azuredatabricks.net/spark/latest/mllib/mlflow.html#mlflow-mlflow-quick-start-notebook under \"Set up a Remote MLflow Tracking Server\" Use \"python3\" and \"pip3\" To get requirements installed (namely azure-storage ), must first run from the VM: sudo apt-get install python3-pip python3-dev libffi-dev libssl-dev libxml2-dev libxslt1-dev libjpeg8-dev zlib1g-dev Need to install \"flask\" to get this server working right: pip3 install flask Add inbound rule for port 5000 to the network security gateway for this VM (should be in the same resource group as VM) - docs for this at https://docs.microsoft.com/en-us/azure/virtual-machines/windows/nsg-quickstart-portal , but just note, an NSG already exists, so just add the rule there When starting the server make sure to start it in background with \"&\" - it should then be at your http://dnsname:5000 Set up for PyTorch and MLFlow by following this tutorial (instructions on Cluster set up there - do GPU with runtime 4.3): https://docs.azuredatabricks.net/_static/notebooks/mlflow/mlflow-pytorch-azure.html Make sure to set the Azure blob storage connection string in Databricks before trying Mlflow server tracking (see https://docs.azuredatabricks.net/spark/latest/mllib/mlflow.html#mlflow-mlflow-quick-start-notebook ) If encountering trouble with cluster deployment, try Worker as NC12 and driver as NC12 type VM and ensure you have the proper quotas Set these in something like \".azurerc\" and source it before launching the tracking server: AZURE_STORAGE_ACCESS_KEY= AZURE_STORAGE_CONNECTION_STRING=\"DefaultEndpointsProtocol=https;AccountName=whatsinaname;AccountKey=lotsofalphanumericsymbols==;EndpointSuffix=core.windows.net\" Example See an example MLflow project at https://github.com/michhar/mlflow-keras-example . /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */ var disqus_config = function () { this.page.url = 'https://michhar.github.io/setting-up-for-and-work-with-mlflow/'; // Replace PAGE_URL with your page's canonical URL variable this.page.identifier = 'happycat1'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable }; (function() { // DON'T EDIT BELOW THIS LINE var d = document, s = d.createElement('script'); s.src = 'https://michhar.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); Please enable JavaScript to view the comments powered by Disqus.","title":"Working with MLflow - A Reproducible Project Pattern for ML Training and Deployments"},{"location":"setting-up-for-and-work-with-mlflow/#introduction-to-mlflow","text":"Recently, I discovered MLflow on a hunt for a way to track my models and environment, parameters, training and deployments (that was a nice addition) in a single project and a single place. A few features that caught my eye: Run training and deployment from a remote GitHub repo Log the environment in which a model was trained (other logging and even a UI for tracking) Many deployment options (local and Azure ML among others) After trying MLflow, I discovered that I liked: The flexibility of running the project in several locations (local, Databricks , remote linux VMs, etc.) Error messaging is clear and includes a nice stacktrace The deployment was straighforward as the project for AzureML was generated for me with the files I needed","title":"Introduction to MLFlow"},{"location":"setting-up-for-and-work-with-mlflow/#get-the-model-and-data","text":"Feel free to follow along or create your own project. This is an example of using MLFlow with an existing repo. Clone the YOLOv3 Keras example repo: git clone https://github.com/michhar/mlflow-keras-example.git cd into model directory: cd mlflow-keras-example/model_data/ Download the model, data (or create your own) and pointer file to data. Get a Keras-friendly YOLOv3 base model, converted directly from the Tiny YOLOv3 Darknet model (here, the Tiny weights are used - nice for constrained devices): Click to access through Azure Storage here (34MB) , download and place model in model_data subdirectory Get some sample data of lego minifigures with helmets and other head gear to train a model to detect what the figurine is wearing on its head, placing the uznipped folder in the voc subdirectory. Click to access through from here (173MB) or use your own data (according to instructions on this repo in Step 1 - you'll need to label as well). Get the list of images as a small text file with associated bounding boxes and class. Click to access from here and place it in the voc subdirectory","title":"Get the Model and Data"},{"location":"setting-up-for-and-work-with-mlflow/#setup-for-mlflow","text":"Required Python packages: * mlflow * azure-cli (if deploying with AzureML) * To deploy with azureml one will need, also, an Azure Subscription. MLproject file is an excellent source of control over things. Optional, but recommended for the following reasons: * Points to conda dependencies file for building this environment before training * Parameters, types and defaults * Entrypoint command (with all options) - this is the master command that is run when the mlflow training is run on the command line A simplified MLproject file is as follows: name: My Awesome Project conda_env: smart_conda.yaml entry_points: main: parameters: parameter1: {type: str, default: \"voc/list_master.txt\"} parameter2: {type: float, default: 1e-2} parameter3: {type: int, default: 16} command: \"python train.py --parameter1 {parameter1} --parameter2 {parameter2} --parameter3 {parameter3}\" If there are defaults, none of these parameters need to exist on the command line when running with mlflow , however they may be overridden.","title":"Setup for MLflow"},{"location":"setting-up-for-and-work-with-mlflow/#training","text":"To train, all you should need to do from within the cloned repo folder is (runs with default parameters in MLproject entrypoint command): mlfow run . Or if you want to modify a default parameter or two (use -P per parameter) like the number of epochs for the transfer learning stage ( frozen_epochs ) and network fine tuning stage ( fine_tune_epochs ) (note you'd use 100s to 1000s of epochs for these in the real world): mlflow run . -P frozen_epochs=5 -P fine_tune_epochs=3 Also, you can monitor the run through tensorboard which is part of a callback in the model.fit method (change logdir as appropriate). tensorboard --logdir logs/default An mlflow option for monitoring is with: mlflow ui The metric and model (as an artifact) is recorded by the following: # Added for MLflow mlflow . keras . log_model ( model , \"keras-yolo-model-frozen-pass\" ) mlflow . log_metric ( 'frozen_loss' , history . history [ 'val_loss' ][ - 1 ]) What does mlflow run actually do? As follows: Creates the conda environment based on the yolo_conda.yml file (or whatever you named the dependencies file) Runs the entrypoint script described in the MLproject file (this file is not mandatory, but makes customizing the training process much easier). Logs any artifacts specified in train.py (e.g. save model as an asset with mlflow.keras.log_model(model, \"keras-yolo-model-frozen-pass\") ) Logs any metrics specified in train.py (e.g. log loss with mlflow.log_metric('finetune_loss', model.loss) )","title":"Training"},{"location":"setting-up-for-and-work-with-mlflow/#artifacts","text":"If model is logged as in this training script, a folder should appear in the mlflow UI with information on the training run and the model itself. To find the model, look in the mlruns directory at the base of the project: ls mlruns/0/<run id given at end of successful run>/artifacts/<model key from train.py> E.g. For the final model after fine-tuning: ls mlruns/0/8237d734f1d94fd893368dd455565f2d/artifacts/keras-yolo-model It will be called something like model.h5 .","title":"Artifacts"},{"location":"setting-up-for-and-work-with-mlflow/#deploying-with-the-azure-machine-learning-integration","text":"Now, to get the package for the AzureML deployment CLI: pip install -r https://aka.ms/az-ml-o16n-cli-requirements-file (Corresponding to azure-cli-ml==0.1.0a27.post3 at the time of writing). See AzureML export option and CLI commands from mlflow for the detailed directions. For instance, to export to azureml -friendly deployment format/structure (and create neccessary files for this deployment type) the command will have the format: mlflow azureml export -m mlruns/<run folder>/<run id>/artifacts/<name of mlflow project> -o <name of new folder for azureml> E.g.: mlflow azureml export -m mlruns/0/8237d734f1d94fd893368dd455565f2d/artifacts/keras-yolo-model -o yolo-output Note, some additional libraries may need to be specified in the generated score.py 's init() and run() , such as keras here: def init (): global model import keras model = load_pyfunc ( \"model\" ) def run ( s ): import keras input_df = pd . read_json ( s , orient = \"records\" ) return get_jsonable_obj ( model . predict ( input_df )) Note: not including necessary packages is the most common source of error in deploying with AzureML Ensure yolo-output (or name used for mlflow generated file directory) has all necessary packages beyond mlflow , namely: numpy==1.14.2 matplotlib==2.2.2 Keras==2.2.2 tensorflow==1.8.0 Pillow==5.1.0 mlflow==0.5.2 The azureml CLI commands are: Note: one may need to register some environment providers in Azure. az provider register -n Microsoft.MachineLearningCompute az provider register -n Microsoft.ContainerRegistry az provider register -n Microsoft.ContainerService az login az ml env setup -l [Azure Region, e.g. eastus2] -n [your environment name] [-g [existing resource group]] az ml env set -n [environment name] -g [resource group] mlflow azureml deploy <parameters> See AzureML documentation for more information on the az ml commands for deployment or type az ml -h . Use a model management account (or create one). List them with: az ml account modelmanagement list Set one with: az ml account modelmanagement set -g [resource group] -n [model management name] For example, the commands used in this project to deploy locally are as follows Log in to Azure: az login Set up an environment: az ml env setup -l eastus2 -g localyoloenvrg -n localyoloenv This will take a few minutes. Choose the environment: az ml env set -g localyoloenvrg -n localyoloenv Deploy the project, now (locally, but linked to a few resources online): mlflow azureml deploy --model-path model -n yoloapp123 When done, clean up by deleting the resource group with: az group delete -g localyoloenvrg","title":"Deploying with the Azure Machine Learning Integration"},{"location":"setting-up-for-and-work-with-mlflow/#setting-up-mlflow-on-databricks","text":"Recommendations. Set this up and begin with a PyTorch tutorial with Mlflow server to ensure all is well before moving on to code. Note: Make sure the versions of packages on Databricks matches the versions of packages on the Linux VM hosting the Mlflow tracking server. Provision Databricks workspace on Azure - Premium tier in WestUS2 (Premium tier for access control so we can manage users) Provision a small, general-purpose Linux VM (Ubuntu 16.04 Server is good) for an ML tracking server to let us monitor, log and save artifacts from training experiments on Databricks: Use Password for authentication Open up traffic to 80 (HTTP) and 22 (SSH) Provision a new Storage Account or create a container for mlflow artifacts in an existing blob Storage Account. Follow directions for setting up the Mlflow tracking server on this new Linux VM at https://docs.azuredatabricks.net/spark/latest/mllib/mlflow.html#mlflow-mlflow-quick-start-notebook under \"Set up a Remote MLflow Tracking Server\" Use \"python3\" and \"pip3\" To get requirements installed (namely azure-storage ), must first run from the VM: sudo apt-get install python3-pip python3-dev libffi-dev libssl-dev libxml2-dev libxslt1-dev libjpeg8-dev zlib1g-dev Need to install \"flask\" to get this server working right: pip3 install flask Add inbound rule for port 5000 to the network security gateway for this VM (should be in the same resource group as VM) - docs for this at https://docs.microsoft.com/en-us/azure/virtual-machines/windows/nsg-quickstart-portal , but just note, an NSG already exists, so just add the rule there When starting the server make sure to start it in background with \"&\" - it should then be at your http://dnsname:5000 Set up for PyTorch and MLFlow by following this tutorial (instructions on Cluster set up there - do GPU with runtime 4.3): https://docs.azuredatabricks.net/_static/notebooks/mlflow/mlflow-pytorch-azure.html Make sure to set the Azure blob storage connection string in Databricks before trying Mlflow server tracking (see https://docs.azuredatabricks.net/spark/latest/mllib/mlflow.html#mlflow-mlflow-quick-start-notebook ) If encountering trouble with cluster deployment, try Worker as NC12 and driver as NC12 type VM and ensure you have the proper quotas Set these in something like \".azurerc\" and source it before launching the tracking server: AZURE_STORAGE_ACCESS_KEY= AZURE_STORAGE_CONNECTION_STRING=\"DefaultEndpointsProtocol=https;AccountName=whatsinaname;AccountKey=lotsofalphanumericsymbols==;EndpointSuffix=core.windows.net\"","title":"Setting up MLflow on Databricks"},{"location":"setting-up-for-and-work-with-mlflow/#example","text":"See an example MLflow project at https://github.com/michhar/mlflow-keras-example . /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */ var disqus_config = function () { this.page.url = 'https://michhar.github.io/setting-up-for-and-work-with-mlflow/'; // Replace PAGE_URL with your page's canonical URL variable this.page.identifier = 'happycat1'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable }; (function() { // DON'T EDIT BELOW THIS LINE var d = document, s = d.createElement('script'); s.src = 'https://michhar.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); Please enable JavaScript to view the comments powered by Disqus.","title":"Example"},{"location":"single-artifical-neuron-for-nonlinear-separable-data/","text":"tl:dr : Getting a simple, predictive framework distinguishing two types of leukemia based on biological markers from a single-layer neural network was not the intent of this exercise. It is, however, indicative of the power of a single artificial neuron and thoughtful feature reduction. Posted: 2017-07-19 Introduction The intent of this post originally was to show the inner workings and limitations of a single artificial neuron using some moderately complex, noisy data; a challenge of sorts - \"is this noisy data linearly separable with a single artificial neuron and if not, why is that?\". However, I found with some data and algorithm exploration, that I could distinguish between two types of leukemia \u2014 a naive approach and not really biologically significant, but an interesting outcome nonetheless. So, even though this post is about the data science, it also touches on a potential method to use in the real world. In this post, you'll find information on the use of PCA for data reduction/feature engineering, scaling and normalization for preprocessing, the Adaline algorithm (artificial neuron), different activation functions, among other topics and concepts. What is an Adaline artificial neuron Adaline with a sigmoid activation function Choosing an activation function The noisy data 3D to run through network and 2D to gain insights Conclusion from my experiment Credits and further reading What is an Adaline artificial neuron The ADAptive LInear NEuron (Adaline) algorithm is very similar to a Perceptron (simplest of the artificial neurons) except that in the Perceptron the weights are updated based on a unit step activation function output (see figure below) whereas Adaline uses a linear activation function to update it's weights giving it a more robust result (that even converges with samples that are not completely separable by a linear hyperplane, unlike the Perceptron). In Adaline a quantizer after the activation function, is used to then predict class labels. Beyond the linear activation function and the quantizer , we see the use of a cost function , or objective function , to update the weights. In this case we want to minimize this function with an optimization method. The optimization of the cost function happens with yet another function aptly and simply named an optimization function . In this case our optimization function is stochastic gradient decent , which one can of as \"climbing down a hill\" (using part of the data to calculate, shuffled as well) to get to the minima of the cost function's convex curve (as it updates weights iteratively from a shuffled dataset). A really great discussion from which much of this information was adapted can be found in Sebastian Raschka's Python Machine Learning book (link here ) and excellent blog post on this topic here on the single-layer neurons. Adaline with a sigmoid activation function I grabbed Raschka's ADAptive LInear NEuron (Adaline) classifier open-source code here (the AdalineSGD class) and updated the activation function to logistic sigmoid from a linear function. Note, with the Adaline (versus the Perceptron) we use a continuous number rather than the binary class label, to compute the model error and update the weights. Then to predict a class label, another function is used called a quantizer . Also, the weights are updated in a more sophisticated manner. Choosing an activation function In code, given this \"net input\" function: def net_input ( self , X ): \"\"\"Calculate net input\"\"\" return np . dot ( X , self . w_ [ 1 :]) + self . w_ [ 0 ] I update the activation function from linear as in: def activation ( self , X ): \"\"\"Compute linear activation\"\"\" return self . net_input ( X ) To a logistic sigmoidal function: def activation ( self , X ): \"\"\"Compute sigmoidal activation Returns ------- A 1d array of length n_samples \"\"\" x = self . net_input ( X ) func = lambda v : 1 / ( 1 + math . exp ( - v )) return np . array ( list ( map ( func , x ))) Full code here and here . We still get linear classification boundaries These single-neuron classifiers can only result in linear decision boundaries, even if using a non-linear activation, because it's still using a single threshold value, z as in diagram above, to decide whether a data point is classified as 1 or -1. The noisy data The data was downloaded from the Machine Learning Data Set Repository mldata.org using a convenience function from scikit-learn . from sklearn.datasets.mldata import fetch_mldata # Fetch a small leukemia dataset from mldata.org # http://mldata.org/repository/data/viewslug/leukemia-all-vs-aml/ test_data_home = tempfile . mkdtemp () leuk = fetch_mldata ( 'leukemia' , transpose_data = True , data_home = test_data_home ) The data is a small, but wide acute lymphocytic leukemia (ALL) vs. acute myelogenous leukemia (AML) dataset. It has approximately 7000 biological markers (our features), vs. 72 samples (our data points). Given the noisy nature of the data and possible skewedness, it was standardized and normalized with convenience functions from scikit-learn : from sklearn.preprocessing import RobustScaler , Normalizer # Fit the scalar to the training dataset for # zero mean and unit variance of features. # Using a robust scaler which is more resistent to outliers. scaler = RobustScaler () scaler . fit ( X_train ) # Apply the transform X_train = scaler . transform ( X_train ) # Apply the same transform to the test dataset # (simulating what happens when we get new data) X_test = scaler . transform ( X_test ) # Normalizing data as well to scale samples to unit norm normalizer = Normalizer () . fit ( X_train ) X_train = normalizer . transform ( X_train ) X_test = normalizer . transform ( X_test ) Full code here and here . I tried just one feature reduction with PCA to reduce all 7129 dimensions to 2D at first. However, I could not separate out the ALL samples from AML - this wasn't necessarily important to my post on Adaline neurons I was writing, but I decided to try something I'd read about recently for kicks. In fact the idea sprung from a comment in a Python script where a perceptron was used to create non-linear separation of data for a plot (from this script on Github). The comment went: # map the data into a space with one addition dimension so that # it becomes linearly separable So, I gave it a shot. 3D to run through network and 2D to gain insights My next step was to try feeding the neural network the data in 3D space (the 3 features or components from the first PCA reduction). I then reduced the 3D data to 2D, mainly to visualize it. A hyperplane was drawn (blank dashed line) to represent the decision boundary. The surface in the diagram below is representative of a sigmoidal output along the direction of the weight vector. Full code here and here . Note, the stochastic part of the single-neuron optimizer, stochastic gradient decent, causes some variation in the results if run again. It might be a good idea to do a batch version of the Adaline neuron. Another note is that one does not necessarily have to use a logistic sigmoidal activation function; it was just used here as an experiment and to prove to myself I'd always get a linear decision boundary. Conclusion from my experiment I was surprised and impressed that I got a linearly separable result! Albeit, that was not the intent of this exercise, but indicative of the power of a single neuron and thoughtful feature reduction. It makes me wonder what a small neural network could do! Credits and further reading Sebastian Raschka's Python Machine Learning book The open-source notebooks with code accompanying the Python Machine Learning book here and related code here Raschka's blog post on Single-Layer Neural Networks and Gradient Descent Scikit-learn 's preprocessing data module link for scaling features and samples","title":"On using an Adaline Artificial Neuron for Classification"},{"location":"single-artifical-neuron-for-nonlinear-separable-data/#introduction","text":"The intent of this post originally was to show the inner workings and limitations of a single artificial neuron using some moderately complex, noisy data; a challenge of sorts - \"is this noisy data linearly separable with a single artificial neuron and if not, why is that?\". However, I found with some data and algorithm exploration, that I could distinguish between two types of leukemia \u2014 a naive approach and not really biologically significant, but an interesting outcome nonetheless. So, even though this post is about the data science, it also touches on a potential method to use in the real world. In this post, you'll find information on the use of PCA for data reduction/feature engineering, scaling and normalization for preprocessing, the Adaline algorithm (artificial neuron), different activation functions, among other topics and concepts. What is an Adaline artificial neuron Adaline with a sigmoid activation function Choosing an activation function The noisy data 3D to run through network and 2D to gain insights Conclusion from my experiment Credits and further reading","title":"Introduction"},{"location":"single-artifical-neuron-for-nonlinear-separable-data/#what-is-an-adaline-artificial-neuron","text":"The ADAptive LInear NEuron (Adaline) algorithm is very similar to a Perceptron (simplest of the artificial neurons) except that in the Perceptron the weights are updated based on a unit step activation function output (see figure below) whereas Adaline uses a linear activation function to update it's weights giving it a more robust result (that even converges with samples that are not completely separable by a linear hyperplane, unlike the Perceptron). In Adaline a quantizer after the activation function, is used to then predict class labels. Beyond the linear activation function and the quantizer , we see the use of a cost function , or objective function , to update the weights. In this case we want to minimize this function with an optimization method. The optimization of the cost function happens with yet another function aptly and simply named an optimization function . In this case our optimization function is stochastic gradient decent , which one can of as \"climbing down a hill\" (using part of the data to calculate, shuffled as well) to get to the minima of the cost function's convex curve (as it updates weights iteratively from a shuffled dataset). A really great discussion from which much of this information was adapted can be found in Sebastian Raschka's Python Machine Learning book (link here ) and excellent blog post on this topic here on the single-layer neurons.","title":"What is an Adaline artificial neuron"},{"location":"single-artifical-neuron-for-nonlinear-separable-data/#adaline-with-a-sigmoid-activation-function","text":"I grabbed Raschka's ADAptive LInear NEuron (Adaline) classifier open-source code here (the AdalineSGD class) and updated the activation function to logistic sigmoid from a linear function. Note, with the Adaline (versus the Perceptron) we use a continuous number rather than the binary class label, to compute the model error and update the weights. Then to predict a class label, another function is used called a quantizer . Also, the weights are updated in a more sophisticated manner.","title":"Adaline with a sigmoid activation function"},{"location":"single-artifical-neuron-for-nonlinear-separable-data/#choosing-an-activation-function","text":"In code, given this \"net input\" function: def net_input ( self , X ): \"\"\"Calculate net input\"\"\" return np . dot ( X , self . w_ [ 1 :]) + self . w_ [ 0 ] I update the activation function from linear as in: def activation ( self , X ): \"\"\"Compute linear activation\"\"\" return self . net_input ( X ) To a logistic sigmoidal function: def activation ( self , X ): \"\"\"Compute sigmoidal activation Returns ------- A 1d array of length n_samples \"\"\" x = self . net_input ( X ) func = lambda v : 1 / ( 1 + math . exp ( - v )) return np . array ( list ( map ( func , x ))) Full code here and here . We still get linear classification boundaries These single-neuron classifiers can only result in linear decision boundaries, even if using a non-linear activation, because it's still using a single threshold value, z as in diagram above, to decide whether a data point is classified as 1 or -1.","title":"Choosing an activation function"},{"location":"single-artifical-neuron-for-nonlinear-separable-data/#the-noisy-data","text":"The data was downloaded from the Machine Learning Data Set Repository mldata.org using a convenience function from scikit-learn . from sklearn.datasets.mldata import fetch_mldata # Fetch a small leukemia dataset from mldata.org # http://mldata.org/repository/data/viewslug/leukemia-all-vs-aml/ test_data_home = tempfile . mkdtemp () leuk = fetch_mldata ( 'leukemia' , transpose_data = True , data_home = test_data_home ) The data is a small, but wide acute lymphocytic leukemia (ALL) vs. acute myelogenous leukemia (AML) dataset. It has approximately 7000 biological markers (our features), vs. 72 samples (our data points). Given the noisy nature of the data and possible skewedness, it was standardized and normalized with convenience functions from scikit-learn : from sklearn.preprocessing import RobustScaler , Normalizer # Fit the scalar to the training dataset for # zero mean and unit variance of features. # Using a robust scaler which is more resistent to outliers. scaler = RobustScaler () scaler . fit ( X_train ) # Apply the transform X_train = scaler . transform ( X_train ) # Apply the same transform to the test dataset # (simulating what happens when we get new data) X_test = scaler . transform ( X_test ) # Normalizing data as well to scale samples to unit norm normalizer = Normalizer () . fit ( X_train ) X_train = normalizer . transform ( X_train ) X_test = normalizer . transform ( X_test ) Full code here and here . I tried just one feature reduction with PCA to reduce all 7129 dimensions to 2D at first. However, I could not separate out the ALL samples from AML - this wasn't necessarily important to my post on Adaline neurons I was writing, but I decided to try something I'd read about recently for kicks. In fact the idea sprung from a comment in a Python script where a perceptron was used to create non-linear separation of data for a plot (from this script on Github). The comment went: # map the data into a space with one addition dimension so that # it becomes linearly separable So, I gave it a shot.","title":"The noisy data"},{"location":"single-artifical-neuron-for-nonlinear-separable-data/#3d-to-run-through-network-and-2d-to-gain-insights","text":"My next step was to try feeding the neural network the data in 3D space (the 3 features or components from the first PCA reduction). I then reduced the 3D data to 2D, mainly to visualize it. A hyperplane was drawn (blank dashed line) to represent the decision boundary. The surface in the diagram below is representative of a sigmoidal output along the direction of the weight vector. Full code here and here . Note, the stochastic part of the single-neuron optimizer, stochastic gradient decent, causes some variation in the results if run again. It might be a good idea to do a batch version of the Adaline neuron. Another note is that one does not necessarily have to use a logistic sigmoidal activation function; it was just used here as an experiment and to prove to myself I'd always get a linear decision boundary.","title":"3D to run through network and 2D to gain insights"},{"location":"single-artifical-neuron-for-nonlinear-separable-data/#conclusion-from-my-experiment","text":"I was surprised and impressed that I got a linearly separable result! Albeit, that was not the intent of this exercise, but indicative of the power of a single neuron and thoughtful feature reduction. It makes me wonder what a small neural network could do!","title":"Conclusion from my experiment"},{"location":"single-artifical-neuron-for-nonlinear-separable-data/#credits-and-further-reading","text":"Sebastian Raschka's Python Machine Learning book The open-source notebooks with code accompanying the Python Machine Learning book here and related code here Raschka's blog post on Single-Layer Neural Networks and Gradient Descent Scikit-learn 's preprocessing data module link for scaling features and samples","title":"Credits and further reading"},{"location":"sqlheartpython/","text":"As I follow along this getting-started tutorial, I do begin to get excited as I set up my Linux (Ubuntu) Docker container with SQLServer image living inside. I'm also itching to get started creating some connections with pyodbc . Why is this so feverishly exciting? Because of dask out-of-core data frames and pyarrow Apache Arrow columnar/in-memory efficient tables of course. Run the handy SQL command tool: sqlcmd -S 127.0.0.1 -U sa -P $SQL_PASS where $SQL_PASS is the SQL Server password as an environment variable. Then, one line at a time, enter: USE SampleDB ; INSERT INTO Employees ( Name , Location ) VALUES ( N 'Alice' , N 'Australia' ), ( N 'Mad Hatter' , N 'England' ), ( N 'White Rabbit' , N 'India' ); GO ; Few snags: ODBC connections are looking for the ODBC driver for SQL Server in the wrong place, so used odbcinst -j to find out where it was looking then symlink'd to that location as in this SO . Problems: - No extra word \"DATABASE\" needed when using \"USE\" - mapping to \"localhost\" isn't working, only 127.0.0.1 - Spelling error: Successfuly","title":"The Not-So-Slow dance of Python and SQLServer"},{"location":"teaching-notes-post/","text":"Posted: 2016-10-24 Teaching notes Have the attendees introduce themselves to their neighbor(s) The isolation and separateness that ones feels in a room of mostly strangers, about to embark on a few hours or a few days of training can melt away in the matter of 5 minutes. In this technique, for about 5 minutes at the very beginning before any presentation begins, I ask everyone to take a moment and introduce themselves to their neighbors(s). I encourage them to state their name, role and an interesting fact about themselves. This technique forms very useful connections that will serve in the future when one person may need help or a discussion needs to take place to formulate a question to the instructor. When the hands-on-labs role around, the students now know some of their neighbors and can work together on a problem. When it comes time for a build-a-thon or hackathon, the students will likely feel more comfortable forming groups. Lead with a motivating example or at least \"what you will be able to do by the end\" In the best scenario I have a demo that wows the audience or at least immediately gets their attention. The demo could be a Power BI dashboard driven by streaming data flowing in from a weather station. It could be a service that allows me to upload a picture and returns the contents of that image in natural language (like CaptionBot ). It could be a fast process, such as running a large machine learning computation out-of-core on a single laptop, lightning fast, perhaps predicting tens of thousands of numeric labels on a hand-written data set of digits using a trained classificaion model. Seeing something run and produce an amazingly quick and/or fascinating result or a dashboard powered by fast moving data streaming in, real-time, can definitely inspire and motivate those who like that sort of thing. Always have advanced exercises for students who want to go ahead I feel one of the worse outcomes is not being challenged. We mostly all like a good challenge and in the end, for me at least, it gives a feeling of accomplishment. I learn best when I am challenged to think and/or be creative. In the hands-on-labs this is especially important. And it's really easy to incorporate Advanced Exercises into the lab instructions. Then I simply let them work and encourage them to keep going if they finish something earlier than others. Just remember, it's got to be motivating and fun. Basically, I try to always peak their curiosity and keep it rolling. Make mistakes and be a real person This helps normalize the instructor to the students, removing barriers that might be there or percieved. It humanizes the instructor and puts the students at ease knowing they too can explore their studies and make mistakes. After all, the instructor did this in front of everyone. When I make a mistake, I admit it and I ask for help from the audience. Also, when I forget something, can't remember a detail or am stumped, I pose the question to the audience to get them both thinking and my answer back (hopefully). In this way, it serves many purposes: engagement by having a group exercise and audience participation, normalizing everyone (especially the instructor to the students) and getting the answer. Sticky notes (red = I need help or I'm not ready to move on, another color = no problem here) I borrowed this idea from Michael Levy (see Credits at the bottom) who teaches R at scale. The basic premise is to allow students to indicate a problem, be it I'm moving too fast, they don't fully understand something, they need more time or have a quesion. The red sticky note is then placed on the front of the laptop so that I can see it from the front of the room. It's mainly useful in the hands-on-labs portions or the build-a-thon/hackathon. This way students don't have to keep a hand raised or feel they are intruding or disturbing a class. It equalizes things a little bit I believe, adding to the ease and freedom to ask a quesion or ask for help. It's been popular and successful in the past especially in situations where the room is very quiet (like in a self-paced, self-guided tutorial). At the end you may have the students leave the sticky notes (a red is dissatisfied and the other color is satisfied) in an anonymous spot. Some attendees expressing their appreciation: Credits I have to give Michael Levy credit for many of these ideas from his fantastic blog post on Teaching R to 200 students in a week here . /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/ /* var disqus_config = function () { this.page.url = http://127.0.0.1:8000/teaching-notes-post/; // Replace PAGE_URL with your page's canonical URL variable this.page.identifier = teaching-notes-post; // Replace PAGE_IDENTIFIER with your page's unique identifier variable }; */ (function() { // DON'T EDIT BELOW THIS LINE var d = document, s = d.createElement('script'); s.src = '//michhar2.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); Please enable JavaScript to view the comments powered by Disqus.","title":"Tips I have Learned by Being a Trainer for a Year"},{"location":"teaching-notes-post/#teaching-notes","text":"","title":"Teaching notes"},{"location":"teaching-notes-post/#have-the-attendees-introduce-themselves-to-their-neighbors","text":"The isolation and separateness that ones feels in a room of mostly strangers, about to embark on a few hours or a few days of training can melt away in the matter of 5 minutes. In this technique, for about 5 minutes at the very beginning before any presentation begins, I ask everyone to take a moment and introduce themselves to their neighbors(s). I encourage them to state their name, role and an interesting fact about themselves. This technique forms very useful connections that will serve in the future when one person may need help or a discussion needs to take place to formulate a question to the instructor. When the hands-on-labs role around, the students now know some of their neighbors and can work together on a problem. When it comes time for a build-a-thon or hackathon, the students will likely feel more comfortable forming groups.","title":"Have the attendees introduce themselves to their neighbor(s)"},{"location":"teaching-notes-post/#lead-with-a-motivating-example-or-at-least-what-you-will-be-able-to-do-by-the-end","text":"In the best scenario I have a demo that wows the audience or at least immediately gets their attention. The demo could be a Power BI dashboard driven by streaming data flowing in from a weather station. It could be a service that allows me to upload a picture and returns the contents of that image in natural language (like CaptionBot ). It could be a fast process, such as running a large machine learning computation out-of-core on a single laptop, lightning fast, perhaps predicting tens of thousands of numeric labels on a hand-written data set of digits using a trained classificaion model. Seeing something run and produce an amazingly quick and/or fascinating result or a dashboard powered by fast moving data streaming in, real-time, can definitely inspire and motivate those who like that sort of thing.","title":"Lead with a motivating example or at least \"what you will be able to do by the end\""},{"location":"teaching-notes-post/#always-have-advanced-exercises-for-students-who-want-to-go-ahead","text":"I feel one of the worse outcomes is not being challenged. We mostly all like a good challenge and in the end, for me at least, it gives a feeling of accomplishment. I learn best when I am challenged to think and/or be creative. In the hands-on-labs this is especially important. And it's really easy to incorporate Advanced Exercises into the lab instructions. Then I simply let them work and encourage them to keep going if they finish something earlier than others. Just remember, it's got to be motivating and fun. Basically, I try to always peak their curiosity and keep it rolling.","title":"Always have advanced exercises for students who want to go ahead"},{"location":"teaching-notes-post/#make-mistakes-and-be-a-real-person","text":"This helps normalize the instructor to the students, removing barriers that might be there or percieved. It humanizes the instructor and puts the students at ease knowing they too can explore their studies and make mistakes. After all, the instructor did this in front of everyone. When I make a mistake, I admit it and I ask for help from the audience. Also, when I forget something, can't remember a detail or am stumped, I pose the question to the audience to get them both thinking and my answer back (hopefully). In this way, it serves many purposes: engagement by having a group exercise and audience participation, normalizing everyone (especially the instructor to the students) and getting the answer.","title":"Make mistakes and be a real person"},{"location":"teaching-notes-post/#sticky-notes-red-i-need-help-or-im-not-ready-to-move-on-another-color-no-problem-here","text":"I borrowed this idea from Michael Levy (see Credits at the bottom) who teaches R at scale. The basic premise is to allow students to indicate a problem, be it I'm moving too fast, they don't fully understand something, they need more time or have a quesion. The red sticky note is then placed on the front of the laptop so that I can see it from the front of the room. It's mainly useful in the hands-on-labs portions or the build-a-thon/hackathon. This way students don't have to keep a hand raised or feel they are intruding or disturbing a class. It equalizes things a little bit I believe, adding to the ease and freedom to ask a quesion or ask for help. It's been popular and successful in the past especially in situations where the room is very quiet (like in a self-paced, self-guided tutorial). At the end you may have the students leave the sticky notes (a red is dissatisfied and the other color is satisfied) in an anonymous spot. Some attendees expressing their appreciation:","title":"Sticky notes (red = I need help or I'm not ready to move on, another color = no problem here)"},{"location":"teaching-notes-post/#credits","text":"I have to give Michael Levy credit for many of these ideas from his fantastic blog post on Teaching R to 200 students in a week here . /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/ /* var disqus_config = function () { this.page.url = http://127.0.0.1:8000/teaching-notes-post/; // Replace PAGE_URL with your page's canonical URL variable this.page.identifier = teaching-notes-post; // Replace PAGE_IDENTIFIER with your page's unique identifier variable }; */ (function() { // DON'T EDIT BELOW THIS LINE var d = document, s = d.createElement('script'); s.src = '//michhar2.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); Please enable JavaScript to view the comments powered by Disqus.","title":"Credits"},{"location":"two-cents-on-python-package-structure/","text":"tl;dr : There's a standard for structuring a Python package and setup.py is at the heart of it. Posted : 2017-03-06 I've pulled from several sources (see Resources below) and have mashed them together to create a brief synopsis of what I plan on doing for my first python package. I thought I'd share these findings with you. I've tried to be python \u2154 agnostic as needed. To make the most barebones package we can use the following structure (if we include the right code in setup.py this could be a pip installable package in no time!). coolname_project is the GitHub repo name and what I refer to as the base folder. This is the structure of our barebones package: coolname_project coolname/ __init__.py setup.py An example of a more common structure I've seen: coolname_project coolname/ __init__.py somecoolmodule.py command_line.py test/ test_somecoolmodule.py test_command_line.py docs/ greatdoc.txt bin/ runsomestuff.sh examples/ snippet.py setup.py README Keep reading to find out what goes in these folders and files. A very quick example of a barebones package To __init__.py add this: from __future__ import print_function def foo (): print ( 42 ) Now we can use our brand new package in python: >>> import coolname >>> coolname.foo () Add the following to setup.py (more should go here and we'll see in a bit, but this is barebones right now): from setuptools import setup setup ( name = 'coolname' , version = '0.1' , description = 'The coolest package around' , url = 'http://github.com/<your username>/coolname_project' , author = 'Your Name' , author_email = 'name@example.com' , license = 'MIT' , packages = [ 'coolname' ], zip_safe = False ) Adding a setup.py with certain info allows us to be able to do an awesome thing: pip install our package (here, locally). In our base package folder just type: pip install . Congrats! You have an awesome, little package (although it doesn't do anything very cool yet - that's up to you!). Note: the setup.py is a powerful tool and will likely contain much, much more like dependency specifications, more metadata around the package, entry points, testing framework specs, etc. Read on...the dual-purpose README Don't you just love reusability? If we write our README in reStructuredText format it not only will look good on GitHub, it'll serve as the long_description or detailed description of our package on PyPi. To make sure this happens we need a file called MANIFEST.in as well. MANIFEST.in also does some more useful things (see Jeff Knupp's article in Resources below). So, we could, for example, have in our README.rst file: The Coolest Package Ever -------- To use (with caution), simply do:: >>> import coolname >>> coolname.foo() The in our MANIFEST.in (this file does other things down the road, but for now we'll use it to include our README): include README.rst Summary of the folder/files in a package This is what I've gleaned so far from guides. Basics: coolname/ \u2014 the source folder with sub-modules (e.g. sub-module file called dosomething.py ) and containing an __init__.py file (usually empty, but req'd for installation) coolname/test/ \u2014 package folder to hold tests; place files that begin with \"test\" such as test_dosomething.py so that programs like pytest can find them and execute. NOTE: There's an alternative test folder structure where the test-containing folder is named tests (plural) and placed at the base of the package (with setup.py ) - check out this doc on pytest 'ing and folder structures. setup.py \u2014 script to install/test the package and provide metadata (e.g. the long_description for PyPi) - necessary to have a pip installable package. README \u2014 basic information on the package, how to use, how to install, etc. bin/ - executables folder (non-py files) Often included: docs/ \u2014 documentation folder for the package (as .txt, .md, etc. \u2014 need to indicate this folder in setup.py if you want it in distribution) examples/ - a folder with some samples and code snippets of package usage scripts/ \u2014 folder for command line tools like entry points (e.g. with a main() ) Makefile \u2014 a file sometimes included for running the unit tests and more Note: if there's only one file containing all the source code you can skip creating the /coolname project folder with the __init__.py and just place the source code file in the base directory. References and places to go for more Check out the python-packaging guide which walks you through pip-friendly package creation here (although it's targeted for python 2). Check out Open Sourcing a Python Project the Right Way for a detailed package dev workflow with tons of sample code and great explanations by Jeff Knupp. Jake VanderPlas has a great blog post with videos talking about writing python packages and testing with PyTest among other things here . Check out the do's and don'ts here for a quick \"do/don't-do\" synopsis around packaging in python by Jean-Paul Calderone.","title":"Wading In a Tide Pool of Choices, How to Write a Package in Python?"},{"location":"two-cents-on-python-package-structure/#a-very-quick-example-of-a-barebones-package","text":"To __init__.py add this: from __future__ import print_function def foo (): print ( 42 ) Now we can use our brand new package in python: >>> import coolname >>> coolname.foo () Add the following to setup.py (more should go here and we'll see in a bit, but this is barebones right now): from setuptools import setup setup ( name = 'coolname' , version = '0.1' , description = 'The coolest package around' , url = 'http://github.com/<your username>/coolname_project' , author = 'Your Name' , author_email = 'name@example.com' , license = 'MIT' , packages = [ 'coolname' ], zip_safe = False ) Adding a setup.py with certain info allows us to be able to do an awesome thing: pip install our package (here, locally). In our base package folder just type: pip install . Congrats! You have an awesome, little package (although it doesn't do anything very cool yet - that's up to you!). Note: the setup.py is a powerful tool and will likely contain much, much more like dependency specifications, more metadata around the package, entry points, testing framework specs, etc.","title":"A very quick example of a barebones package"},{"location":"two-cents-on-python-package-structure/#read-onthe-dual-purpose-readme","text":"Don't you just love reusability? If we write our README in reStructuredText format it not only will look good on GitHub, it'll serve as the long_description or detailed description of our package on PyPi. To make sure this happens we need a file called MANIFEST.in as well. MANIFEST.in also does some more useful things (see Jeff Knupp's article in Resources below). So, we could, for example, have in our README.rst file: The Coolest Package Ever -------- To use (with caution), simply do:: >>> import coolname >>> coolname.foo() The in our MANIFEST.in (this file does other things down the road, but for now we'll use it to include our README): include README.rst","title":"Read on...the dual-purpose README"},{"location":"two-cents-on-python-package-structure/#summary-of-the-folderfiles-in-a-package","text":"This is what I've gleaned so far from guides.","title":"Summary of the folder/files in a package"},{"location":"two-cents-on-python-package-structure/#basics","text":"coolname/ \u2014 the source folder with sub-modules (e.g. sub-module file called dosomething.py ) and containing an __init__.py file (usually empty, but req'd for installation) coolname/test/ \u2014 package folder to hold tests; place files that begin with \"test\" such as test_dosomething.py so that programs like pytest can find them and execute. NOTE: There's an alternative test folder structure where the test-containing folder is named tests (plural) and placed at the base of the package (with setup.py ) - check out this doc on pytest 'ing and folder structures. setup.py \u2014 script to install/test the package and provide metadata (e.g. the long_description for PyPi) - necessary to have a pip installable package. README \u2014 basic information on the package, how to use, how to install, etc. bin/ - executables folder (non-py files)","title":"Basics:"},{"location":"two-cents-on-python-package-structure/#often-included","text":"docs/ \u2014 documentation folder for the package (as .txt, .md, etc. \u2014 need to indicate this folder in setup.py if you want it in distribution) examples/ - a folder with some samples and code snippets of package usage scripts/ \u2014 folder for command line tools like entry points (e.g. with a main() ) Makefile \u2014 a file sometimes included for running the unit tests and more Note: if there's only one file containing all the source code you can skip creating the /coolname project folder with the __init__.py and just place the source code file in the base directory.","title":"Often included:"},{"location":"two-cents-on-python-package-structure/#references-and-places-to-go-for-more","text":"Check out the python-packaging guide which walks you through pip-friendly package creation here (although it's targeted for python 2). Check out Open Sourcing a Python Project the Right Way for a detailed package dev workflow with tons of sample code and great explanations by Jeff Knupp. Jake VanderPlas has a great blog post with videos talking about writing python packages and testing with PyTest among other things here . Check out the do's and don'ts here for a quick \"do/don't-do\" synopsis around packaging in python by Jean-Paul Calderone.","title":"References and places to go for more"},{"location":"yolo_wip_post/","text":"Notes on Training a Tiny YOLO v3 Model for Use in an iOS App Pipeline for Creating and Using a Darknet-Based YOLOv3 Model Label and train with Darknet (Windows 10 and NVIDIA GeForce GPU) Convert Darknet model to Keras (macOS) - keras-yolo3 project Convert Keras model to CoreML (macOS) - coremltools package is mac OS only as of writing this Run iOS XCode project and build to iPhone/iPad (macOS/iOS) Convert to Keras with script from https://github.com/qqwweee/keras-yolo3 python convert.py experiment/yolov3-tiny.cfg model_data/yolov3-tiny_lr0.001_July7.weights model_data/yolov3-tiny-minifig.h5 CoreML in https://github.com/Ma-Dan/YOLOv3-CoreML/tree/master/Convert * Update output?? In https://github.com/hollance/YOLO-CoreML-MPSNNGraph XCode project * Update labels in Helpers.swift * Update numClasses in YOLO.swift * Add the TinyYOLO.mlmodel from the custom training/conversion above macOS Instructions for Training with Darknet (CPU) My System macOS High Sierra 10.13.5 MacBook Pro (15-inch, 2017) Steps Build Yolo_mark for Linux according to instructions at https://github.com/AlexeyAB/Yolo_mark (clone and run) Build darknet with opencv (mostly following instructions at https://pjreddie.com/darknet/install/ ): 2.1 brew install opencv@2 2.2 Clone darknet repo: git clone https://github.com/pjreddie/darknet 2.3 cd darknet 2.4 In the Makefile update the opencv parameter to OPENCV=1 (change any other pertinent parameters as well here) 2.5 Run make in the base darknet directory Label images with bouding boxes and classes according to the steps on Yolo_mark under \"To use for labeling your custom images\" Train a Tiny YOLO v3 model on custom images according to more instructions at https://github.com/AlexeyAB/darknet#how-to-train-tiny-yolo-to-detect-your-custom-objects 4.1 Note, put your obj.data file in the cfg folder and use full paths to the train.txt and obj.names 4.2 If doing transfer learning, place stopbackward=1 in the yolov3-tiny.cfg 4.3 Run with: ./darknet detector train cfg/lego.data experiment/yolov3-tiny.cfg experiment/yolov3-tiny.conv.15 4.4 The final trained model with be in the backup folder. Windows Instructions for Training with Darknet (GPU) TIP: when moving any files from Windows to macOS, check for proper newlines (e.g. lack of ^M characters in text files) My System Windows 10 NVIDIA GeForce GTX 1060 Graphics Card CUDA 9.1 cuDNN 7.0 Steps (WIP) Setup - see https://github.com/AlexeyAB/darknet for details and download links * MS Visual Studio 2017 (instructions say 2015, but 2017 worked well for me) * OpenCV 3.4.0 Built Yolo_mark for Windows according to instructions at https://github.com/AlexeyAB/Yolo_mark (clone and run) Make sure you have OpenCV 3.4.0 at it's at C:\\opencv_3.0\\opencv (Linux instructions on repo Readme) This will produce an executable at C:\\Users\\<directory to solution>\\Yolo_mark\\x64\\Release\\yolo_mark.exe Find files opencv_world320.dll and opencv_ffmpeg320_64.dll (or opencv_world340.dll and opencv_ffmpeg340_64.dll ) in C:\\opencv_3.0\\opencv\\build\\x64\\vc14\\bin and put it near with yolo_mark.exe Label images with bouding boxes and classes according to the steps on Yolo_mark under \"To use for labeling your custom images\" Built darknet for Windows according to instructions at https://github.com/AlexeyAB/darknet#how-to-compile-on-windows Except, use VS 2017 with Platform Toolset Visual Studio 2015 (v140) (right-click solution -> Properties -> Configuration Properties (General)) Make sure cudnn.h is in C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.1\\include folder Make sure cudnn.lib is in C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.1\\lib\\x64 folder TIP: when installing the NVIDIA CUDA Toolkit 9.1, you may need to do an Advanced/Custom install and \"uncheck\" Visual Studio Integration as this may cause install not to work properly Train a Tiny YOLO v3 model on custom images according to more instructions at https://github.com/AlexeyAB/darknet#how-to-train-tiny-yolo-to-detect-your-custom-objects May need to Place certain cuDNN libs into v9.1 CUDA directory (e.g. cudnn64_7.dll found in search after a cuDNN intall into C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.1\\bin ) Run with: darknet.exe detector train cfg\\lego.data experiment\\yolov3-tiny.cfg experiment\\yolov3-tiny.conv.15 The final trained model with be in the backup folder. What is YOLO and Object Detection Great Series of Videos on Object Detection and YOLO (Convolutional Neural Network (CNNs) by Andrew Ng [Full Course]): * Object Detection is CNN20-31, with YOLO being CNN31 ( Videos ) Great articles here: * Official fun YOLOv3 Paper * Great post from an ML expert and iOS App builder, Matthijs Hollemans Blog","title":"Notes on Training a Tiny YOLO v3 Model for Use in an iOS App"},{"location":"yolo_wip_post/#notes-on-training-a-tiny-yolo-v3-model-for-use-in-an-ios-app","text":"","title":"Notes on Training a Tiny YOLO v3 Model for Use in an iOS App"},{"location":"yolo_wip_post/#pipeline-for-creating-and-using-a-darknet-based-yolov3-model","text":"Label and train with Darknet (Windows 10 and NVIDIA GeForce GPU) Convert Darknet model to Keras (macOS) - keras-yolo3 project Convert Keras model to CoreML (macOS) - coremltools package is mac OS only as of writing this Run iOS XCode project and build to iPhone/iPad (macOS/iOS) Convert to Keras with script from https://github.com/qqwweee/keras-yolo3 python convert.py experiment/yolov3-tiny.cfg model_data/yolov3-tiny_lr0.001_July7.weights model_data/yolov3-tiny-minifig.h5 CoreML in https://github.com/Ma-Dan/YOLOv3-CoreML/tree/master/Convert * Update output?? In https://github.com/hollance/YOLO-CoreML-MPSNNGraph XCode project * Update labels in Helpers.swift * Update numClasses in YOLO.swift * Add the TinyYOLO.mlmodel from the custom training/conversion above","title":"Pipeline for Creating and Using a Darknet-Based YOLOv3 Model"},{"location":"yolo_wip_post/#macos-instructions-for-training-with-darknet-cpu","text":"","title":"macOS Instructions for Training with Darknet (CPU)"},{"location":"yolo_wip_post/#my-system","text":"macOS High Sierra 10.13.5 MacBook Pro (15-inch, 2017)","title":"My System"},{"location":"yolo_wip_post/#steps","text":"Build Yolo_mark for Linux according to instructions at https://github.com/AlexeyAB/Yolo_mark (clone and run) Build darknet with opencv (mostly following instructions at https://pjreddie.com/darknet/install/ ): 2.1 brew install opencv@2 2.2 Clone darknet repo: git clone https://github.com/pjreddie/darknet 2.3 cd darknet 2.4 In the Makefile update the opencv parameter to OPENCV=1 (change any other pertinent parameters as well here) 2.5 Run make in the base darknet directory Label images with bouding boxes and classes according to the steps on Yolo_mark under \"To use for labeling your custom images\" Train a Tiny YOLO v3 model on custom images according to more instructions at https://github.com/AlexeyAB/darknet#how-to-train-tiny-yolo-to-detect-your-custom-objects 4.1 Note, put your obj.data file in the cfg folder and use full paths to the train.txt and obj.names 4.2 If doing transfer learning, place stopbackward=1 in the yolov3-tiny.cfg 4.3 Run with: ./darknet detector train cfg/lego.data experiment/yolov3-tiny.cfg experiment/yolov3-tiny.conv.15 4.4 The final trained model with be in the backup folder.","title":"Steps"},{"location":"yolo_wip_post/#windows-instructions-for-training-with-darknet-gpu","text":"TIP: when moving any files from Windows to macOS, check for proper newlines (e.g. lack of ^M characters in text files)","title":"Windows Instructions for Training with Darknet (GPU)"},{"location":"yolo_wip_post/#my-system_1","text":"Windows 10 NVIDIA GeForce GTX 1060 Graphics Card CUDA 9.1 cuDNN 7.0","title":"My System"},{"location":"yolo_wip_post/#steps-wip","text":"Setup - see https://github.com/AlexeyAB/darknet for details and download links * MS Visual Studio 2017 (instructions say 2015, but 2017 worked well for me) * OpenCV 3.4.0 Built Yolo_mark for Windows according to instructions at https://github.com/AlexeyAB/Yolo_mark (clone and run) Make sure you have OpenCV 3.4.0 at it's at C:\\opencv_3.0\\opencv (Linux instructions on repo Readme) This will produce an executable at C:\\Users\\<directory to solution>\\Yolo_mark\\x64\\Release\\yolo_mark.exe Find files opencv_world320.dll and opencv_ffmpeg320_64.dll (or opencv_world340.dll and opencv_ffmpeg340_64.dll ) in C:\\opencv_3.0\\opencv\\build\\x64\\vc14\\bin and put it near with yolo_mark.exe Label images with bouding boxes and classes according to the steps on Yolo_mark under \"To use for labeling your custom images\" Built darknet for Windows according to instructions at https://github.com/AlexeyAB/darknet#how-to-compile-on-windows Except, use VS 2017 with Platform Toolset Visual Studio 2015 (v140) (right-click solution -> Properties -> Configuration Properties (General)) Make sure cudnn.h is in C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.1\\include folder Make sure cudnn.lib is in C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.1\\lib\\x64 folder TIP: when installing the NVIDIA CUDA Toolkit 9.1, you may need to do an Advanced/Custom install and \"uncheck\" Visual Studio Integration as this may cause install not to work properly Train a Tiny YOLO v3 model on custom images according to more instructions at https://github.com/AlexeyAB/darknet#how-to-train-tiny-yolo-to-detect-your-custom-objects May need to Place certain cuDNN libs into v9.1 CUDA directory (e.g. cudnn64_7.dll found in search after a cuDNN intall into C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.1\\bin ) Run with: darknet.exe detector train cfg\\lego.data experiment\\yolov3-tiny.cfg experiment\\yolov3-tiny.conv.15 The final trained model with be in the backup folder.","title":"Steps (WIP)"},{"location":"yolo_wip_post/#what-is-yolo-and-object-detection","text":"Great Series of Videos on Object Detection and YOLO (Convolutional Neural Network (CNNs) by Andrew Ng [Full Course]): * Object Detection is CNN20-31, with YOLO being CNN31 ( Videos ) Great articles here: * Official fun YOLOv3 Paper * Great post from an ML expert and iOS App builder, Matthijs Hollemans Blog","title":"What is YOLO and Object Detection"}]}